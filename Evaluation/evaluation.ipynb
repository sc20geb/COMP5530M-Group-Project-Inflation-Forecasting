{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalution and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(fileName:str):\n",
    "    '''\n",
    "    This function gets the model name according to the following naming convention: {model name}_{information...}.npy,\n",
    "    where {information} is optional. \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    fileName: A string of the models predictions file name.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    A string of the models name.\n",
    "    '''\n",
    "\n",
    "    return re.findall(r'([a-zA-Z0-9]+)',fileName)[0]\n",
    "\n",
    "\n",
    "def get_predictions(predsPath:Path):\n",
    "    '''\n",
    "    This function combines all predictions of all models into one dataframe and includes the ground truth.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    predsPath: A path/string of a path, to the directory of all the predictions\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Returns a pandas Dataframe of all the models predictions, as well as the ground truth. \n",
    "    '''\n",
    "    # Get the ground truth:\n",
    "    groundTruth= pd.read_csv(Path('../Data/Test/test1990s.csv'),parse_dates=[0],date_format='%m%Y',index_col=0, usecols=[0,1])\n",
    "\n",
    "    # make an empty Dataframe to store the models predictions (making the index  the observation data):\n",
    "    predsDf= pd.DataFrame(index=groundTruth.index)\n",
    "\n",
    "    # Add the ground truth to the predictions dataframe\n",
    "    predsDf['ground_truth']= groundTruth\n",
    "\n",
    "    # Loop over all the files in the predictions folder:\n",
    "    for i in list(predsPath.glob('*.npy')):\n",
    "        # Add the predictions to the predictions dataframe, where the column is the model name\n",
    "        predsDf[get_model_name(i.name)]= np.load(i)[:48]\n",
    "    \n",
    "    return predsDf\n",
    "\n",
    "def calc_metrics(predictionsDf:pd.DataFrame):\n",
    "    '''\n",
    "    This function calculates the evaluation metrics of each model, given the predictions dataframe.\n",
    "    The following metrics are used:\n",
    "        * RMSE\n",
    "        * MAE\n",
    "        * r^2 score\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictionsDf: a pandas datframe containg the ground truth and all the predictions for each model, organized in columns.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Returns a pandas Dataframe containg all the evaluation metrics of all the models, where each column represents a metric and each row represents a model.\n",
    "    '''\n",
    "    # create an empty dataframe with columns reprresnting an evaluation metric\n",
    "    metricsDf= pd.DataFrame(columns=['RMSE','MAE', 'r2'])\n",
    "    \n",
    "    # Loop over all columns/models in the prtedictions dataframe\n",
    "    for model in predictionsDf.columns.drop('ground_truth'):\n",
    "        # Calculate the metrics and add them to the metrics dataframe\n",
    "        metricsDf.loc[model,'RMSE']=root_mean_squared_error(predictionsDf['ground_truth'],predictionsDf[model])\n",
    "        metricsDf.loc[model,'MAE']=mean_absolute_error(predictionsDf['ground_truth'],predictionsDf[model])\n",
    "        metricsDf.loc[model,'r2']=r2_score(predictionsDf['ground_truth'],predictionsDf[model])\n",
    "\n",
    "    return metricsDf\n",
    "\n",
    "\n",
    "def get_predictions_metrics(predsPath:Path):\n",
    "    '''\n",
    "    This Function combines get_predictions and calc_metrics, which combines all predictions of all models into one dataframe and includes the ground truth and calculates\n",
    "    the evaluation metrics of each model, given the predictions dataframe.\n",
    "    The following metrics are used:\n",
    "        * RMSE\n",
    "        * MAE\n",
    "        * r^2 score\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    predsPath: A path/string of a path, to the directory of all the predictions\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Returns a tuple, where the 1st element is a pandas Dataframe of all the models predictions, as well as the ground truth,\n",
    "    The 2nd element is a pandas Dataframe containg all the evaluation metrics of all the models, where each column represents a metric and each row represents a model.\n",
    "    '''\n",
    "    predsDf= get_predictions(predsPath)\n",
    "    metricsDf= calc_metrics(predsDf)\n",
    "\n",
    "    return predsDf, metricsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsPath= Path('../Predictions/')\n",
    "predsDf, metricsDf=get_predictions_metrics(predsPath)\n",
    "\n",
    "display(predsDf)\n",
    "display(metricsDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
