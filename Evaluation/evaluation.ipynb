{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalution and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(fileName:str):\n",
    "    '''\n",
    "    This function gets the model name according to the following naming convention: {model name}_{information...}.npy,\n",
    "    where {information} is optional. \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    fileName: A string of the models predictions file name.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    A string of the models name.\n",
    "    '''\n",
    "\n",
    "    return re.findall(r'([a-zA-Z0-9]+)',fileName)[0]\n",
    "\n",
    "\n",
    "def get_predictions(predsPath:Path):\n",
    "    '''\n",
    "    This function combines all predictions of all models into one dataframe and includes the ground truth.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    predsPath: A path/string of a path, to the directory of all the predictions\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Returns a pandas Dataframe of all the models predictions, as well as the ground truth. \n",
    "    '''\n",
    "    # Get the ground truth:\n",
    "    groundTruth= pd.read_csv(Path('../Data/Test/test1990s.csv'),parse_dates=[0],date_format='%m%Y',index_col=0, usecols=[0,1])\n",
    "\n",
    "    # make an empty Dataframe to store the models predictions (making the index  the observation data):\n",
    "    predsDf= pd.DataFrame(index=groundTruth.index)\n",
    "\n",
    "    # Add the ground truth to the predictions dataframe\n",
    "    predsDf['ground_truth']= groundTruth\n",
    "\n",
    "    # Loop over all the files in the predictions folder:\n",
    "    for i in list(predsPath.glob('*.npy')):\n",
    "        # Add the predictions to the predictions dataframe, where the column is the model name\n",
    "        predsDf[get_model_name(i.name)]= np.load(i)[:48]\n",
    "    \n",
    "    return predsDf\n",
    "\n",
    "def calc_metrics(predictionsDf:pd.DataFrame, horizon = None):\n",
    "    '''\n",
    "    This function calculates the evaluation metrics of each model, given the predictions dataframe.\n",
    "    The following metrics are used:\n",
    "        * RMSE\n",
    "        * MAE\n",
    "        * r^2 score\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictionsDf: a pandas datframe containg the ground truth and all the predictions for each model, organized in columns.\n",
    "\n",
    "    horizon: number of timesteps to calculate the metrics.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Returns a pandas Dataframe containg all the evaluation metrics of all the models, where each column represents a metric and each row represents a model.\n",
    "    '''\n",
    "    # Deafult to horizon of 2 years:\n",
    "    if horizon is None:\n",
    "        horizon= predictionsDf.shape[0]\n",
    "\n",
    "    # create an empty dataframe with columns reprresnting an evaluation metric\n",
    "    metricsDf= pd.DataFrame(columns=['RMSE','MAE', 'r2'])\n",
    "    \n",
    "    # Loop over all columns/models in the prtedictions dataframe\n",
    "    for model in predictionsDf.columns.drop('ground_truth'):\n",
    "        # Calculate the metrics and add them to the metrics dataframe\n",
    "        metricsDf.loc[model,'RMSE']=root_mean_squared_error(predictionsDf['ground_truth'].iloc[:horizon],predictionsDf[model].iloc[:horizon])\n",
    "        metricsDf.loc[model,'MAE']=mean_absolute_error(predictionsDf['ground_truth'].iloc[:horizon],predictionsDf[model].iloc[:horizon])\n",
    "        metricsDf.loc[model,'r2']=r2_score(predictionsDf['ground_truth'].iloc[:horizon],predictionsDf[model].iloc[:horizon])\n",
    "\n",
    "    return metricsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsPath= Path('../Predictions/')\n",
    "predsDf=get_predictions(predsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>modelx</th>\n",
       "      <td>103.483968</td>\n",
       "      <td>103.472</td>\n",
       "      <td>-3003.93443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              RMSE      MAE          r2\n",
       "modelx  103.483968  103.472 -3003.93443"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metricsDf= calc_metrics(predsDf, horizon=12)\n",
    "display(metricsDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
