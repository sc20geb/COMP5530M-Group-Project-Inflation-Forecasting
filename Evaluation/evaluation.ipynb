{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalution and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from Helper.evaluation_helpers import get_predictions, calc_metrics, error_plot,plot_metric\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "predsPath = Path('../Predictions/')\n",
    "for i in list(predsPath.glob('*.npy')):\n",
    "    data = np.load(i)\n",
    "    print(f\" {i.name} has {len(data)} values\")  # Show length of each prediction file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsPath= Path('../Predictions/')\n",
    "predsDf=get_predictions(predsPath)\n",
    "display(predsDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric DataFrames are sorted by the main metric before being displayed\n",
    "main_metric = 'RMSE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon = 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsPath_1= Path('../Predictions/Horizon1')\n",
    "predsDf_1=get_predictions(predsPath_1)\n",
    "\n",
    "metrics_1= calc_metrics(predsDf_1)\n",
    "display(metrics_1.sort_values(main_metric, axis=0))\n",
    "\n",
    "error_plot(predsDf_1,model='ARIMA1990',absolute=True, title= 'ARIMA1990 Horizon 1 Error plot:')\n",
    "\n",
    "error_plot(predsDf_1,model=['ARIMA1990','ARIMAX1990', 'SARIMA1990','SARIMAX1990'],absolute=True, title='Horizon 1 Error plot: ')\n",
    "error_plot(predsDf_1,model='all',absolute=False, title= 'Horizon 1 Error plot:')\n",
    "\n",
    "for i in metrics_1.columns:\n",
    "    plot_metric(metrics_1,i, title='Horizon 1:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon = 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsPath_3= Path('../Predictions/Horizon3')\n",
    "predsDf_3=get_predictions(predsPath_3)\n",
    "\n",
    "metrics_3= calc_metrics(predsDf_3)\n",
    "display(metrics_3.sort_values(main_metric, axis=0))\n",
    "\n",
    "error_plot(predsDf_3,model='all',absolute=False, title= 'Horizon 3 Error plot:')\n",
    "for i in metrics_3.columns:\n",
    "    plot_metric(metrics_3,i, title='Horizon 3:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon = 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsPath_6= Path('../Predictions/Horizon6')\n",
    "predsDf_6=get_predictions(predsPath_6)\n",
    "\n",
    "metrics_6= calc_metrics(predsDf_6)\n",
    "display(metrics_6.sort_values(main_metric, axis=0))\n",
    "\n",
    "error_plot(predsDf_6,model='all',absolute=False, title= 'Horizon 6 Error plot:')\n",
    "for i in metrics_6.columns:\n",
    "    plot_metric(metrics_6,i, title='Horizon 6:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon = 12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_12= calc_metrics(predsDf, horizon=12)\n",
    "display(metrics_12.sort_values(main_metric, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "metrics_12= calc_metrics(predsDf_12)\n",
    "display(metrics_12.sort_values(main_metric, axis=0))\n",
    "\n",
    "for model in predsDf.columns:\n",
    "    plt.plot(predsDf.index, predsDf[model])\n",
    "\n",
    "plt.legend(predsDf.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = plt.axes()\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "plt.locator_params(axis='x', nbins=10)\n",
    "\n",
    "for model in predsDf.columns:\n",
    "    plt.plot(predsDf.index[:int(len(predsDf)/4)], predsDf[model][:int(len(predsDf)/4)])\n",
    "\n",
    "plt.legend(predsDf.columns)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
