{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8591b709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Diebold-Mariano tests on model predictions:\n",
      "\n",
      "Looking in: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Predictions\\Horizon1 — Exists? True\n",
      "observation_date\n",
      "01/2024    122.115\n",
      "02/2024    122.494\n",
      "03/2024    122.912\n",
      "04/2024    123.234\n",
      "05/2024    123.224\n",
      "06/2024    123.369\n",
      "07/2024    123.575\n",
      "08/2024    123.727\n",
      "09/2024    123.939\n",
      "10/2024    124.235\n",
      "11/2024    124.387\n",
      "12/2024    124.705\n",
      "Name: ground_truth, dtype: float64\n",
      "Horizon 1 — 20 models: Index(['ARDL', 'ARIMA1990', 'ARIMAX1990', 'GRU1990', 'LSTM', 'MARS', 'MLR',\n",
      "       'Naive', 'NBEATSx', 'NHITS', 'RFX', 'RNN1990', 'SARIMA1990',\n",
      "       'SARIMAX1990', 'TCN', 'TFT', 'Tide', 'VARccf', 'VARcointegration',\n",
      "       'XGB1990'],\n",
      "      dtype='object')\n",
      "Saved DM test ranked results: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon1_DM_results.csv\n",
      "Saved better model ranking: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon1_Better_Model_Ranking.csv\n",
      "\n",
      "Looking in: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Predictions\\Horizon3 — Exists? True\n",
      "observation_date\n",
      "01/2024    122.115\n",
      "02/2024    122.494\n",
      "03/2024    122.912\n",
      "04/2024    123.234\n",
      "05/2024    123.224\n",
      "06/2024    123.369\n",
      "07/2024    123.575\n",
      "08/2024    123.727\n",
      "09/2024    123.939\n",
      "10/2024    124.235\n",
      "11/2024    124.387\n",
      "12/2024    124.705\n",
      "Name: ground_truth, dtype: float64\n",
      "Horizon 3 — 16 models: Index(['ARDL', 'GRU1990', 'LSTM', 'MARS', 'MLR', 'Naive', 'NBEATSx', 'NHITS',\n",
      "       'RFX', 'RNN1990', 'TCN', 'TFT', 'Tide', 'VARccf', 'VARcointegration',\n",
      "       'XGB1990'],\n",
      "      dtype='object')\n",
      "Saved DM test ranked results: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon3_DM_results.csv\n",
      "Saved better model ranking: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon3_Better_Model_Ranking.csv\n",
      "\n",
      "Looking in: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Predictions\\Horizon6 — Exists? True\n",
      "observation_date\n",
      "01/2024    122.115\n",
      "02/2024    122.494\n",
      "03/2024    122.912\n",
      "04/2024    123.234\n",
      "05/2024    123.224\n",
      "06/2024    123.369\n",
      "07/2024    123.575\n",
      "08/2024    123.727\n",
      "09/2024    123.939\n",
      "10/2024    124.235\n",
      "11/2024    124.387\n",
      "12/2024    124.705\n",
      "Name: ground_truth, dtype: float64\n",
      "Horizon 6 — 16 models: Index(['ARDL', 'GRU1990', 'LSTM', 'MARS', 'MLR', 'Naive', 'NBEATSx', 'NHITS',\n",
      "       'RFX', 'RNN1990', 'TCN', 'TFT', 'Tide', 'VARccf', 'VARcointegration',\n",
      "       'XGB1990'],\n",
      "      dtype='object')\n",
      "Saved DM test ranked results: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon6_DM_results.csv\n",
      "Saved better model ranking: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon6_Better_Model_Ranking.csv\n",
      "\n",
      "Looking in: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Predictions\\Horizon12 — Exists? True\n",
      "observation_date\n",
      "01/2024    122.115\n",
      "02/2024    122.494\n",
      "03/2024    122.912\n",
      "04/2024    123.234\n",
      "05/2024    123.224\n",
      "06/2024    123.369\n",
      "07/2024    123.575\n",
      "08/2024    123.727\n",
      "09/2024    123.939\n",
      "10/2024    124.235\n",
      "11/2024    124.387\n",
      "12/2024    124.705\n",
      "Name: ground_truth, dtype: float64\n",
      "Horizon 12 — 16 models: Index(['ARDL', 'GRU1990', 'LSTM', 'MARS', 'MLR', 'Naive', 'NBEATSx', 'NHITS',\n",
      "       'RFX', 'RNN1990', 'TCN', 'TFT', 'Tide', 'VARccf', 'VARcointegration',\n",
      "       'XGB1990'],\n",
      "      dtype='object')\n",
      "Saved DM test ranked results: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon12_DM_results.csv\n",
      "Saved better model ranking: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon12_Better_Model_Ranking.csv\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "\n",
    "# Ensure project root is on the import path\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Imports\n",
    "from Helper.evaluation_helpers import get_predictions\n",
    "from Helper.DM_Test import dm_test\n",
    "from Training.Helper.PyTorchModular import HORIZONS\n",
    "\n",
    "# Setup project paths\n",
    "PROJECT_ROOT = Path().resolve().parents[0]\n",
    "PRED_BASE = PROJECT_ROOT / 'Predictions'\n",
    "SAVE_DIR = PROJECT_ROOT / 'Evaluation' / 'dm_test_results'\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Running Diebold-Mariano tests on model predictions:\")\n",
    "\n",
    "\n",
    "for h in HORIZONS:\n",
    "    preds_dir = PRED_BASE / f\"Horizon{h}\"\n",
    "    print(f\"\\nLooking in: {preds_dir} — Exists? {preds_dir.exists()}\")\n",
    "\n",
    "    if not preds_dir.exists():\n",
    "        print(f\"Horizon {h}: prediction folder not found — skipping.\")\n",
    "        continue\n",
    "\n",
    "    preds_df = get_predictions(preds_dir)\n",
    "\n",
    "    # Separate ground truth and predictions\n",
    "    ground_truth = preds_df['ground_truth']\n",
    "    models = preds_df.columns.drop('ground_truth')\n",
    "    print(ground_truth)\n",
    "    print(f\"Horizon {h} — {len(models)} models: {models}\")\n",
    "\n",
    "    if len(models) < 2:\n",
    "        print(f\"Horizon {h}: not enough models to compare — skipping.\")\n",
    "        continue\n",
    "\n",
    "    results = []\n",
    "    # Run DM test for each unique pair to prevent duplicate\n",
    "    for model1 in models:\n",
    "        if model1 == \"Naive\":\n",
    "            continue\n",
    "        \n",
    "        dm_stat, p_val = dm_test(ground_truth, preds_df[model1], preds_df[\"Naive\"], h=1, crit=\"MAPE\")\n",
    "        #print(model1,dm_stat)\n",
    "        if p_val < 0.05:\n",
    "            better_model = \"Naive\" if dm_stat > 0 else model1\n",
    "        else:\n",
    "            better_model = \"No significant difference\"\n",
    "        \n",
    "        results.append((model1, \"Naive\", dm_stat, p_val, better_model))\n",
    "        \n",
    "    if not results:\n",
    "        print(f\"No valid DM tests for Horizon {h}.\")\n",
    "        continue\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results, columns=[\"Model_1\", \"Model_2\", \"DM_statistic\", \"p_value\", \"Better_Model\"])\n",
    "    results_df = results_df.sort_values(by=\"p_value\").reset_index(drop=True)\n",
    "    results_df.index += 1  # Start rank from 1\n",
    "    results_df.insert(0, \"Rank\", results_df.index)\n",
    "\n",
    "    # Save full ranked results\n",
    "    save_path = SAVE_DIR / f\"Horizon{h}_DM_results.csv\"\n",
    "    results_df.to_csv(save_path, index=False)\n",
    "    print(f\"Saved DM test ranked results: {save_path}\")\n",
    "\n",
    "    # Count how many times each model wins\n",
    "    better_counts = results_df['Better_Model'].value_counts().reset_index()\n",
    "    better_counts.columns = ['Model', 'Wins']\n",
    "    better_counts = better_counts.sort_values(by=\"Wins\", ascending=False).reset_index(drop=True)\n",
    "    better_counts.index += 1  # Start rank from 1\n",
    "    better_counts.insert(0, \"Rank\", better_counts.index)\n",
    "\n",
    "    # Save better model rankings\n",
    "    better_save_path = SAVE_DIR / f\"Horizon{h}_Better_Model_Ranking.csv\"\n",
    "    better_counts.to_csv(better_save_path, index=False)\n",
    "    print(f\"Saved better model ranking: {better_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3648110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved p-value heatmap: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon1_pvalue_heatmap.png\n",
      "Saved better model wins bar plot: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon1_model_wins_bar.png\n",
      "Saved p-value heatmap: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon3_pvalue_heatmap.png\n",
      "Saved better model wins bar plot: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon3_model_wins_bar.png\n",
      "Saved p-value heatmap: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon6_pvalue_heatmap.png\n",
      "Saved better model wins bar plot: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon6_model_wins_bar.png\n",
      "Saved p-value heatmap: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon12_pvalue_heatmap.png\n",
      "Saved better model wins bar plot: C:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\\Evaluation\\dm_test_results\\Horizon12_model_wins_bar.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup\n",
    "PROJECT_ROOT = Path().resolve().parents[0]\n",
    "SAVE_DIR = PROJECT_ROOT / 'Evaluation' / 'dm_test_results'\n",
    "HORIZONS = [1, 3, 6, 12]\n",
    "\n",
    "# Plot Heatmap of p-values\n",
    "def plot_pvalue_heatmap(results_df, horizon_name):\n",
    "    pivot = results_df.pivot(index='Model_1', columns='Model_2', values='p_value')\n",
    "    combined = pivot.combine_first(pivot.T)  # Symmetrize: fill missing by opposite direction\n",
    "\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(\n",
    "        combined,\n",
    "        cmap=\"coolwarm\",\n",
    "        center=0.05,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray',\n",
    "        cbar_kws={'label': 'p-value'},\n",
    "        annot=False  # Optional: set True if you want the number shown\n",
    "    )\n",
    "    plt.title(f\"Diebold-Mariano p-value Heatmap — {horizon_name}\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = SAVE_DIR / f\"{horizon_name}_pvalue_heatmap.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved p-value heatmap: {save_path}\")\n",
    "\n",
    "# Plot Bar chart of model win counts\n",
    "def plot_win_counts(count_df, horizon_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(count_df['Model'], count_df['Wins'], color='skyblue')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel(\"Number of Wins\")\n",
    "    plt.title(f\"Better Model Wins — {horizon_name}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = SAVE_DIR / f\"{horizon_name}_model_wins_bar.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved better model wins bar plot: {save_path}\")\n",
    "\n",
    "# Main loop\n",
    "for h in HORIZONS:\n",
    "    results_path = SAVE_DIR / f\"Horizon{h}_DM_results.csv\"\n",
    "    wins_path = SAVE_DIR / f\"Horizon{h}_Better_Model_Ranking.csv\"\n",
    "\n",
    "    if not results_path.exists() or not wins_path.exists():\n",
    "        print(f\"Horizon{h}: missing results — skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load CSVs\n",
    "    results_df = pd.read_csv(results_path)\n",
    "    wins_df = pd.read_csv(wins_path)\n",
    "\n",
    "    if not results_df.empty:\n",
    "        plot_pvalue_heatmap(results_df, f\"Horizon{h}\")\n",
    "\n",
    "    if not wins_df.empty:\n",
    "        plot_win_counts(wins_df, f\"Horizon{h}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
