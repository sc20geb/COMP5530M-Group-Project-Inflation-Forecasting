{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine data and Cleaning\n",
    "This notebook:\n",
    "* combines all the exogenous data\n",
    "* removes exogenous variables that start too late or end too early\n",
    "* performs grangers causality test to then remove non-useful exogenous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, adfuller\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbookdir = Path.cwd() #path to current folder\n",
    " \n",
    "datasetsPath= workbookdir.parent / 'Data' /'Raw' #Path to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare start and end dates\n",
    "**Change these values to alter the dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#could import helper and use as suggested below\n",
    "#from data_manipulation_helpers import getTestDate\n",
    "START_YEAR=1990\n",
    "END_YEAR=2024\n",
    "TEST_DATE= datetime(2021,1,1).strftime('%m/%Y')\n",
    "#usage example (first argument is the test set size ratio)\n",
    "#TEST_DATE = getTestDate(0.1, START_YEAR, END_YEAR)\n",
    "\n",
    "TARGET='PCEPI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are used to detect discontinued data or data which starts after the starting period.\n",
    "If an exogenous vatiable starts 6 months late or ends 6 months early, it will be **dropped**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startDate= datetime(START_YEAR,1,1).strftime('%m/%Y')\n",
    "startDate_plus_half_year=datetime(START_YEAR,6,1).strftime('%m/%Y')\n",
    "endDate= datetime(END_YEAR,12,1,1).strftime('%m/%Y')\n",
    "endDate_minus_half_year=datetime(END_YEAR,7,1).strftime('%m/%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Data:\n",
    "### Useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_float(x):\n",
    "    '''\n",
    "    This function converts a string into a float, where the original data has K,M,B as shorthand instead of writing the zeros (investing.com often has data of this form).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: string of format xY, where Y is a character and x is some float\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "\n",
    "    '''\n",
    "    #If input is already a float then return the input:\n",
    "    if type(x)== float:\n",
    "        return x\n",
    "    # convert K to 1000's\n",
    "    if x[-1]=='K':\n",
    "        return float(x[:-1])*1e3\n",
    "    # convert M to millions's\n",
    "    elif x[-1]=='M':\n",
    "        return float(x[:-1])*1e6\n",
    "    #Convert B to billions\n",
    "    elif x[-1]=='B':\n",
    "        return float(x[:-1])*1e9\n",
    "    #Return converted number\n",
    "    return float(x)\n",
    "\n",
    "def rename_cols(df, name):\n",
    "    '''\n",
    "    This functiion renames the columns of a dataframe of the format \"name\"_\"columnName\"\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: Pandas dataFrame which needs renaming of its columnns\n",
    "    name: name to use for renaming\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    void\n",
    "\n",
    "    '''\n",
    "\n",
    "    df.columns= list(map(lambda x: f'{name}_'+x,df))\n",
    "\n",
    "def remove_percent(x):\n",
    "    '''\n",
    "    This functiion removes the percent sign from a string (representing a percentage) and converts it into a float.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: string representatation of a percentage to be converted to a float\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float without percentage sign\n",
    "\n",
    "    '''\n",
    "    return float(x[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import fred data\n",
    "this data includes the target variable (PCEPI **and its varients**) and has been combined from fred: https://fred.stlouisfed.org/categories\n",
    "\n",
    "Fred kindly allows data to be combined into a data list, which can then be downloaded (instead of indivigually downloading each dataset and then combining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly data has been split into 3 CSV files and needs to be combined:\n",
    "\n",
    "#read dataframes:\n",
    "df1= pd.read_csv(datasetsPath/'monthly_1.csv', parse_dates=[0])\n",
    "df2= pd.read_csv(datasetsPath/'monthly_2.csv', parse_dates=[0])\n",
    "df3= pd.read_csv(datasetsPath/'monthly,_end_of_period.csv', parse_dates=[0])\n",
    "\n",
    "#Make a date-time index\n",
    "df1 = df1.set_index(['observation_date'],drop = True)\n",
    "df2 = df2.set_index(['observation_date'],drop = True)\n",
    "df3 = df3.set_index(['observation_date'],drop = True)\n",
    "\n",
    "#Combine Monthly data:\n",
    "combinedDf= pd.concat([df1,df2,df3],axis=1,join='outer')\n",
    "\n",
    "\n",
    "#Reanme and display combined data:\n",
    "rename_cols(combinedDf,'fred')\n",
    "display(combinedDf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fred quarterly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine quarterly data:\n",
    "\n",
    "fred_quartely_1= pd.read_csv(datasetsPath/'quarterly.csv', parse_dates=[0])\n",
    "fred_quartely_2= pd.read_csv(datasetsPath/'quarterly,_end_of_quarter.csv', parse_dates=[0])\n",
    "fred_quartely_3= pd.read_csv(datasetsPath/'quarterly,_end_of_period.csv', parse_dates=[0])\n",
    "\n",
    "# Set Date-time index\n",
    "fred_quartely_1 = fred_quartely_1.set_index(['observation_date'],drop = True)\n",
    "fred_quartely_2 = fred_quartely_2.set_index(['observation_date'],drop = True)\n",
    "fred_quartely_3 = fred_quartely_3.set_index(['observation_date'],drop = True)\n",
    "\n",
    "#Combine data:\n",
    "quarterly=pd.concat([fred_quartely_1,fred_quartely_2,fred_quartely_3],axis=1,join='outer')\n",
    "\n",
    "\n",
    "# Remove data which starts too late or ends to early (used to find the quarterly feature names so they font get removed for having too mant null values (nature of converting quarterly to monthly)):\n",
    "late_data=quarterly.loc[startDate:startDate_plus_half_year].isna().all().loc[quarterly.loc[startDate:startDate_plus_half_year].isna().all()==True].index\n",
    "quarterly.drop(late_data,axis=1,inplace=True)\n",
    "early_data=quarterly.loc[endDate_minus_half_year:endDate].isna().all().loc[quarterly.loc[endDate_minus_half_year:endDate].isna().all()==True].index\n",
    "quarterly.drop(early_data,axis=1,inplace=True)\n",
    "\n",
    "#Rename:\n",
    "rename_cols(quarterly,'fred')\n",
    "\n",
    "quarterlyCols=quarterly.columns # Save the column names\n",
    "\n",
    "#Combine with monthly data:\n",
    "combinedDf= pd.concat([combinedDf,quarterly],axis=1,join='outer')\n",
    "combinedDf= combinedDf.loc[startDate:endDate]\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the index to month/year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDf.set_index(pd.to_datetime(pd.Series(combinedDf.index)).dt.strftime('%m/%Y'),drop = True,inplace=True)\n",
    "combinedDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fred daily and weekly data\n",
    "**aggregates data for each month into one value**\n",
    "Achieves this by removing data which starts too late or ends too early (same as later), then takes the arithmetic mean for that month.\n",
    "\n",
    "**NOTE:** arithmetic mean should not be used for variables which involve change/percentage change/rates of change (geometric or harmonic means must be used instead). However for the sake of simplicity, all variables involving changes have been dropped excluding their levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read daily data\n",
    "dailyDf=pd.read_csv(datasetsPath/'daily.csv', parse_dates=[0])\n",
    "\n",
    "# Change to date-time index\n",
    "dailyDf = dailyDf.set_index(['observation_date'],drop = True)\n",
    "\n",
    "\n",
    "# Remove data that starts too late or ends too early:\n",
    "late_data=dailyDf.loc[startDate:startDate_plus_half_year].isna().all().loc[dailyDf.loc[startDate:startDate_plus_half_year].isna().all()==True].index\n",
    "dailyDf.drop(late_data,axis=1,inplace=True)\n",
    "early_data=dailyDf.loc[endDate_minus_half_year:endDate].isna().all().loc[dailyDf.loc[endDate_minus_half_year:endDate].isna().all()==True].index\n",
    "dailyDf.drop(early_data,axis=1,inplace=True)\n",
    "\n",
    "# Select the data with the dates of interest\n",
    "dailyDf= dailyDf.loc[startDate:endDate]\n",
    "\n",
    "# Remove features involving change:\n",
    "dropCols= []\n",
    "\n",
    "for i in dailyDf.columns:\n",
    "    if '_' in i:\n",
    "        dropCols.append(i)\n",
    "\n",
    "dailyDf.drop(dropCols,axis=1,inplace=True)\n",
    "\n",
    "# Take the mean of the month:\n",
    "dailyDf=dailyDf.resample('M').mean()\n",
    "\n",
    "# Rename columns\n",
    "rename_cols(dailyDf,'fred')\n",
    "\n",
    "display(dailyDf)\n",
    "dailyDf.set_index(pd.to_datetime(pd.Series(dailyDf.index)).dt.strftime('%m/%Y'),drop = True,inplace=True)\n",
    "\n",
    "# Combine data\n",
    "combinedDf=pd.merge(combinedDf,dailyDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read weekly data:\n",
    "weeklyDf=pd.read_csv(datasetsPath/'weekly,_ending_thursday.csv', parse_dates=[0])\n",
    "weeklyDf = weeklyDf.set_index(['observation_date'],drop = True)\n",
    "\n",
    "# Remove data that starts too late or ends too early:\n",
    "late_data=weeklyDf.loc[startDate:startDate_plus_half_year].isna().all().loc[weeklyDf.loc[startDate:startDate_plus_half_year].isna().all()==True].index\n",
    "weeklyDf.drop(late_data,axis=1,inplace=True)\n",
    "early_data=weeklyDf.loc[endDate_minus_half_year:endDate].isna().all().loc[weeklyDf.loc[endDate_minus_half_year:endDate].isna().all()==True].index\n",
    "weeklyDf.drop(early_data,axis=1,inplace=True)\n",
    "\n",
    "# Select the data with the dates of interest\n",
    "weeklyDf= weeklyDf.loc[startDate:endDate]\n",
    "\n",
    "# Remove features involving change:\n",
    "dropCols= []\n",
    "\n",
    "for i in weeklyDf.columns:\n",
    "    if '_' in i:\n",
    "        dropCols.append(i)\n",
    "\n",
    "weeklyDf.drop(dropCols,axis=1,inplace=True)\n",
    "\n",
    "# Take the mean of the month:\n",
    "weeklyDf=weeklyDf.resample('M').mean()\n",
    "\n",
    "# rename columns\n",
    "rename_cols(weeklyDf,'fred')\n",
    "\n",
    "display(weeklyDf)\n",
    "weeklyDf.set_index(pd.to_datetime(pd.Series(weeklyDf.index)).dt.strftime('%m/%Y'),drop = True,inplace=True)\n",
    "\n",
    "# combine:\n",
    "combinedDf=pd.merge(combinedDf,weeklyDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other exogenous Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crude oil:\n",
    "https://www.investing.com/commodities/crude-oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='CrudeOilWTI'\n",
    "\n",
    "#read data:\n",
    "tempDf= pd.read_csv(datasetsPath/f'{name}.csv', parse_dates=[0])\n",
    "\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y') # convert to just month and year\n",
    "#set date as index\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "#remove the order of magnitude symbols:\n",
    "tempDf['Vol.']=tempDf['Vol.'].apply(str_to_float)\n",
    "\n",
    "# Remove percent sign:\n",
    "tempDf['Change %']=tempDf['Change %'].apply(remove_percent)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brent oil:\n",
    "https://www.investing.com/commodities/brent-oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='BrentOil'\n",
    "\n",
    "tempDf= pd.read_csv(datasetsPath/f'{name}.csv', parse_dates=[0])\n",
    "\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "#remove the order of magnitude symbols:\n",
    "tempDf['Vol.']=tempDf['Vol.'].apply(str_to_float)\n",
    "\n",
    "# Remove percent sign:\n",
    "tempDf['Change %']=tempDf['Change %'].apply(remove_percent)\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Gas:\n",
    "https://www.investing.com/commodities/natural-gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='NaturalGas'\n",
    "\n",
    "tempDf= pd.read_csv(datasetsPath/f'{name}.csv', parse_dates=[0])\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "#remove the order of magnitude symbols:\n",
    "tempDf['Vol.']=tempDf['Vol.'].apply(str_to_float)\n",
    "# Remove percent sign:\n",
    "tempDf['Change %']=tempDf['Change %'].apply(remove_percent)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold:\n",
    "https://www.investing.com/commodities/gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='Gold'\n",
    "\n",
    "tempDf= pd.read_csv(datasetsPath/f'{name}.csv', parse_dates=[0])\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "#remove the order of magnitude symbols:\n",
    "tempDf['Vol.']=tempDf['Vol.'].apply(str_to_float)\n",
    "\n",
    "# Remove percent sign:\n",
    "tempDf['Change %']=tempDf['Change %'].apply(remove_percent)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver:\n",
    "https://www.investing.com/commodities/silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='Silver'\n",
    "\n",
    "tempDf= pd.read_csv(datasetsPath/f'{name}.csv', parse_dates=[0])\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "#remove the order of magnitude symbols:\n",
    "tempDf['Vol.']=tempDf['Vol.'].apply(str_to_float)\n",
    "\n",
    "# Remove percent sign:\n",
    "tempDf['Change %']=tempDf['Change %'].apply(remove_percent)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copper:\n",
    "https://www.investing.com/commodities/copper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='Copper'\n",
    "\n",
    "tempDf= pd.read_csv(datasetsPath/f'{name}.csv', parse_dates=[0])\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "#remove the order of magnitude symbols:\n",
    "tempDf['Vol.']=tempDf['Vol.'].apply(str_to_float)\n",
    "\n",
    "# Remove percent sign:\n",
    "tempDf['Change %']=tempDf['Change %'].apply(remove_percent)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Soybean:\n",
    "https://www.investing.com/commodities/us-soybeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='USSoybeans'\n",
    "\n",
    "tempDf= pd.read_csv(datasetsPath/f'{name}.csv', parse_dates=[0])\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "#remove the order of magnitude symbols:\n",
    "tempDf['Vol.']=tempDf['Vol.'].apply(str_to_float)\n",
    "\n",
    "# Remove percent sign:\n",
    "tempDf['Change %']=tempDf['Change %'].apply(remove_percent)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baltic Dry Index:\n",
    "https://uk.investing.com/indices/baltic-dry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='BalticDryIndex'\n",
    "\n",
    "tempDf= pd.read_csv(datasetsPath/f'{name}.csv', parse_dates=[0],dayfirst=True)\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "\n",
    "tempDf.drop('Vol.',axis=1,inplace=True)# vol. not used\n",
    "\n",
    "# Remove percent sign:\n",
    "tempDf['Change %']=tempDf['Change %'].apply(remove_percent)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nasdaq Composite\n",
    "https://www.investing.com/indices/nasdaq-composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name='NASDAQComposite'\n",
    "\n",
    "tempDf= pd.read_csv(datasetsPath/f'{name}.csv', parse_dates=[0],dayfirst=True)\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "tempDf.drop('Vol.',axis=1,inplace=True)# vol. not used\n",
    "\n",
    "# Remove percent sign:\n",
    "tempDf['Change %']=tempDf['Change %'].apply(remove_percent)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S&P 500\n",
    "https://www.investing.com/indices/us-spx-500-historical-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='SP500'\n",
    "\n",
    "tempDf= pd.read_csv(datasetsPath/f'{name}.csv', parse_dates=[0])\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "tempDf.drop('Vol.',axis=1,inplace=True) # vol. not used\n",
    "# Remove percent sign:\n",
    "tempDf['Change %']=tempDf['Change %'].apply(remove_percent)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOE VIX\n",
    "https://www.investing.com/indices/volatility-s-p-500-historical-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='CBOEVIX'\n",
    "\n",
    "tempDf= pd.read_csv(datasetsPath/f'{name}.csv', parse_dates=[0])\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "tempDf.drop('Vol.',axis=1,inplace=True) # vol. not used\n",
    "# Remove percent sign:\n",
    "tempDf['Change %']=tempDf['Change %'].apply(remove_percent)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### food price indices data\n",
    "\n",
    "https://www.fao.org/worldfoodsituation/foodpricesindex/en/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='food_price_indices_data_f'\n",
    "\n",
    "tempDf= pd.read_csv(datasetsPath/f'{name}.csv', parse_dates=[0],header=2,usecols=list(range(0,7)))\n",
    "tempDf.drop(0,axis=0,inplace=True)\n",
    "\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCI\n",
    "https://www.oecd.org/en/data/indicators/business-confidence-index-bci.html?oecdcontrol-b2a0dbca4d-var3=1974-06&oecdcontrol-b2a0dbca4d-var4=2024-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='BCI'\n",
    "\n",
    "tempDf= pd.read_csv(datasetsPath/f'{name}.csv', parse_dates=[0])\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "tempDf.sort_index(inplace=True)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gscpi\n",
    "https://www.newyorkfed.org/research/policy/gscpi#/interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='gscpi_data'\n",
    "\n",
    "tempDf= pd.read_excel(datasetsPath/f'{name}.xls',sheet_name='GSCPI_Monthly_Data',engine=None,skiprows=4,names=['Date','gscpi'],usecols=[0,1])\n",
    "\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UK policy uncertainty Index\n",
    "https://www.policyuncertainty.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='UK_Policy_Uncertainty_Data'\n",
    "\n",
    "tempDf= pd.read_excel(datasetsPath/f'{name}.xlsx',engine=None,skipfooter=1)\n",
    "\n",
    "tempDf['Date']=tempDf['year'].astype(str)+'/'+tempDf['month'].astype(str)\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "\n",
    "tempDf.drop(['year','month'],axis=1,inplace=True)\n",
    "\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US policy uncertainty Index\n",
    "https://www.policyuncertainty.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='US_Policy_Uncertainty_Data'\n",
    "\n",
    "tempDf= pd.read_excel(datasetsPath/f'{name}.xlsx',sheet_name='Main Index',engine=None,skipfooter=1)\n",
    "\n",
    "#Combine year and month columns to form a date:\n",
    "tempDf['Date']=tempDf['Year'].astype(str)+'/'+tempDf['Month'].astype(str)\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "\n",
    "tempDf.drop(['Year','Month'],axis=1,inplace=True)\n",
    "\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='US_Policy_Uncertainty_Data'\n",
    "\n",
    "tempDf= pd.read_excel(datasetsPath/f'{name}.xlsx',sheet_name='Components',engine=None,skipfooter=1)\n",
    "\n",
    "#Combine year and month columns to form a date:\n",
    "tempDf['Date']=tempDf['Year'].astype(str)+'/'+tempDf['Month'].astype(str)\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "\n",
    "tempDf.drop(['Year','Month'],axis=1,inplace=True)\n",
    "\n",
    "tempDf.set_index(['Date'],drop = True,inplace=True)\n",
    "\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Bank Commodity Price Data\n",
    "https://thedocs.worldbank.org/en/doc/5d903e848db1d1b83e0ec8f744e55570-0350012021/related/CMO-Historical-Data-Monthly.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='CMO-Historical-Data-Monthly'\n",
    "\n",
    "tempDf= pd.read_excel(datasetsPath/f'{name}.xlsx',sheet_name='Monthly Prices',engine=None,skiprows=4)\n",
    "tempDf.drop(0,axis=0,inplace=True)\n",
    "\n",
    "#Replace M with '/' to turn it into a valid date:\n",
    "tempDf.iloc[:,0]=tempDf.iloc[:,0].str.replace('M','/')\n",
    "tempDf.iloc[:,0]=pd.to_datetime(tempDf.iloc[:,0]).dt.strftime('%m/%Y')\n",
    "\n",
    "tempDf.set_index(tempDf.columns[0],drop = True,inplace=True)\n",
    "\n",
    "#Drop already used features (and ['Barley','Sorghum','Wheat, US SRW'] because they contain '…' with different encodings):\n",
    "tempDf.drop(['Crude oil, average','Crude oil, Brent','Crude oil, Dubai','Crude oil, WTI', 'Natural gas, US','Natural gas, Europe','Liquefied natural gas, Japan',\n",
    "             'Natural gas index','Soybeans','Copper','Gold','Silver','Barley','Sorghum','Wheat, US SRW'],axis=1,inplace=True)\n",
    "\n",
    "# turn '…' into NaN\n",
    "tempDf.replace('…',pd.NA,inplace=True)\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf.iloc[-1,-1])\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='CMO-Historical-Data-Monthly'\n",
    "\n",
    "tempDf= pd.read_excel(datasetsPath/f'{name}.xlsx',sheet_name='Monthly Indices',engine=None,skiprows=8, names=['Date','Total_Index','Energy','Non_Energy','Agriculture',\n",
    "                                                                                                              'Beverages','Food','Oils_and_meals','Grains','Other_foods','Raw_materials',\n",
    "                                                                                                              'Timber','Other_raw_materials','Fertilizers','Metals_minerals','Base metals', 'Precious_metals'])\n",
    "#replace 'M' with slash to turn it into a date:\n",
    "tempDf['Date']=tempDf['Date'].str.replace('M','/')\n",
    "\n",
    "tempDf['Date']=pd.to_datetime(tempDf['Date']).dt.strftime('%m/%Y')\n",
    "tempDf.set_index(tempDf.columns[0],drop = True,inplace=True)\n",
    "\n",
    "\n",
    "rename_cols(tempDf,name)\n",
    "display(tempDf)\n",
    "\n",
    "combinedDf=pd.merge(combinedDf,tempDf,how= 'left',left_index=True,right_index=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTSE 100\n",
    "https://uk.finance.yahoo.com/quote/%5EFTSE/\n",
    "\n",
    "(**Data contains errorous dates hence why it has been excluded**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicate values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDf.index.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove late/early data:\n",
    "Remove data that misses the first 6 months of `START_YEAR` **or** the last 6 months of `END_YEAR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_data=combinedDf.loc[startDate:startDate_plus_half_year].isna().all().loc[combinedDf.loc[startDate:startDate_plus_half_year].isna().all()==True].index\n",
    "for i in late_data:\n",
    "    print(i)\n",
    "\n",
    "combinedDf.drop(list(late_data),axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_data=combinedDf.loc[endDate_minus_half_year:endDate].isna().all().loc[combinedDf.loc[endDate_minus_half_year:endDate].isna().all()==True].index\n",
    "for i in early_data:\n",
    "    print(i)\n",
    "\n",
    "combinedDf.drop(list(early_data),axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cols=combinedDf.drop(quarterlyCols,axis=1).isna().sum(axis=0).loc[combinedDf.drop(quarterlyCols,axis=1).isna().sum(axis=0)>combinedDf.shape[0]//5]\n",
    "\n",
    "for i in null_cols.index:\n",
    "    print(i)\n",
    "display(null_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon further inspection for TB1YR, there is a big gap between 2001 and 2008. hence all data relating to this variable should be removed (https://fred.stlouisfed.org/series/TB1YR).\n",
    "\n",
    "For TERMCBAUTO48NS and TERMCBPER24NS the data doesnt seem to be released every month all the time henc the number of missing values, hence the levels can stay (and be interpolated) however percentage changes related to this variable should be dropped (https://fred.stlouisfed.org/series/TERMCBAUTO48NS \n",
    "\n",
    "https://fred.stlouisfed.org/series/TERMCBPER24NS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDf.drop([\n",
    "'fred_TB1YR'\n",
    ",'fred_TB1YR_LOG'\n",
    ",'fred_TB1YR_CH1'\n",
    ",'fred_TB1YR_PC1'\n",
    ",'fred_TERMCBAUTO48NS_CH1'\n",
    ",'fred_TERMCBAUTO48NS_LOG'\n",
    ",'fred_TERMCBAUTO48NS_PC1'\n",
    ",'fred_TERMCBPER24NS_CH1'\n",
    ",'fred_TERMCBPER24NS_LOG'\n",
    ",'fred_TERMCBPER24NS_PC1'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform linear interpolation for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDf.interpolate(method='linear',inplace=True)\n",
    "display(combinedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace null values (values that start with nulls, hench why they are still nulls) with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDf.fillna(0,inplace=True)\n",
    "display(combinedDf.isna().any().any())\n",
    "display(combinedDf['fred_TERMCBAUTO48NS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove commas (',') from numbers to make it a valid float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_cols=combinedDf.select_dtypes(include=[object]).columns\n",
    "\n",
    "for i in str_cols:\n",
    "\n",
    "    combinedDf[i]=combinedDf[i].str.replace(',','', regex=True)\n",
    "    combinedDf[i]\n",
    "    try:\n",
    "        combinedDf[i]=pd.to_numeric(combinedDf[i],errors='raise')\n",
    "    except:\n",
    "        display(combinedDf[i])\n",
    "    combinedDf[i].interpolate(method='linear',inplace=True)\n",
    "    combinedDf[i].fillna(0,inplace=True)\n",
    "\n",
    "combinedDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First remove other PCEPI information to prevent dataleakage:\n",
    "TargetCol=  combinedDf['fred_'+TARGET].copy()\n",
    "combinedDf.drop([\n",
    "'fred_PCEPI',\n",
    "'fred_PCEPI_CCA',\n",
    "'fred_PCEPI_CCH',\n",
    "'fred_PCEPI_CH1',\n",
    "'fred_PCEPI_CHG',\n",
    "'fred_PCEPI_PC1',\n",
    "'fred_PCEPI_PCA',\n",
    "'fred_PCEPI_PCH'],\n",
    "axis=1, inplace=True)\n",
    "\n",
    "X_trainDf= combinedDf.iloc[:combinedDf.index.get_loc(TEST_DATE),:]\n",
    "X_testDf= combinedDf.iloc[combinedDf.index.get_loc(TEST_DATE):,:]\n",
    "\n",
    "y_trainDf= TargetCol.iloc[:TargetCol.index.get_loc(TEST_DATE)]\n",
    "y_testDf= TargetCol.iloc[TargetCol.index.get_loc(TEST_DATE):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Granger Causality test:\n",
    "Granger causality test is used to determine if an exogenous variable causes (granger causes) inflation. This hypothesis test assumes the timeseries are stationary, therefore the timeseries should be proccessed accordingly.\n",
    "\n",
    "To test for stationarity, Augmented Dickey-Fuller test is used with a significance level of 5%.\n",
    "\n",
    "The Granger causality test also uses a significance level of 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_granger_caused(feature):\n",
    "\n",
    "    '''\n",
    "    This function returns True if feature granger causes target.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature: the name of the column within X_trainDf to test if it causes y_trainDf.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "\n",
    "    True: if feature granger causes target\n",
    "    False: if feature does NOT granger cause target.\n",
    "    '''\n",
    "\n",
    "    df_cpy= pd.concat((y_trainDf,X_trainDf[feature]),axis=1)# creates a deepcopy\n",
    "\n",
    "\n",
    "    i=1 # initialized to 1 as PCEPI is not sationary with no preproccessing\n",
    "\n",
    "    # peform adfuller test of xth differences untill both time series are stationary\n",
    "    while adfuller(df_cpy.diff(i).dropna().iloc[:,0])[1]>0.05 or adfuller(df_cpy.diff(i).dropna().iloc[:,1])[1]>0.05:\n",
    "        \n",
    "        i+=1\n",
    "\n",
    "        # Fail safe by keeping exogenous variable incase it by cause target, but may have a complex relationship\n",
    "        if i>4:\n",
    "            return True\n",
    "        \n",
    "    #Perform granger test:\n",
    "    test=grangercausalitytests(df_cpy.diff(i).dropna(),maxlag=6,verbose=False)\n",
    "\n",
    "    # see if there is  a time-lag which is granger caused:\n",
    "    for key in test:\n",
    "        \n",
    "        #Return true if granger caused\n",
    "        if test[key][0]['ssr_chi2test'][1]<0.05:\n",
    "            return True\n",
    "    \n",
    "    # Else return false NOT granger caused)\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keepCols=[]# keeps granger caused columns\n",
    "\n",
    "# Iterate over all columns and test for granger causality\n",
    "for i in X_trainDf.columns:\n",
    "    if is_granger_caused(i):\n",
    "        keepCols.append(i)\n",
    "    \n",
    "print(len(keepCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caused_train_df= pd.concat([y_trainDf,X_trainDf[keepCols]],axis=1)\n",
    "caused_test_df= pd.concat([y_testDf,X_testDf[keepCols]],axis=1)\n",
    "\n",
    "display(caused_train_df)\n",
    "display(caused_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caused_train_df.to_csv(workbookdir.parent / 'Data' /'Train'/f'trains{START_YEAR}s.csv')\n",
    "caused_test_df.to_csv(workbookdir.parent / 'Data' /'Test'/f'test{START_YEAR}s.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
