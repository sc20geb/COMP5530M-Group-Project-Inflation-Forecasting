{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Manually set the project root directory (adjust if needed)\n",
    "project_root = os.path.abspath(os.path.join('..', '..'))  # Moves up one level to project root\n",
    "# Add the project directory to sys.path\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/ Clean Model Weight Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.weightFileCleaner import cleanWeightFiles\n",
    "model_save_path = os.path.join('.')\n",
    "cleanWeightFiles('RNN', dirPath=model_save_path, earlyStopped=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# **Define relative file path for training data**\n",
    "train_file = os.path.join('..', '..', 'Data', 'Train', 'train1990s.csv')\n",
    "\n",
    "# **Load Training Data with Automatic Column Detection**\n",
    "train_df = pd.read_csv(train_file)\n",
    "#print(\"Columns in dataset:\", train_df.columns)  # Debugging: Show available columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = 'observation_date'\n",
    "target_col = 'fred_PCEPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.dataPreprocessing import sklearn_fit_transform\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "# **Normalize Data**\n",
    "# Perform min-max scaling on input data (no exogenous variables)\n",
    "train_dataframes, scaler = sklearn_fit_transform(pd.DataFrame(train_df[target_col]), MinMaxScaler())\n",
    "# Returns list of DataFrames, so extract correct DataFrame, then extract values, then reshape\n",
    "train_series = train_dataframes[0].values.reshape(1, -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.dataPreprocessing import create_sequences\n",
    "\n",
    "# **Set Sequence Length**\n",
    "# Create sequences from the training series\n",
    "sequence_length = 12\n",
    "X, y = create_sequences(train_series, train_series, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# **Train-Validation Split (80% Train, 20% Validation)**\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.dataPreprocessing import add_dimension\n",
    "\n",
    "# Reshape data as expected\n",
    "X_train, y_train, X_val, y_val = [add_dimension(dataset) for dataset in [X_train, y_train, X_val, y_val]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.dataPreprocessing import prepare_dataloader\n",
    "\n",
    "# **Create DataLoaders**\n",
    "batch_size = 32\n",
    "train_loader = prepare_dataloader(X_train, y_train, batch_size=batch_size, shuffle=False)\n",
    "val_loader = prepare_dataloader(X_val, y_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now import the RNN model\n",
    "from Models.RNN import RNNModel\n",
    "from Training.Helper.PyTorchModular import train_model\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# **Train Model Using Modular Functions**\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rnn_model = RNNModel(input_size=1, hidden_size=64, num_layers=2).to(device)\n",
    "\n",
    "# **Define Loss Function and Optimizer**\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# **Train the Model Using Modular Functions**\n",
    "train_data = train_model(\n",
    "    model=rnn_model,\n",
    "    maxEpochs=50,\n",
    "    modelSavePath=model_save_path,\n",
    "    modelName=\"RNN\",\n",
    "    dataLoaderTrain=train_loader,\n",
    "    dataLoaderValid=val_loader,\n",
    "    lossFn=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    batchStatusUpdate=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.PyTorchModular import train_model, loss_curve\n",
    "\n",
    "# **Plot Training vs. Validation Loss**\n",
    "loss_curve(trainLoss=train_data[\"trainLoss\"], validLoss=train_data[\"validLoss\"], title=\"RNN Training vs. Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation.Helper.evaluation_helpers import get_best_model_path, evaluate_model\n",
    "\n",
    "# Get path of the best model\n",
    "best_model_path = get_best_model_path(model_save_path, 'RNN')\n",
    "\n",
    "# **Extract the dates corresponding to the validation predictions**\n",
    "val_dates = train_df[date_col].iloc[len(X_train) + sequence_length:].values\n",
    "\n",
    "# Plot evaluation plot of the best model (loaded from path above) and get metrics\n",
    "eval_axes, metrics = evaluate_model(rnn_model, val_loader, scaler, val_dates, device, savepath=best_model_path, print_dates=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Display metrics for Validation Predictions**\n",
    "print(\"Metrics for RNN model:\")\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of weight file cleaner, uncomment if needed\n",
    "\n",
    "#from Training.Helper.weightFileCleaner import cleanWeightFiles\n",
    "#cleanWeightFiles('RNN', earlyStopped=True, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
