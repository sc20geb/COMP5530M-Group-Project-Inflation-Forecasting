{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBEATSx MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.fftpack import fft\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Load and Process Data\n",
    "def load_data(train_file, sequence_length=48):\n",
    "    df = pd.read_csv(train_file)\n",
    "    df[\"observation_date\"] = pd.to_datetime(df[\"observation_date\"], format=\"%m/%Y\")\n",
    "    df = df.sort_values(by=\"observation_date\").reset_index(drop=True)\n",
    "\n",
    "    target_col = \"fred_PCEPI\"\n",
    "    exogenous_cols = [col for col in df.columns if col not in [\"observation_date\", target_col]]\n",
    "\n",
    "    data = df[[target_col]].values.astype(np.float32)\n",
    "    exogenous_data = df[exogenous_cols].values.astype(np.float32)\n",
    "\n",
    "    # Train-Validation-Test Split\n",
    "    train_idx = int(len(data) * 0.6)\n",
    "    valid_idx = int(len(data) * 0.8)\n",
    "\n",
    "    train_data, valid_data, test_data = data[:train_idx], data[train_idx:valid_idx], data[valid_idx:]\n",
    "    train_exog, valid_exog, test_exog = exogenous_data[:train_idx], exogenous_data[train_idx:valid_idx], exogenous_data[valid_idx:]\n",
    "\n",
    "    # Apply Scaling Separately\n",
    "    scaler = MinMaxScaler()\n",
    "    train_data = scaler.fit_transform(train_data)\n",
    "    valid_data = scaler.transform(valid_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "\n",
    "    exog_scaler = StandardScaler()\n",
    "    train_exog = exog_scaler.fit_transform(train_exog)\n",
    "    valid_exog = exog_scaler.transform(valid_exog)\n",
    "    test_exog = exog_scaler.transform(test_exog)\n",
    "\n",
    "    y_scaler = MinMaxScaler()\n",
    "    train_target = y_scaler.fit_transform(train_data)\n",
    "    valid_target = y_scaler.transform(valid_data)\n",
    "    test_target = y_scaler.transform(test_data)\n",
    "\n",
    "    # Apply PCA on Exogenous Data\n",
    "    pca = PCA(n_components=min(5, train_exog.shape[1]))\n",
    "    train_exog = pca.fit_transform(train_exog)\n",
    "    valid_exog = pca.transform(valid_exog)\n",
    "    test_exog = pca.transform(test_exog)\n",
    "\n",
    "    # Apply Fourier Features\n",
    "    def apply_fft_per_sequence(data, seq_len):\n",
    "        fft_real, fft_imag = [], []\n",
    "        for i in range(len(data) - seq_len):\n",
    "            fft_transformed = fft(data[i : i + seq_len].flatten())\n",
    "            fft_real.append(np.log1p(np.abs(np.real(fft_transformed[:seq_len]))))\n",
    "            fft_imag.append(np.log1p(np.abs(np.imag(fft_transformed[:seq_len]))))\n",
    "        return np.array(fft_real), np.array(fft_imag)\n",
    "\n",
    "    train_fft_real, train_fft_imag = apply_fft_per_sequence(train_data, sequence_length)\n",
    "    valid_fft_real, valid_fft_imag = apply_fft_per_sequence(valid_data, sequence_length)\n",
    "    test_fft_real, test_fft_imag = apply_fft_per_sequence(test_data, sequence_length)\n",
    "\n",
    "    # Create Sequences\n",
    "    def create_sequences(data, exog, fft_real, fft_imag, target, seq_len):\n",
    "        X, X_exog, y = [], [], []\n",
    "        for i in range(len(data) - seq_len):\n",
    "            X.append(np.hstack([data[i : i + seq_len].flatten(), fft_real[i], fft_imag[i]]))  \n",
    "            X_exog.append(exog[i])  # Only past exogenous variables\n",
    "            y.append(target[i + seq_len])  \n",
    "        return np.array(X), np.array(X_exog), np.array(y)\n",
    "\n",
    "    X_train, X_exog_train, y_train = create_sequences(train_data, train_exog, train_fft_real, train_fft_imag, train_target, sequence_length)\n",
    "    X_valid, X_exog_valid, y_valid = create_sequences(valid_data, valid_exog, valid_fft_real, valid_fft_imag, valid_target, sequence_length)\n",
    "    X_test, X_exog_test, y_test = create_sequences(test_data, test_exog, test_fft_real, test_fft_imag, test_target, sequence_length)\n",
    "\n",
    "    return X_train, X_exog_train, y_train, X_valid, X_exog_valid, y_valid, X_test, X_exog_test, y_test, df[\"observation_date\"][sequence_length:], scaler, exog_scaler, y_scaler\n",
    "\n",
    "# Define N-BEATSx Model\n",
    "class NBeats(nn.Module):\n",
    "    def __init__(self, input_size, exog_size, num_blocks=5, theta_size=1):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_size + exog_size, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, theta_size)\n",
    "            ) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, exog):\n",
    "        forecast = torch.zeros_like(x[:, -1:], dtype=torch.float32)\n",
    "        for block in self.blocks:\n",
    "            forecast += block(torch.cat((x, exog), dim=1))\n",
    "        return forecast\n",
    "\n",
    "# Train Model\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, exog_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch, exog_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.6f}')\n",
    "\n",
    "# Plot Predictions\n",
    "def plot_results(dates, actual_values, predictions):\n",
    "    predictions_smoothed = gaussian_filter1d(predictions, sigma=2)  \n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(dates, actual_values, label='Actual PCE', marker='o', linestyle='dashed', color='blue')\n",
    "    plt.plot(dates, predictions_smoothed, label='Predicted PCE (N-BEATSx)', marker='x', linestyle='solid', color='orange')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Actual vs. Predicted PCE Inflation (N-BEATSx)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"PCE Index\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Final Steps\n",
    "train_file = \"../Data/Train/train1990s.csv\"\n",
    "X_train, X_exog_train, y_train, X_valid, X_exog_valid, y_valid, X_test, X_exog_test, y_test, dates, scaler, exog_scaler, y_scaler = load_data(train_file)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "X_exog_test_tensor = torch.tensor(X_exog_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Train and Predict\n",
    "model = NBeats(input_size=X_train.shape[1], exog_size=X_exog_train.shape[1])\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.HuberLoss()\n",
    "train_model(model, DataLoader(TensorDataset(X_test_tensor, X_exog_test_tensor, y_test_tensor), batch_size=128, shuffle=False), criterion, optimizer)\n",
    "\n",
    "# Plot Results\n",
    "predictions = model(X_test_tensor, X_exog_test_tensor).detach().cpu().numpy()\n",
    "predictions = y_scaler.inverse_transform(predictions)\n",
    "plot_results(dates[-len(predictions):], y_scaler.inverse_transform(y_test), predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Plot Actual vs. Predicted PCE Inflation\n",
    "def plot_predictions(dates, actual_values, predictions):\n",
    "    predictions_smoothed = gaussian_filter1d(predictions, sigma=2)  # Apply slight smoothing\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(dates, actual_values, label='Actual PCE', marker='o', linestyle='dashed', color='blue')\n",
    "    plt.plot(dates, predictions_smoothed, label='Predicted PCE (N-BEATSx)', marker='x', linestyle='solid', color='orange')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Actual vs. Predicted PCE Inflation (N-BEATSx)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"PCE Index\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Run the Plot\n",
    "y_true = y_scaler.inverse_transform(y_test).flatten()  # Convert back to original scale\n",
    "y_pred = y_scaler.inverse_transform(predictions).flatten()\n",
    "\n",
    "plot_predictions(dates[-len(y_true):], y_true, y_pred)\n",
    "\n",
    "# Plot Prediction Error Trend\n",
    "def plot_error_trend(dates, y_true, y_pred):\n",
    "    errors = y_true - y_pred\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(dates, errors, marker='o', linestyle='-', color='red', label=\"Prediction Error\")\n",
    "    plt.axhline(y=0, color='black', linestyle='dashed', linewidth=1)\n",
    "    plt.title(\"N-BEATSx Model Error Trend Over Time\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Prediction Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_error_trend(dates[-len(y_true):], y_true, y_pred)\n",
    "\n",
    "# Plot Error Distribution\n",
    "def plot_error_distribution(y_true, y_pred):\n",
    "    errors = y_true - y_pred\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.histplot(errors, bins=30, kde=True, color='red')\n",
    "    plt.title(\"Error Distribution (Prediction - Actual)\")\n",
    "    plt.xlabel(\"Prediction Error\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_error_distribution(y_true, y_pred)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Evaluate Model Performance\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # Percentage\n",
    "    print(\" Model Evaluation Metrics: \")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "\n",
    "evaluate_model(y_true, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
