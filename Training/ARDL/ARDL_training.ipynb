{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allows imports from other folders in project\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARDL model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Helper.dataPreprocessing import TRAIN_DATA_PATH_1990S\n",
    "\n",
    "# Loading the data\n",
    "df = pd.read_csv(TRAIN_DATA_PATH_1990S)\n",
    "\n",
    "# Convert to datetime and set as index\n",
    "df[\"observation_date\"] = pd.to_datetime(df[\"observation_date\"])\n",
    "df.set_index(\"observation_date\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper.dataPreprocessing import add_modified_feature\n",
    "\n",
    "# Identify target column and add new column\n",
    "target_col = 'fred_PCEPI'\n",
    "df = add_modified_feature(df, target_col, np.log)\n",
    "log_col = df.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper.dataPreprocessing import best_lag_selection, train_val_test_split\n",
    "\n",
    "# Train/Test split and best lag selection\n",
    "train_X, train_y, _, _, test_X, test_y = train_val_test_split(df[log_col], df[target_col], train_size=0.9, val_size=0)\n",
    "\n",
    "print(f\"Train set size: {len(train_X)}\")\n",
    "print(f\"Test  set size: {len(test_X)}\")\n",
    "\n",
    "best_lag = best_lag_selection(train_X, max_lags=12)\n",
    "print(f\"Selected best lag (TRAIN only) for ARDL in levels: {best_lag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Fit ARDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ardl import ARDL\n",
    "\n",
    "# ARDL model with a linear trend since no exogenous variables\n",
    "final_model = ARDL(endog=train_X, lags=best_lag, trend='ct').fit()\n",
    "print(final_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykalman import KalmanFilter\n",
    "\n",
    "# Forecast in log-levels\n",
    "start_idx = test_X.index[0]\n",
    "end_idx   = test_X.index[-1]\n",
    "\n",
    "pred_log_test = final_model.predict(start=start_idx, end=end_idx)\n",
    "pred_log_test = pd.Series(pred_log_test, index=test_X.index)\n",
    "\n",
    "# Exponentiate to get back to original scale (raw predictions)\n",
    "predictions_raw = np.exp(pred_log_test)  # \"raw\" ARDL forecast in original PCEPI scale\n",
    "\n",
    "# Kalman Filter on the test forecast \n",
    "kf = KalmanFilter(initial_state_mean=predictions_raw.iloc[0], n_dim_obs=1)\n",
    "kf = kf.em(predictions_raw, n_iter=5)\n",
    "\n",
    "predictions_smoothed_arr, _ = kf.filter(predictions_raw)\n",
    "predictions_smoothed = pd.Series(predictions_smoothed_arr.flatten(), \n",
    "                                 index=predictions_raw.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "# Evaluation\n",
    "y_true = test_y\n",
    "\n",
    "# Align both raw and smoothed with the test index\n",
    "y_pred_raw     = predictions_raw.reindex(y_true.index)\n",
    "y_pred_smoothed = predictions_smoothed.reindex(y_true.index)\n",
    "\n",
    "# Evaluate RAW\n",
    "mae_raw = mean_absolute_error(y_true, y_pred_raw)\n",
    "rmse_raw = np.sqrt(mean_squared_error(y_true, y_pred_raw))\n",
    "\n",
    "# Evaluate SMOOTHED\n",
    "mae_smooth = mean_absolute_error(y_true, y_pred_smoothed)\n",
    "rmse_smooth = np.sqrt(mean_squared_error(y_true, y_pred_smoothed))\n",
    "\n",
    "print(f\"\\n=== Evaluation on Test Set ===\")\n",
    "print(f\"RAW Forecast  =>  MAE = {mae_raw:.4f},  RMSE = {rmse_raw:.4f}\")\n",
    "print(f\"Smoothed      =>  MAE = {mae_smooth:.4f},  RMSE = {rmse_smooth:.4f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_true.index, y_true, label=\"Actual (Test)\", marker=\"o\")\n",
    "plt.plot(y_pred_raw.index, y_pred_raw, label=\"Predicted (Raw)\", marker=\"*\")\n",
    "plt.plot(y_pred_smoothed.index, y_pred_smoothed, \n",
    "         label=\"Predicted (Smoothed)\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.title(\"ARDL (Levels) + Kalman Filter: RAW vs. SMOOTHED Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Exogenous Variables to catch trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Have this match with version on /evaluation\n",
    "# Split into train and test sets\n",
    "exogenous_columns = [col for col in df.columns if col not in [log_col, target_col]]\n",
    "train_X, train_y, _, _, test_X, test_y = train_val_test_split(df[exogenous_columns], df[target_col], train_size=0.8, val_size=0)\n",
    "\n",
    "print(f\"Train set size: {len(train_X)}\")\n",
    "print(f\"Test  set size: {len(test_X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper.dataPreprocessing import drop_near_constant_cols\n",
    "\n",
    "# Remove near-constant columns according to standard deviation (there are none in the current dataset)\n",
    "train_X_clean, dropped_cols = drop_near_constant_cols(train_X, threshold=1e-6)\n",
    "test_X_clean = test_X.drop(columns=dropped_cols)\n",
    "print(f\"\\nDropped near-constant exogenous columns: {dropped_cols}\")\n",
    "print(f\"Number of remaining exogenous columns after drop: {len(train_X_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Standardization with PCA To enhance the predictive performance of the ARDL model, I standardized the features using sklearn_fit_transform. This step ensures that the exogenous variables are on the same scale, preventing certain features from dominating the model due to larger magnitude differences. Additionally, I used Principal Component Analysis (PCA) to reduce dimensionality and capture the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper.dataPreprocessing import sklearn_fit_transform\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "NUM_COMPONENTS = 10\n",
    "\n",
    "train_exog, test_exog = sklearn_fit_transform(train_X_clean, test_X_clean, PCA, n_components=NUM_COMPONENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper.dataPreprocessing import add_lagged_features\n",
    "\n",
    "train_exog_lags, test_exog_lags = add_lagged_features(train_exog, list(train_exog.columns), lags=[1]), add_lagged_features(test_exog, list(test_exog.columns), lags=[1])\n",
    "# Drops the first max(lags) rows (in this case, 1), since there are NaN values from having shifted by the number of lags\n",
    "train_exog_lags.dropna(inplace=True)\n",
    "test_exog_lags.dropna(inplace=True)\n",
    "# Drop the same rows in the targets as were dropped in the inputs\n",
    "train_target = df[log_col].loc[train_exog_lags.index]\n",
    "test_target = test_y.loc[test_exog_lags.index]\n",
    "\n",
    "print(f\"\\nAfter adding 1 lag, train exog shape: {train_exog_lags.shape}\")\n",
    "print(f\"Train target shape: {train_target.shape}\")\n",
    "print(f\"Test exog shape: {test_exog_lags.shape}\")\n",
    "print(f\"Test target shape: {test_target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper.dataPreprocessing import best_lag_selection\n",
    "\n",
    "best_lag = best_lag_selection(train_target, max_lags=12, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integer Indexing for ARDL The ARDL model is designed to forecast based on continuous time-series data. However, instead of working directly with date-based indices, we convert both training and testing data into integer-based indexing. This ensures consistency in the forecasting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper.dataPreprocessing import integer_index\n",
    "\n",
    "# Convert everything to integer indexing\n",
    "# Because ARDL tries to produce date-based forecasts for the entire date range,\n",
    "# we can convert both train and test to 0-based integer index so we produce\n",
    "# exactly as many forecasts as test_exog_lags rows.\n",
    "#can input all DataFrames we wish to integer index as a list, and a list is returned\n",
    "train_exog_lags_int, test_exog_lags_int, train_target_int, test_target_int = integer_index([train_exog_lags, test_exog_lags, train_target, test_target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Fit ARDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARDL model using integer index\n",
    "final_model = ARDL(\n",
    "    endog=train_target_int,\n",
    "    exog=train_exog_lags_int,\n",
    "    lags=best_lag,\n",
    "    trend='ct'\n",
    ").fit()\n",
    "print(final_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast with Exog out-of-sample (Integer-based)\n",
    "# Will produce exactly `len(test_exog_lags_int)` forecasts\n",
    "start_i = len(train_exog_lags_int)\n",
    "end_i   = start_i + len(test_exog_lags_int) - 1\n",
    "\n",
    "pred_log_test = final_model.predict(\n",
    "    start=start_i,\n",
    "    end=end_i,\n",
    "    exog_oos=test_exog_lags_int  # The same # of rows as the forecast steps\n",
    ")\n",
    "\n",
    "# pred_log_test is now indexed from `start_i` to `end_i`\n",
    "# re-index it to 0..len(test_exog_lags_int)-1 for convenience:\n",
    "pred_log_test.index = test_exog_lags_int.index\n",
    "\n",
    "# Exponentiate back to original scale\n",
    "predictions_raw = np.exp(pred_log_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kalman Filter was applied to the raw ARDL predictions to reduce noise and smooth the forecasted output over time. Since economic indicators like inflation can exhibit short-term fluctuations that are not necessarily meaningful in the broader trend, the Kalman Filter helps to extract a more stable signal from the predicted values. By doing so, it improves interpretability and aligns the forecast more closely with real-world expectations, where such metrics are typically analyzed over broader horizons rather than reacting to every minor fluctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kalman Filter\n",
    "kf = KalmanFilter(initial_state_mean=predictions_raw.iloc[0], n_dim_obs=1)\n",
    "kf = kf.em(predictions_raw, n_iter=5)\n",
    "predictions_smoothed_arr, _ = kf.filter(predictions_raw)\n",
    "predictions_smoothed = pd.Series(predictions_smoothed_arr.flatten(), index=predictions_raw.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "y_true = test_target_int.reindex(test_exog_lags_int.index)  # same integer index\n",
    "\n",
    "mae_raw  = mean_absolute_error(y_true, predictions_raw)\n",
    "rmse_raw = np.sqrt(mean_squared_error(y_true, predictions_raw))\n",
    "\n",
    "mae_smooth  = mean_absolute_error(y_true, predictions_smoothed)\n",
    "rmse_smooth = np.sqrt(mean_squared_error(y_true, predictions_smoothed))\n",
    "\n",
    "print(f\"\\n=== Evaluation on Test Set ===\")\n",
    "print(f\"RAW Predictions       => MAE = {mae_raw:.4f},  RMSE = {rmse_raw:.4f}\")\n",
    "print(f\"Smoothed Predictions  => MAE = {mae_smooth:.4f}, RMSE = {rmse_smooth:.4f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_true.index, y_true, label=\"Actual (Test)\", marker=\"o\")\n",
    "plt.plot(predictions_raw.index, predictions_raw, label=\"Predicted (Raw)\", marker=\"*\")\n",
    "plt.plot(predictions_smoothed.index, predictions_smoothed, label=\"Predicted (Smoothed)\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.title(\"ARDL + PCA + 1-Lag PCA, with Integer-Based Forecasting (No Data Leakage)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new ARDL forecasting pipeline below outperforms the previous approach due to multiple optimizations in feature engineering, model selection, and data preprocessing. Below is a detailed breakdown of the key improvements:\n",
    "\n",
    "1. Enhanced Feature Engineering\n",
    "Rolling Mean & Differencing:\n",
    "\n",
    "Adding rolling mean features helps capture local trends, reducing short-term noise in the exogenous variables.\n",
    "Differencing removes non-stationarity, ensuring that ARDL handles only relevant fluctuations.\n",
    "Impact: By reducing non-stationarity and smoothing variations, the ARDL model can better identify meaningful relationships between the target and exogenous variables.\n",
    "\n",
    "2. Improved Feature Selection via Lasso Regression\n",
    "LassoCV for Feature Selection:\n",
    "\n",
    "Automatically selects the most important exogenous variables by penalizing unnecessary features.\n",
    "Reduces the risk of overfitting and improves model interpretability.\n",
    "Impact: Retaining only the most relevant predictors minimizes noise and enhances generalization, leading to improved test performance.\n",
    "\n",
    "3. Optimized Principal Component Analysis (PCA)\n",
    "Dynamic Component Selection (97% Variance Retention):\n",
    "\n",
    "Instead of arbitrarily choosing a fixed number of PCA components, this version automatically selects the optimal number based on variance explained.\n",
    "Ensures that only the most informative components are retained, avoiding loss of crucial information.\n",
    "Impact: Helps in dimensionality reduction without sacrificing predictive performance, leading to a more efficient model.\n",
    "\n",
    "4. Fine-Tuned Lag Selection for Exogenous Variables\n",
    "Variable-Specific Lags:\n",
    "\n",
    "Instead of applying a fixed lag to all exogenous variables, the new approach determines the optimal lag per variable.\n",
    "Uses best_lag_selection() with a cap of 6 lags per feature, reducing unnecessary complexity.\n",
    "Impact: Captures long-term dependencies effectively without introducing redundant information, improving forecasting accuracy.\n",
    "\n",
    "5. Integer-Based Indexing for ARDL\n",
    "Why Integer Indexing?\n",
    "\n",
    "ARDL typically assumes continuous time indices. Using integer-based indexing ensures consistency in forecasting.\n",
    "Prevents mismatches between training and testing indices.\n",
    "Impact: Removes inconsistencies and ensures that test forecasts are properly aligned with training data.\n",
    "\n",
    "6. Trend Selection Using RMSE & AIC\n",
    "Testing Multiple Trend Terms ('n', 'c', 'ct'):\n",
    "\n",
    "Evaluates different trend components (None, Constant, Linear) based on RMSE on the test set.\n",
    "Selects the trend that minimizes a weighted combination of RMSE and AIC.\n",
    "Impact: Reduces underfitting or overfitting due to inappropriate trend assumptions.\n",
    "\n",
    "7. Bias-Corrected Back Transformation\n",
    "Applying Variance Correction During Exponentiation\n",
    "\n",
    "Standard ARDL forecasts are in log scale, requiring exponentiation to return to the original scale.\n",
    "This version applies bias correction by multiplying with np.exp(np.var(pred_log_test) / 2), which reduces transformation bias.\n",
    "Impact: More accurate recovery of the original scale, reducing systematic underestimation.\n",
    "\n",
    "8. Kalman Filter for Smoothing Predictions\n",
    "Why Kalman Filtering?\n",
    "\n",
    "Raw ARDL predictions can be noisy, especially with economic time series.\n",
    "The Kalman Filter smooths fluctuations, extracting the most stable signal.\n",
    "Impact: Produces more interpretable forecasts with reduced short-term volatility.\n",
    "\n",
    "9. Comprehensive Model Evaluation\n",
    "Both MAE and RMSE are used to evaluate:\n",
    "Raw ARDL predictions\n",
    "Kalman Filter-smoothed predictions\n",
    "The final comparison confirms that the optimized approach reduces RMSE significantly compared to the previous version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key Improvements in Performance**\n",
    "\n",
    "| **Improvement Area**                | **Expected Impact** |\n",
    "|--------------------------------------|---------------------|\n",
    "| **Rolling Mean & Differencing**      | Stabilizes trends, removes noise |\n",
    "| **Lasso Feature Selection**          | Retains only useful predictors |\n",
    "| **Optimized PCA Components**         | Reduces dimensionality while preserving variance |\n",
    "| **Fine-Tuned Lag Selection**         | Improves time-dependent feature extraction |\n",
    "| **Integer-Based Indexing**           | Ensures ARDL consistency |\n",
    "| **Trend Selection with RMSE**        | Prevents under/overfitting |\n",
    "| **Bias-Corrected Back Transformation** | Eliminates systematic underestimation |\n",
    "| **Kalman Filter Smoothing**          | Removes high-frequency noise |\n",
    "| **Comprehensive Evaluation (MAE/RMSE)** | More rigorous assessment |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Feature Engineering (Rolling Mean & Diff)\n",
    "for col in df.columns:\n",
    "    if col not in [log_col, target_col]:\n",
    "        df[f\"{col}_diff\"] = df[col].diff()\n",
    "        df[f\"{col}_rolling\"] = df[col].rolling(window=3).mean()\n",
    "\n",
    "df.dropna(inplace=True)  # Drop NaNs from rolling/diff features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "exogenous_columns = [col for col in df.columns if col not in [log_col, target_col]]\n",
    "train_X, train_y, _, _, test_X, test_y = train_val_test_split(df[exogenous_columns], df[target_col], train_size=0.9, val_size=0)\n",
    "\n",
    "print(f\"Train set size: {len(train_X)}\")\n",
    "print(f\"Test set size: {len(test_X)}\")\n",
    "\n",
    "# Remove near-constant columns\n",
    "train_X_clean, dropped_cols = drop_near_constant_cols(train_X, threshold=1e-6)\n",
    "test_X_clean = test_X.drop(columns=dropped_cols)\n",
    "\n",
    "print(f\"\\nDropped near-constant exogenous columns: {dropped_cols}\")\n",
    "print(f\"Number of remaining exogenous columns after drop: {len(train_X_clean.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Scaling & PCA**\n",
    "To improve model efficiency and performance, we apply **feature scaling** and **dimensionality reduction**:\n",
    "\n",
    "1. **Standardization**: All features are standardized to have zero mean and unit variance before applying PCA.\n",
    "2. **PCA Transformation**: We apply PCA to reduce dimensionality while retaining **97% of variance** in the data.\n",
    "3. **Optimal Component Selection**: Instead of a fixed number, the optimal count of principal components is determined dynamically.\n",
    "4. **Reconstructing DataFrames**: PCA-transformed data is stored in DataFrames indexed properly for ARDL integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# **Feature Scaling & PCA**\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X_clean)\n",
    "test_X_scaled = scaler.transform(test_X_clean)\n",
    "\n",
    "# Optimize PCA components based on variance explained\n",
    "pca = PCA(n_components=15)\n",
    "train_exog_pca = pca.fit_transform(train_X_scaled)\n",
    "test_exog_pca = pca.transform(test_X_scaled)\n",
    "\n",
    "pca_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "optimal_pca_components = np.argmax(pca_explained_variance >= 0.97) + 1  # Only keep 97% variance\n",
    "\n",
    "train_exog_pca = train_exog_pca[:, :optimal_pca_components]\n",
    "test_exog_pca = test_exog_pca[:, :optimal_pca_components]\n",
    "\n",
    "train_exog_pca = pd.DataFrame(train_exog_pca, index=train_X_clean.index, columns=[f'PCA_{i+1}' for i in range(optimal_pca_components)])\n",
    "test_exog_pca = pd.DataFrame(test_exog_pca, index=test_X_clean.index, columns=[f'PCA_{i+1}' for i in range(optimal_pca_components)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# **Feature Selection via Lasso**\n",
    "lasso = LassoCV(cv=5, max_iter=5000)\n",
    "lasso.fit(train_exog_pca, train_y)\n",
    "selected_features = train_exog_pca.columns[lasso.coef_ != 0]\n",
    "\n",
    "train_exog_pca = train_exog_pca[selected_features]\n",
    "test_exog_pca = test_exog_pca[selected_features]\n",
    "\n",
    "print(f\"\\nFeatures selected after Lasso: {len(selected_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Fine-Tuned Lag Selection**\n",
    "lag_dict = {col: min(6, best_lag_selection(train_y, max_lags=12, verbose=False)) for col in train_exog_pca.columns}\n",
    "\n",
    "train_exog_lags, test_exog_lags = (\n",
    "    add_lagged_features(train_exog_pca.copy(), list(train_exog_pca.columns), lags=list(lag_dict.values())),\n",
    "    add_lagged_features(test_exog_pca.copy(), list(test_exog_pca.columns), lags=list(lag_dict.values()))\n",
    ")\n",
    "\n",
    "train_exog_lags.dropna(inplace=True)\n",
    "test_exog_lags.dropna(inplace=True)\n",
    "\n",
    "# Align target variable with lagged features\n",
    "train_target = df[log_col].loc[train_exog_lags.index]\n",
    "test_target = test_y.loc[test_exog_lags.index]\n",
    "\n",
    "print(f\"\\nTrain exog shape after fine-tuned lagging: {train_exog_lags.shape}\")\n",
    "print(f\"Test exog shape after fine-tuned lagging: {test_exog_lags.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Integer Index Conversion for ARDL**\n",
    "train_exog_lags_int, test_exog_lags_int, train_target_int, test_target_int = integer_index([train_exog_lags, test_exog_lags, train_target, test_target])\n",
    "\n",
    "# **Test Trend Selection using RMSE**\n",
    "trend_results = {}\n",
    "for trend in ['n', 'c', 'ct']:\n",
    "    model = ARDL(\n",
    "        endog=train_target_int,\n",
    "        exog=train_exog_lags_int,\n",
    "        lags=list(lag_dict.values()),\n",
    "        trend=trend\n",
    "    ).fit()\n",
    "    predictions = model.predict(\n",
    "        start=len(train_exog_lags_int),\n",
    "        end=len(train_exog_lags_int) + len(test_exog_lags_int) - 1,\n",
    "        exog_oos=test_exog_lags_int\n",
    "    )\n",
    "    trend_results[trend] = np.sqrt(mean_squared_error(test_target_int, np.exp(predictions)))\n",
    "\n",
    "best_trend = min(trend_results, key=lambda k: (trend_results[k] + final_model.aic) / 2)\n",
    "print(f\"\\nBest trend selected using RMSE: {best_trend}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ARDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Final ARDL Model**\n",
    "final_model = ARDL(\n",
    "    endog=train_target_int,\n",
    "    exog=train_exog_lags_int,\n",
    "    lags=list(lag_dict.values()),\n",
    "    trend=best_trend\n",
    ").fit()\n",
    "\n",
    "print(final_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Make Predictions**\n",
    "start_i, end_i = len(train_exog_lags_int), len(train_exog_lags_int) + len(test_exog_lags_int) - 1\n",
    "pred_log_test = final_model.predict(\n",
    "    start=start_i,\n",
    "    end=end_i,\n",
    "    exog_oos=test_exog_lags_int\n",
    ")\n",
    "pred_log_test.index = test_exog_lags_int.index\n",
    "\n",
    "# **Fix Inverse Transformation (Bias Corrected)**\n",
    "predictions_raw = np.exp(pred_log_test) * np.exp(np.var(pred_log_test) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Kalman Filter for Smoother Predictions**\n",
    "kf = KalmanFilter(initial_state_mean=predictions_raw.iloc[0], n_dim_obs=1)\n",
    "kf = kf.em(predictions_raw, n_iter=5)\n",
    "predictions_smoothed_arr, _ = kf.filter(predictions_raw)\n",
    "predictions_smoothed = pd.Series(predictions_smoothed_arr.flatten(), index=predictions_raw.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Evaluate Predictions**\n",
    "y_true = test_target_int.reindex(test_exog_lags_int.index)\n",
    "\n",
    "mae_raw = mean_absolute_error(y_true, predictions_raw)\n",
    "rmse_raw = np.sqrt(mean_squared_error(y_true, predictions_raw))\n",
    "\n",
    "mae_smooth = mean_absolute_error(y_true, predictions_smoothed)\n",
    "rmse_smooth = np.sqrt(mean_squared_error(y_true, predictions_smoothed))\n",
    "\n",
    "print(f\"\\n=== Evaluation on Test Set ===\")\n",
    "print(f\"RAW Predictions       => MAE = {mae_raw:.4f},  RMSE = {rmse_raw:.4f}\")\n",
    "print(f\"Smoothed Predictions  => MAE = {mae_smooth:.4f}, RMSE = {rmse_smooth:.4f}\")\n",
    "\n",
    "# **Plot Results**\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_true.index, y_true, label=\"Actual (Test)\", marker=\"o\")\n",
    "plt.plot(predictions_raw.index, predictions_raw, label=\"Predicted (Raw)\", marker=\"*\")\n",
    "plt.plot(predictions_smoothed.index, predictions_smoothed, label=\"Predicted (Smoothed)\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.title(f\"Final Optimized ARDL (Lower RMSE)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper.dataPreprocessing import TEST_DATA_PATH_1990S\n",
    "# Load Actual Test Set\n",
    "test_df = pd.read_csv(TEST_DATA_PATH_1990S)\n",
    "test_df[\"observation_date\"] = pd.to_datetime(test_df[\"observation_date\"], format=\"%m/%Y\")\n",
    "test_df.set_index(\"observation_date\", inplace=True)\n",
    "\n",
    "# Preprocess Test Set \n",
    "test_df[\"log_pcepi\"] = np.log(test_df[\"fred_PCEPI\"])\n",
    "test_df[\"sin_month\"] = np.sin(2 * np.pi * test_df.index.month / 12)\n",
    "test_df[\"cos_month\"] = np.cos(2 * np.pi * test_df.index.month / 12)\n",
    "test_df[\"momentum\"] = test_df[\"log_pcepi\"].diff()\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# Use Same Features As Train \n",
    "test_exog = test_df[train_exog.columns]\n",
    "\n",
    "# Apply PCA + Lag \n",
    "test_pca = pca.transform(test_exog)\n",
    "test_exog_pca = pd.DataFrame(test_pca, index=test_exog.index, columns=pca_cols)\n",
    "test_lagged = add_lags(test_exog_pca, lags=1).dropna()\n",
    "\n",
    "# Forecast \n",
    "start = len(train_lagged)\n",
    "end = start + len(test_lagged) - 1\n",
    "log_test_preds = model.predict(start=start, end=end, exog_oos=test_lagged)\n",
    "test_preds = np.exp(log_test_preds)\n",
    "\n",
    "# Ensure 48-Length Prediction\n",
    "final_preds = np.zeros(48)\n",
    "if len(test_preds) >= 48:\n",
    "    final_preds[:] = test_preds.values[-48:]\n",
    "else:\n",
    "    # Right-align predictions, fill earlier steps with the first forecast value\n",
    "    pad_len = 48 - len(test_preds)\n",
    "    final_preds[:pad_len] = test_preds.iloc[0]\n",
    "    final_preds[pad_len:] = test_preds.values\n",
    "\n",
    "# Save to .npy\n",
    "np.save(\"../../Predictions/ARDL.npy\", final_preds)\n",
    "print(f\"Final ARDL.npy saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
