{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Manually set the project root directory (adjust if needed)\n",
    "project_root = os.path.abspath(\"..\")  # Moves up one level to project root\n",
    "# Add the project directory to sys.path\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import time\n",
    "import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "from EarlyStopping import EarlyStopping  # Ensure EarlyStopping is available\n",
    "from Training.PyTorchModular import train_model, loss_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Now import the RNN model\n",
    "from Models.RNN import RNNModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/ Clean Model Weight Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# **Ensure Model Save Path Exists**\n",
    "model_save_path = os.path.join('..', 'Models', 'Weights', 'RNN')\n",
    "os.makedirs(model_save_path, exist_ok=True)  # Create if not exists\n",
    "\n",
    "# **Delete all existing files in the RNN model directory**\n",
    "for file in os.listdir(model_save_path):\n",
    "    file_path = os.path.join(model_save_path, file)\n",
    "    if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "        os.unlink(file_path)  # Remove files and symlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Define relative file path for training data**\n",
    "train_file = os.path.join('..', 'Data', 'Train', 'train1990s.csv')\n",
    "\n",
    "# **Load Training Data with Automatic Column Detection**\n",
    "train_df = pd.read_csv(train_file)\n",
    "#print(\"Columns in dataset:\", train_df.columns)  # Debugging: Show available columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = 'observation_date'\n",
    "target_col = 'fred_PCEPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataPreprocessing import minMaxScale\n",
    "\n",
    "# **Normalize Data**\n",
    "# Perform min-max scaling on input data (no exogenous variables)\n",
    "train_X_scaled = minMaxScale(train_df[[target_col]].values)\n",
    "train_series = train_X_scaled.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataPreprocessing import create_sequences\n",
    "\n",
    "# **Set Sequence Length**\n",
    "# Create sequences from the training series\n",
    "sequence_length = 12\n",
    "X, y = create_sequences(train_series, train_series, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataPreprocessing import train_val_test_split\n",
    "\n",
    "# **Train-Validation Split (80% Train, 20% Validation)**\n",
    "X_train, y_train, X_val, y_val, _, _ = train_val_test_split(X, y, train_size=0.8, val_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Convert Data to PyTorch Tensors**\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(-1)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "# **Create DataLoaders**\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Train Model Using Modular Functions**\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rnn_model = RNNModel(input_size=1, hidden_size=64, num_layers=2).to(device)\n",
    "\n",
    "# **Define Loss Function and Optimizer**\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# **Train the Model Using Modular Functions**\n",
    "train_data = train_model(\n",
    "    model=rnn_model,\n",
    "    maxEpochs=50,\n",
    "    modelSavePath=model_save_path,\n",
    "    modelName=\"RNN\",\n",
    "    dataLoaderTrain=train_loader,\n",
    "    dataLoaderValid=val_loader,\n",
    "    lossFn=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    batchStatusUpdate=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Plot Training vs. Validation Loss**\n",
    "loss_curve(trainLoss=train_data[\"trainLoss\"], validLoss=train_data[\"validLoss\"], title=\"RNN Training vs. Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation.evaluation_helpers import get_best_model_path\n",
    "\n",
    "best_model_path = get_best_model_path(model_save_path, 'RNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Load the Best or Latest Model**\n",
    "best_rnn_model = RNNModel(input_size=1, hidden_size=64, num_layers=2).to(device)\n",
    "best_rnn_model.load_state_dict(torch.load(best_model_path))\n",
    "best_rnn_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# **Define relative file path for training data**\n",
    "train_file = os.path.join('..', 'Data', 'Train', 'train1990s.csv')\n",
    "\n",
    "# **Load Training Data with Automatic Column Detection**\n",
    "train_df = pd.read_csv(train_file)\n",
    "print(\"Columns in dataset:\", train_df.columns)  # Debugging: Show available columns\n",
    "\n",
    "# **Find the correct date column**\n",
    "possible_date_cols = [\"Date\", \"date\", \"observation_date\", \"timestamp\"]\n",
    "for col in possible_date_cols:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = pd.to_datetime(train_df[col])\n",
    "        train_df.set_index(col, inplace=True)\n",
    "        print(f\"Using '{col}' as Date column.\")\n",
    "        break\n",
    "else:\n",
    "    raise ValueError(\"No valid date column found in dataset!\")\n",
    "\n",
    "# **Rename Value Column (Check if it exists)**\n",
    "if \"Value\" in train_df.columns:\n",
    "    train_df.rename(columns={\"Value\": \"PCE\"}, inplace=True)\n",
    "elif \"fred_PCEPI\" in train_df.columns:\n",
    "    train_df.rename(columns={\"fred_PCEPI\": \"PCE\"}, inplace=True)\n",
    "else:\n",
    "    raise ValueError(\"No valid column found for PCE data!\")\n",
    "\n",
    "# **Normalize Data**\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[[\"PCE\"]])\n",
    "\n",
    "# **Convert the scaled data to a 1D array**\n",
    "train_series = train_scaled.flatten()\n",
    "\n",
    "# **Function to Create Sequences**\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Creates sequences of length `seq_length` as inputs and the subsequent value as the target.\n",
    "    \"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i : i + seq_length])\n",
    "        ys.append(data[i + seq_length])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# **Set Sequence Length**\n",
    "sequence_length = 12\n",
    "X, y = create_sequences(train_series, sequence_length)\n",
    "\n",
    "# **Train-Validation Split (80% Train, 20% Validation)**\n",
    "split_ratio = 0.8\n",
    "split_idx = int(len(X) * split_ratio)\n",
    "\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# **Convert Data to PyTorch Tensors**\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(-1)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "# **Create DataLoaders**\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# **Plot Training vs. Validation Loss**\n",
    "loss_curve(trainLoss=train_data[\"trainLoss\"], validLoss=train_data[\"validLoss\"], title=\"RNN Training vs. Validation Loss\")\n",
    "\n",
    "# **Find the best saved model dynamically**\n",
    "best_model_files = glob.glob(os.path.join(model_save_path, \"RNN_BEST_STOPPED_AT_*.pth\"))\n",
    "latest_model_path = os.path.join(model_save_path, \"RNN_latest.pth\")\n",
    "\n",
    "if best_model_files:\n",
    "    best_model_path = sorted(best_model_files)[-1]  # Pick the latest best-stopped model\n",
    "    print(f\"Loading best model: {best_model_path}\")\n",
    "elif os.path.exists(latest_model_path):\n",
    "    best_model_path = latest_model_path\n",
    "    print(f\"No early-stopped model found. Loading latest model instead: {best_model_path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No saved model found! Ensure training was completed successfully.\")\n",
    "\n",
    "# **Load the Best or Latest Model**\n",
    "best_rnn_model = RNNModel(input_size=1, hidden_size=64, num_layers=2).to(device)\n",
    "best_rnn_model.load_state_dict(torch.load(best_model_path))\n",
    "best_rnn_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation.evaluation_helpers import make_evaluation_predictions\n",
    "\n",
    "rnn_predictions_inv, actuals_inv = make_evaluation_predictions(best_rnn_model, best_model_path, val_loader, y_scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Extract the dates corresponding to the validation predictions**\n",
    "val_dates = train_df.index[split_idx + sequence_length:]\n",
    "\n",
    "# **Create a DataFrame for comparison**\n",
    "df_comparison = pd.DataFrame({\n",
    "    \"Date\": val_dates,\n",
    "    \"Actual PCE\": actuals_inv.flatten(),\n",
    "    \"Predicted PCE\": rnn_predictions_inv.flatten()\n",
    "})\n",
    "\n",
    "# **Display the first few rows of the comparison DataFrame**\n",
    "print(df_comparison.head())\n",
    "\n",
    "# **Plot the Actual vs. Predicted PCE values**\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_comparison[\"Date\"], df_comparison[\"Actual PCE\"], label=\"Actual PCE\", marker=\"o\", linestyle=\"-\")\n",
    "plt.plot(df_comparison[\"Date\"], df_comparison[\"Predicted PCE\"], label=\"Predicted PCE (RNN)\", marker=\"x\", linestyle=\"--\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"PCE\")\n",
    "plt.title(\"Comparison of Actual vs. Predicted PCE (RNN)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Compute RMSE for Validation Predictions**\n",
    "rmse_rnn = np.sqrt(mean_squared_error(actuals_inv, rnn_predictions_inv))\n",
    "print(f\"Root Mean Squared Error (RMSE) for RNN Model: {rmse_rnn:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
