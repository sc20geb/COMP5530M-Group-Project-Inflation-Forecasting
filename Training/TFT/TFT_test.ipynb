{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89eeda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from torchmetrics import MeanSquaredError\n",
    "from torch.nn import MSELoss\n",
    "import sys\n",
    "sys.path.append('../Helper/')\n",
    "from dataPreprocessing import rank_features_ccf, get_untransformed_exog\n",
    "from hyperparameters import tune_hyperparameters, load_best_hyperparameters, save_best_hyperparameters\n",
    "import numpy as np\n",
    "import torch\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c27ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('../../Data/Train/train1990s.csv')\n",
    "\n",
    "data= get_untransformed_exog(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd554d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES=data.shape[0]\n",
    "INPUT_SIZE=12\n",
    "VALID_SIZE=3\n",
    "HORIZON=1\n",
    "valid_metric= MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3207114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= data.copy()\n",
    "df['observation_date'] = pd.to_datetime(df['observation_date'], format='%m/%Y')\n",
    "df.set_index('observation_date', inplace=True)\n",
    "ranked_exog_cols = rank_features_ccf(df,targetCol='fred_PCEPI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f750496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prediction(input_size, df_exog,target_series,exog_scaler, target_scaler, pred_date):\n",
    "\n",
    "    X=TimeSeries.from_dataframe(df_exog.loc[pred_date-pd.DateOffset(months=input_size):pred_date-pd.DateOffset(months=1),:])\n",
    "    y_past= TimeSeries.from_series(target_series.loc[pred_date-pd.DateOffset(months=input_size):pred_date-pd.DateOffset(months=1)])\n",
    "\n",
    "    return exog_scaler.transform(X), target_scaler.transform(y_past)\n",
    "\n",
    "def train_valid_split_darts(exog_df, target_series, valid_size,input_size):\n",
    "    train_target= TimeSeries.from_series(target_series.iloc[:-valid_size])\n",
    "    valid_target= TimeSeries.from_series(target_series.iloc[-valid_size-input_size:])\n",
    "\n",
    "    train_exog= TimeSeries.from_dataframe(exog_df.iloc[:-valid_size,:])\n",
    "    valid_exog= TimeSeries.from_dataframe(exog_df.iloc[-valid_size-input_size:,:])\n",
    "\n",
    "    return train_target, valid_target, train_exog, valid_exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD PRUNING\n",
    "def objective(trial):\n",
    "\n",
    "    # Optuna suggestions:\n",
    "    num_exog= trial.suggest_int(\"num_exog\",1,df.shape[0],step=1)\n",
    "    loss_fn_type= trial.suggest_categorical(\"loss_fn\", [\"QuantileRegression\", \"MSE\"])\n",
    "\n",
    "    if loss_fn_type==\"QuantileRegression\":\n",
    "        likelihood= QuantileRegression((0.25,0.5,0.75))\n",
    "        loss_fn=None\n",
    "    else:\n",
    "        likelihood=None\n",
    "        loss_fn=torch.nn.MSELoss()\n",
    "    lr= trial.suggest_float(\"lr\",1e-5,1e-1,log=True)\n",
    "    hidden_size=trial.suggest_int(\"hidden_size\",1,32,step=1)\n",
    "    lstm_layers=trial.suggest_int(\"lstm_layers\",1,32,step=1)\n",
    "    num_attention_heads=trial.suggest_int(\"num_attention_heads\",1,8,step=1)\n",
    "\n",
    "\n",
    "    # Modify data:\n",
    "    df_exog=df[ranked_exog_cols[:num_exog]]\n",
    "    \n",
    "    train_target, valid_target, train_exog, valid_exog = train_valid_split_darts(df_exog,df['fred_PCEPI'],VALID_SIZE,INPUT_SIZE)\n",
    "\n",
    "    target_scaler = Scaler()\n",
    "    exog_scaler = Scaler()\n",
    "\n",
    "    train_target_scaled = target_scaler.fit_transform(train_target)\n",
    "    train_exog_scaled = exog_scaler.fit_transform(train_exog)\n",
    "\n",
    "    valid_target_scaled = target_scaler.transform(valid_target)\n",
    "    valid_exog_scaled = exog_scaler.transform(valid_exog)\n",
    "\n",
    "    #Callbacks:\n",
    "    early_stopper = EarlyStopping(\n",
    "        monitor=\"val_MeanSquaredError\",\n",
    "        patience=25,\n",
    "        min_delta=1e-4,\n",
    "        mode=\"min\",\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    #Define model:\n",
    "    model = TFTModel(\n",
    "        input_chunk_length=INPUT_SIZE,\n",
    "        output_chunk_length=HORIZON,\n",
    "        hidden_size=hidden_size,\n",
    "        lstm_layers=lstm_layers,\n",
    "        num_attention_heads=num_attention_heads,\n",
    "        dropout=0.1,\n",
    "        optimizer_cls=AdamW,\n",
    "        n_epochs=100,\n",
    "        random_state=42,\n",
    "        likelihood=likelihood,\n",
    "        optimizer_kwargs={\n",
    "            \"lr\":lr,\n",
    "            'betas':(0.9, 0.999), \n",
    "            'eps':1e-08, \n",
    "            'weight_decay':0.01, \n",
    "            'amsgrad':False,\n",
    "            'maximize':False, \n",
    "            'foreach':None, \n",
    "            'capturable':False, \n",
    "            'differentiable':False, \n",
    "            'fused':None\n",
    "        },\n",
    "        pl_trainer_kwargs={\n",
    "            \"accelerator\":\"gpu\",\n",
    "            \"devices\": -1,\n",
    "            \"callbacks\": [early_stopper]\n",
    "        },\n",
    "        loss_fn=loss_fn,\n",
    "        add_encoders={\n",
    "            \"cyclic\": {\"future\": [\"month\", \"quarter\"]}\n",
    "        },\n",
    "        save_checkpoints=True,\n",
    "        torch_metrics=valid_metric,\n",
    "        model_name='TFT_optuna',\n",
    "        force_reset=True\n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        series=train_target_scaled,\n",
    "        past_covariates=train_exog_scaled,\n",
    "        val_series=valid_target_scaled,\n",
    "        val_past_covariates=valid_exog_scaled,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    model.load_from_checkpoint(model_name='TFT_optuna',best=True)\n",
    "\n",
    "    valid_scores=[]\n",
    "    ground_truth=[]\n",
    "\n",
    "    for i in range(1,VALID_SIZE +1):\n",
    "        x,y=load_prediction(INPUT_SIZE,df_exog,df['fred_PCEPI'],exog_scaler,target_scaler,df.index[-i])\n",
    "        if isinstance(likelihood,QuantileRegression):\n",
    "            # If the model is probabilistic, take 100 samples and average them\n",
    "            preds_=[]\n",
    "            for i in range(0,15):\n",
    "                preds_.append(target_scaler.inverse_transform(model.predict(n=HORIZON,series=y,past_covariates=x,verbose=False)).values().reshape(-1))\n",
    "            \n",
    "            valid_scores.append(np.mean(preds_))\n",
    "\n",
    "        else:\n",
    "            valid_scores.append(target_scaler.inverse_transform(model.predict(n=HORIZON,series=y,past_covariates=x,verbose=False)).values().reshape(-1))\n",
    "        \n",
    "        ground_truth.append(df.loc[df.index[-i],'fred_PCEPI'])\n",
    "    \n",
    "\n",
    "    mse_score=mean_squared_error(np.asarray(valid_scores),np.asarray(ground_truth))\n",
    "    \n",
    "    return mse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d8b98e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 20:45:33,772] A new study created in memory with name: no-name-e0263f8b-00cd-4928-b5b9-29371e14a627\n",
      "2025-04-15 20:45:34,156 - INFO - Train dataset contains 393 samples.\n",
      "2025-04-15 20:45:34,479 - INFO - Time series values are 64-bits; casting model to float64.\n",
      "[W 2025-04-15 20:45:34,775] Trial 0 failed with parameters: {'num_exog': 394, 'loss_fn': 'MSE', 'lr': 9.937150333794777e-05, 'hidden_size': 8, 'lstm_layers': 8, 'num_attention_heads': 3} because of the following error: ValueError('Expected a parent').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_13864\\4215432116.py\", line 83, in objective\n",
      "    model.fit(\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\utils\\torch.py\", line 80, in decorator\n",
      "    return decorated(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py\", line 842, in fit\n",
      "    return self.fit_from_dataset(*params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\utils\\torch.py\", line 80, in decorator\n",
      "    return decorated(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py\", line 1023, in fit_from_dataset\n",
      "    *self._setup_for_train(\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py\", line 1176, in _setup_for_train\n",
      "    trainer = self._setup_trainer(trainer, model, verbose, train_num_epochs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py\", line 541, in _setup_trainer\n",
      "    return self._init_trainer(trainer_params=trainer_params, max_epochs=epochs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py\", line 554, in _init_trainer\n",
      "    return pl.Trainer(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py\", line 70, in insert_env_defaults\n",
      "    return fn(self, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 434, in __init__\n",
      "    self._callback_connector.on_trainer_init(\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py\", line 82, in on_trainer_init\n",
      "    _validate_callbacks_list(self.trainer.callbacks)\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py\", line 247, in _validate_callbacks_list\n",
      "    stateful_callbacks = [cb for cb in callbacks if is_overridden(\"state_dict\", instance=cb)]\n",
      "                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py\", line 42, in is_overridden\n",
      "    raise ValueError(\"Expected a parent\")\n",
      "ValueError: Expected a parent\n",
      "[W 2025-04-15 20:45:34,794] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected a parent",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_hyper_params\u001b[38;5;241m=\u001b[39m\u001b[43mtune_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_hyper_params)\n\u001b[0;32m      3\u001b[0m save_best_hyperparameters(best_hyper_params)\n",
      "File \u001b[1;32mh:\\My Drive\\UniversityOfLeeds\\CS\\Year4\\GroupProject\\repo\\COMP5530M-Group-Project-Inflation-Forecasting\\Training\\TFT\\../Helper\\hyperparameters.py:37\u001b[0m, in \u001b[0;36mtune_hyperparameters\u001b[1;34m(objective, n_trials)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m BEST_HYPERPARAMETERS\n\u001b[0;32m     36\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m BEST_HYPERPARAMETERS \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m     39\u001b[0m save_best_hyperparameters(BEST_HYPERPARAMETERS)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[6], line 83\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#Define model:\u001b[39;00m\n\u001b[0;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m TFTModel(\n\u001b[0;32m     45\u001b[0m     input_chunk_length\u001b[38;5;241m=\u001b[39mINPUT_SIZE,\n\u001b[0;32m     46\u001b[0m     output_chunk_length\u001b[38;5;241m=\u001b[39mHORIZON,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     force_reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     80\u001b[0m )\n\u001b[1;32m---> 83\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_target_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_exog_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_series\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_target_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_past_covariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_exog_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m model\u001b[38;5;241m.\u001b[39mload_from_checkpoint(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTFT_optuna\u001b[39m\u001b[38;5;124m'\u001b[39m,best\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     93\u001b[0m valid_scores\u001b[38;5;241m=\u001b[39m[]\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\utils\\torch.py:80\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[0;32m     79\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:842\u001b[0m, in \u001b[0;36mTorchForecastingModel.fit\u001b[1;34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, trainer, verbose, epochs, max_samples_per_ts, dataloader_kwargs, sample_weight, val_sample_weight)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;66;03m# call super fit only if user is actually fitting the model\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    838\u001b[0m     series\u001b[38;5;241m=\u001b[39mseq2series(series),\n\u001b[0;32m    839\u001b[0m     past_covariates\u001b[38;5;241m=\u001b[39mseq2series(past_covariates),\n\u001b[0;32m    840\u001b[0m     future_covariates\u001b[38;5;241m=\u001b[39mseq2series(future_covariates),\n\u001b[0;32m    841\u001b[0m )\n\u001b[1;32m--> 842\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\utils\\torch.py:80\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[0;32m     79\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1023\u001b[0m, in \u001b[0;36mTorchForecastingModel.fit_from_dataset\u001b[1;34m(self, train_dataset, val_dataset, trainer, verbose, epochs, dataloader_kwargs)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;129m@random_method\u001b[39m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_from_dataset\u001b[39m(\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    977\u001b[0m     dataloader_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    978\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorchForecastingModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    979\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;124;03m    Train the model with a specific :class:`darts.utils.data.TrainingDataset` instance.\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;124;03m    These datasets implement a PyTorch ``Dataset``, and specify how the target and covariates are sliced\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;124;03m        Fitted model.\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\n\u001b[1;32m-> 1023\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_for_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1031\u001b[0m     )\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1176\u001b[0m, in \u001b[0;36mTorchForecastingModel._setup_for_train\u001b[1;34m(self, train_dataset, val_dataset, trainer, verbose, epochs, dataloader_kwargs)\u001b[0m\n\u001b[0;32m   1173\u001b[0m train_num_epochs \u001b[38;5;241m=\u001b[39m epochs \u001b[38;5;28;01mif\u001b[39;00m epochs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;66;03m# setup trainer\u001b[39;00m\n\u001b[1;32m-> 1176\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_num_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mepochs_trained \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_ckpt_path:\n\u001b[0;32m   1179\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to retrain/fine-tune the model without resuming from a checkpoint. This is currently \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscouraged. Consider model `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.load_weights()` to load the weights for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine-tuning.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1183\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:541\u001b[0m, in \u001b[0;36mTorchForecastingModel._setup_trainer\u001b[1;34m(self, trainer, model, verbose, epochs)\u001b[0m\n\u001b[0;32m    536\u001b[0m     trainer_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_model_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    537\u001b[0m         verbose \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mepochs_trained \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    538\u001b[0m     )\n\u001b[0;32m    539\u001b[0m     trainer_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_progress_bar\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m verbose\n\u001b[1;32m--> 541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:554\u001b[0m, in \u001b[0;36mTorchForecastingModel._init_trainer\u001b[1;34m(trainer_params, max_epochs)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# prevent lightning from adding callbacks to the callbacks list in `self.trainer_params`\u001b[39;00m\n\u001b[0;32m    553\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m trainer_params_copy\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrainer_params_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:434\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir, model_registry)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate_grad_batches \u001b[38;5;241m=\u001b[39m accumulate_grad_batches\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# init callbacks\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# Declare attributes to be set in _callback_connector on_trainer_init\u001b[39;00m\n\u001b[1;32m--> 434\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trainer_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_checkpointing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_root_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_model_summary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# init data flags\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_val_every_n_epoch: Optional[\u001b[38;5;28mint\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:82\u001b[0m, in \u001b[0;36m_CallbackConnector.on_trainer_init\u001b[1;34m(self, callbacks, enable_checkpointing, enable_progress_bar, default_root_dir, enable_model_summary, max_time)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_model_summary_callback(enable_model_summary)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mextend(_load_external_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_lightning.callbacks_factory\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 82\u001b[0m \u001b[43m_validate_callbacks_list\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# push all model checkpoint callbacks to the end\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# it is important that these are the last callbacks to run\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reorder_callbacks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:247\u001b[0m, in \u001b[0;36m_validate_callbacks_list\u001b[1;34m(callbacks)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_callbacks_list\u001b[39m(callbacks: \u001b[38;5;28mlist\u001b[39m[Callback]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     stateful_callbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_overridden\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    248\u001b[0m     seen_callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m stateful_callbacks:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:42\u001b[0m, in \u001b[0;36mis_overridden\u001b[1;34m(method_name, instance, parent)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         _check_mixed_imports(instance)\n\u001b[1;32m---> 42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a parent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning_utilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_overridden \u001b[38;5;28;01mas\u001b[39;00m _is_overridden\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _is_overridden(method_name, instance, parent)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected a parent"
     ]
    }
   ],
   "source": [
    "best_hyper_params=tune_hyperparameters(objective, n_trials=5)\n",
    "print(best_hyper_params)\n",
    "save_best_hyperparameters(best_hyper_params)\n",
    "\n",
    "loaded_hyper_params= load_best_hyperparameters()\n",
    "print(loaded_hyper_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
