{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish project and weights directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up two levels from notebook (Training/GRU) to project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"Project root added to sys.path:\", project_root)\n",
    "\n",
    "# Ensure the model save directory exists\n",
    "model_save_path = os.path.join(project_root, 'Models', 'Weights', 'GRU')\n",
    "os.makedirs(model_save_path, exist_ok=True)  # Creates directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess data & Train the Model with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Models.GRU import GRUModel   # Import the GRU model\n",
    "# Import modularized functions\n",
    "from Training.Helper.dataPreprocessing import load_data, prepare_dataloader, TRAIN_DATA_PATH_1990S\n",
    "from Training.Helper.PyTorchModular import optuna_tune_and_train\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "config = {\"use_fft\": False, \"use_exog\": False}\n",
    "\n",
    "(X_train_seq, y_train_seq, X_valid_seq, y_valid_seq, X_test_seq, y_test_seq, observation_dates, y_scaler) = load_data(\n",
    "    TRAIN_DATA_PATH_1990S, sequence_length=12, config=config\n",
    ")\n",
    "\n",
    "X_train_seq = X_train_seq.reshape(X_train_seq.shape[0], X_train_seq.shape[1], 1)\n",
    "X_valid_seq = X_valid_seq.reshape(X_valid_seq.shape[0], X_valid_seq.shape[1], 1)\n",
    "\n",
    "y_train_seq = y_train_seq.reshape(-1, 1)\n",
    "y_valid_seq = y_valid_seq.reshape(-1, 1)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = prepare_dataloader(X_train_seq, y_train_seq, batch_size=batch_size)\n",
    "val_loader = prepare_dataloader(X_valid_seq, y_valid_seq, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Run Optuna & Train Model\n",
    "model, metadata = optuna_tune_and_train(\n",
    "    model_class=GRUModel, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    device=device,\n",
    "    max_epochs=50,\n",
    "    model_save_path=model_save_path,\n",
    "    model_name=\"GRU_inflation\",\n",
    "    use_best_hyperparams=False,  # Set to False to force Optuna tuning every time\n",
    "    n_trials=20,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjustments for 48-Month Forecast & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Make 48-month future predictions using last known sequence ---\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# Prepare the starting sequence (last sequence in test set)\n",
    "last_sequence = X_test_seq[-1].reshape(1, 12, 1)  # Shape: (1, 12, 1)\n",
    "preds = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    current_seq = torch.tensor(last_sequence, dtype=torch.float32).to(device)\n",
    "\n",
    "    for _ in range(48):  # Predict next 48 steps\n",
    "        next_step = model(current_seq)          # Shape: (1, 1)\n",
    "        next_value = next_step.item()\n",
    "        preds.append(next_value)\n",
    "\n",
    "        # Shift window left and append prediction\n",
    "        current_input = current_seq.cpu().numpy().reshape(12, 1)  # Always (12, 1)\n",
    "        next_input = np.append(current_input[1:], [[next_value]], axis=0)  # Shape: (12, 1)\n",
    "        current_seq = torch.tensor(next_input.reshape(1, 12, 1), dtype=torch.float32).to(device)\n",
    "\n",
    "# Inverse transform if scaling was applied\n",
    "if y_scaler is not None:\n",
    "    preds = y_scaler.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Save predictions\n",
    "output_path = os.path.join(project_root, \"Predictions\", \"GRU.npy\")\n",
    "np.save(output_path, np.array(preds))\n",
    "print(f\" Saved 48-month forecast to: {output_path}\")\n",
    "\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(y_test_seq, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(f\"GRU Test MSE: {mse}\")\n",
    "print(f\"GRU Test RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Predictions on Validation Set\n",
    "\n",
    "*used in standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Evaluation.Helper.evaluation_helpers import evaluate_model\n",
    "\n",
    "# # Evaluate Model\n",
    "# df_comparison, rmse = evaluate_model(\n",
    "#     model=model, \n",
    "#     val_loader=val_loader, \n",
    "#     y_scaler=y_scaler, \n",
    "#     observation_dates=observation_dates, \n",
    "#     device=device,\n",
    "#     verbose=True\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
