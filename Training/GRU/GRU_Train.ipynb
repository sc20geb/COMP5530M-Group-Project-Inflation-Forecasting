{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish project and weights directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up two levels from notebook (Training/GRU) to project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"Project root added to sys.path:\", project_root)\n",
    "\n",
    "# Ensure the model save directory exists\n",
    "model_save_path = os.path.join('.')\n",
    "os.makedirs(model_save_path, exist_ok=True)  # Creates directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess data & Train the Model with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "from Training.Helper.dataPreprocessing import TRAIN_DATA_PATH_1990S, TRAIN_DATA_SPLIT, VAL_DATA_SPLIT, create_sequences, sklearn_fit_transform, add_dimension, prepare_dataloader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read data into DataFrame, and identify target data (no exogenous data used)\n",
    "df = pd.read_csv(TRAIN_DATA_PATH_1990S)\n",
    "df[\"observation_date\"] = pd.to_datetime(df[\"observation_date\"], format=\"%m/%Y\")\n",
    "df = df.sort_values(by=\"observation_date\").reset_index(drop=True)\n",
    "target_col = \"fred_PCEPI\"\n",
    "\n",
    "data = df[[target_col]].values.astype(np.float32)\n",
    "\n",
    "Xy_train, Xy_val = train_test_split(data, train_size=TRAIN_DATA_SPLIT, test_size=VAL_DATA_SPLIT, shuffle=False)\n",
    "\n",
    "#MinMax scale x and y\n",
    "[Xy_train, Xy_val], y_scaler = sklearn_fit_transform(Xy_train, Xy_val, MinMaxScaler())\n",
    "Xy_train, Xy_val = Xy_train.values, Xy_val.values  #get back out of DataFrame form\n",
    "\n",
    "# Make sequences for model\n",
    "sequence_length = 12\n",
    "X_train_seq, y_train_seq = create_sequences(Xy_train, Xy_train, seq_len=sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(Xy_val, Xy_train, seq_len=sequence_length)\n",
    "\n",
    "# Appropriately resize training and validation input sequences\n",
    "[X_train_seq, X_val_seq] = [add_dimension(nparrays) for nparrays in [X_train_seq, X_val_seq]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloaders\n",
    "batch_size = 32\n",
    "train_loader = prepare_dataloader(X_train_seq, y_train_seq, batch_size=batch_size)\n",
    "val_loader = prepare_dataloader(X_val_seq, y_val_seq, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Models.GRU import GRUModel   # Import the GRU model\n",
    "# Import modularized functions\n",
    "from Training.Helper.PyTorchModular import optuna_tune_and_train\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Run Optuna & Train Model\n",
    "model, metadata, study = optuna_tune_and_train(\n",
    "    model_class=GRUModel, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    device=device,\n",
    "    model_search_space={\"hidden_size\": (int, (32, 256)), \"num_layers\": (int, (1, 4))},\n",
    "    optim_search_space={\"lr\": (float, (1e-5, 1e-1))},\n",
    "    max_epochs=50,\n",
    "    model_save_path=model_save_path,\n",
    "    model_name=\"GRU_inflation\",\n",
    "    use_best_hyperparams=False,  # Set to False to force Optuna tuning every time\n",
    "    n_trials=20,\n",
    "    return_study=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjustments for 48-Month Forecast & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Make 48-month future predictions using last known sequence ---\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# Prepare the starting sequence (last sequence in test set)\n",
    "last_sequence = X_val_seq[-1].reshape(1, 12, 1)  # Shape: (1, 12, 1)\n",
    "preds = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    current_seq = torch.tensor(last_sequence, dtype=torch.float32).to(device)\n",
    "\n",
    "    for _ in range(48):  # Predict next 48 steps\n",
    "        next_step = model(current_seq)          # Shape: (1, 1)\n",
    "        next_value = next_step.item()\n",
    "        preds.append(next_value)\n",
    "\n",
    "        # Shift window left and append prediction\n",
    "        current_input = current_seq.cpu().numpy().reshape(12, 1)  # Always (12, 1)\n",
    "        next_input = np.append(current_input[1:], [[next_value]], axis=0)  # Shape: (12, 1)\n",
    "        current_seq = torch.tensor(next_input.reshape(1, 12, 1), dtype=torch.float32).to(device)\n",
    "\n",
    "# Inverse transform if scaling was applied\n",
    "if y_scaler is not None:\n",
    "    preds = y_scaler.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Save predictions\n",
    "output_path = os.path.join(project_root, \"Predictions\", \"GRU.npy\")\n",
    "np.save(output_path, np.array(preds))\n",
    "print(f\" Saved 48-month forecast to: {output_path}\")\n",
    "\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(y_test_seq, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(f\"GRU Test MSE: {mse}\")\n",
    "print(f\"GRU Test RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Predictions on Validation Set\n",
    "\n",
    "*used in standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation.Helper.evaluation_helpers import evaluate_model\n",
    "\n",
    "observation_dates = df['observation_date'][int(len(df)*TRAIN_DATA_SPLIT):]\n",
    "\n",
    "# Evaluate Model\n",
    "df_comparison, rmse = evaluate_model(\n",
    "     model=model, \n",
    "     val_loader=val_loader, \n",
    "     y_scaler=y_scaler, \n",
    "     observation_dates=observation_dates, \n",
    "     device=device,\n",
    "     verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these to clean all weight files just produced\n",
    "#from Training.Helper.weightFileCleaner import cleanWeightFiles\n",
    "#cleanWeightFiles('GRU', earlyStopped=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
