{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import TiDEModel\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping as EarlyStopping_lightning\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: c:\\Users\\James\\COMP5530M-Group-Project-Inflation-Forecasting\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up two levels from notebook (Training/MLR) to project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"Project root added to sys.path:\", project_root)\n",
    "# Ensure the model save directory exists\n",
    "model_save_path = os.path.join('.')\n",
    "os.makedirs(model_save_path, exist_ok=True)  # Creates directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Training.Helper.dataPreprocessing import TRAIN_DATA_PATH_1990S, get_untransformed_exog, TEST_DATA_PATH_1990S\n",
    "date_col = 'observation_date'\n",
    "\n",
    "# Load and format training data (only using PCEPI)\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH_1990S)\n",
    "train_df = get_untransformed_exog(train_df)\n",
    "train_df['observation_date'] = pd.to_datetime(train_df['observation_date'], format='%m/%Y')\n",
    "train_df.set_index('observation_date', inplace=True)\n",
    "\n",
    "test_df = pd.read_csv(TEST_DATA_PATH_1990S)\n",
    "test_df= get_untransformed_exog(test_df)\n",
    "test_df['observation_date'] = pd.to_datetime(test_df['observation_date'], format='%m/%Y')\n",
    "test_df.set_index('observation_date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.dataPreprocessing import rank_features_ccf\n",
    "\n",
    "# Ranks all non-date variables\n",
    "ranked_features = rank_features_ccf(train_df)\n",
    "    \n",
    "# Define the number of features that should be used\n",
    "FEATURES_TO_USE = len(ranked_features) #ensure this is all features for Optuna to work properly\n",
    "used_features = ranked_features[:FEATURES_TO_USE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_df(df, ranked_features, features_to_use):\n",
    "    return df.loc[:,ranked_features[:features_to_use]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target_col = \"fred_PCEPI\"\n",
    "\n",
    "train_df_ranked = get_ranked_df(train_df, ranked_features, FEATURES_TO_USE)\n",
    "train_df_ranked[target_col] = train_df[target_col]\n",
    "\n",
    "train_exog= train_df.drop('fred_PCEPI',axis=1)\n",
    "train_target = train_df['fred_PCEPI']\n",
    "\n",
    "\n",
    "test_exog= test_df.drop('fred_PCEPI',axis=1)\n",
    "test_target= test_df['fred_PCEPI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "exogenous_train = TimeSeries.from_dataframe(train_exog)\n",
    "target_train = TimeSeries.from_series(train_df['fred_PCEPI'])\n",
    "\n",
    "exogenous_test = TimeSeries.from_dataframe(test_exog)\n",
    "target_test = TimeSeries.from_series(test_df['fred_PCEPI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split validation and training then scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform_TimeSeries(train_ts, val_ts, scaler):\n",
    "    scaled_train = scaler.fit_transform(train_ts)\n",
    "    scaled_val = scaler.transform(val_ts)\n",
    "    return scaled_train, scaled_val, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.dataPreprocessing import TRAIN_DATA_SPLIT\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "def split_and_scale_TimeSeries(target_series, exogenous_series):\n",
    "    train_target, val_target = target_series.split_after(TRAIN_DATA_SPLIT)\n",
    "    train_exo, val_exo = exogenous_series.split_after(TRAIN_DATA_SPLIT)\n",
    "    \n",
    "    # default uses sklearn's MinMaxScaler\n",
    "    scaled_train_target, scaled_val_target, targetScaler = fit_transform_TimeSeries(train_target, val_target, Scaler(RobustScaler()))\n",
    "    scaled_train_exo, scaled_val_exo, exoScaler = fit_transform_TimeSeries(train_exo, val_exo, Scaler(RobustScaler()))\n",
    "    return scaled_train_target, scaled_train_exo, scaled_val_target, scaled_val_exo, targetScaler, exoScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_target, scaled_train_exo, scaled_val_target, scaled_val_exo, targetScaler, exoScaler = split_and_scale_TimeSeries(target_train, exogenous_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_LENGTH = 12\n",
    "\n",
    "early_stopper = EarlyStopping_lightning(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=1e-3,\n",
    "    mode='min'\n",
    ")\n",
    "lr_scheduler_kwargs = {\n",
    "    \"gamma\": 0.999,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.metrics import mse\n",
    "from Training.Helper.PyTorchModular import optuna_trial_get_kwargs,load_prediction\n",
    "\n",
    "def get_optuna_ranked_series(trial, scaled_train_exo, scaled_val_exo, ranked_features):\n",
    "    n_features = optuna_trial_get_kwargs(trial, {'n': (int, 1, scaled_train_exo.n_components)})['n']\n",
    "    return scaled_train_exo.drop_columns(ranked_features[n_features:]), scaled_val_exo.drop_columns(ranked_features[n_features:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target, val_target = target_train.split_after(TRAIN_DATA_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from darts.metrics import mse\n",
    "from Training.Helper.PyTorchModular import optuna_trial_get_kwargs\n",
    "\n",
    "model_search_space = {\n",
    "    'input_chunk_length': (int, 24, 60),\n",
    "    'num_encoder_layers': (int, 1, 3),\n",
    "    'num_decoder_layers': (int, 1, 3),\n",
    "    'hidden_size': (int, 64, 512),\n",
    "    'dropout': (float, 0.1, 0.5, {'log': True}),\n",
    "    'optimizer_kwargs': {\"lr\": (float, 1e-4, 1e-2)},\n",
    "    'lr_scheduler_kwargs': {\"gamma\": (float, 0.9, 1.0)},\n",
    "    'use_reversible_instance_norm': ('categorical', [True, False]),\n",
    "}\n",
    "\n",
    "def createObjective(model_invariates,HORIZON):\n",
    "    def objective(trial):\n",
    "\n",
    "        model_kwargs = optuna_trial_get_kwargs(trial, model_search_space)\n",
    "\n",
    "        scaled_train_exo_ranked, scaled_val_exo_ranked = get_optuna_ranked_series(trial, scaled_train_exo, scaled_val_exo, ranked_features)\n",
    "        \n",
    "        # Initialize the TiDEModel with suggested hyperparameters\n",
    "        model = TiDEModel(**model_kwargs, **model_invariates)\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(series = scaled_train_target,\n",
    "                past_covariates = scaled_train_exo_ranked,\n",
    "                val_series = scaled_val_target,\n",
    "                val_past_covariates = scaled_val_exo_ranked,\n",
    "                epochs=1000,\n",
    "                verbose = False)\n",
    "\n",
    "        # Evaluate the model\n",
    "        # (this is an alternative option for evaluation, where the model must predict the final prediction_size elements of the validation data having been given all other validation data;\n",
    "        #  if switching to this method, ensure that final prediction is performed with the same setup (this is currently done just by predicting the next n values))\n",
    "        #scaled_val_predictions = model.predict(n=prediction_size,series=scaled_val_target[:-prediction_size],past_covariates=scaled_val_exo[:-prediction_size], verbose=False)]\n",
    "        #val_predictions = targetScaler.inverse_transform(scaled_val_predictions, verbose=False)\n",
    "        #error = mse(val_target[-prediction_size:], val_predictions, verbose=False)\n",
    "\n",
    "        # Raw output is scaled, so inverse transform to become comparable with validation set\n",
    "        scaled_val_predictions = model.predict(n=HORIZON, verbose=False)\n",
    "        val_predictions = targetScaler.inverse_transform(scaled_val_predictions, verbose=False)\n",
    "        # Only uses the first prediction_size values of val_target, since this is the size of the prediction made by the model\n",
    "        error = mse(val_target[:HORIZON], val_predictions, verbose=False)\n",
    "        return error\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.PyTorchModular import reformat_best_params\n",
    "def getParams(study):\n",
    "    # Retrieve the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "    # Get the 'n' parameter out of the best_params dictionary and extract just the value\n",
    "    best_n_features = reformat_best_params(best_params, {'n': (int, (1, 2))})['n']\n",
    "    # Format parameters returned by study into the same style as the search space definition (can be passed straight into model as kwargs)\n",
    "    best_params = reformat_best_params(best_params, model_search_space)\n",
    "    print('Best hyperparameters:')\n",
    "    display(best_params)\n",
    "    print('Best number of features to include:', best_n_features)\n",
    "    return best_params ,best_n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tide(model,target,ranked_exo):\n",
    "    scaled_train_target, scaled_train_exo, scaled_val_target, scaled_val_exo, targetScaler, exoScaler = split_and_scale_TimeSeries(target, ranked_exo)\n",
    "    model.fit(series = scaled_train_target,\n",
    "        past_covariates = scaled_train_exo,\n",
    "        val_series = scaled_val_target,\n",
    "        val_past_covariates = scaled_val_exo,\n",
    "        epochs=1000,\n",
    "        verbose = False)\n",
    "    return model, targetScaler,exoScaler\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horizon 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 02:25:43,336] A new study created in memory with name: no-name-20bfec31-11fb-4771-bdb3-604dcebbbf9b\n",
      "2025-04-24 02:25:43,367 - INFO - Train dataset contains 275 samples.\n",
      "2025-04-24 02:25:43,377 - INFO - Time series values are 64-bits; casting model to float64.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-04-24 02:25:46,653] Trial 0 finished with value: 2.9276368271989526 and parameters: {'input_chunk_length': 51, 'num_encoder_layers': 3, 'num_decoder_layers': 1, 'hidden_size': 298, 'dropout': 0.1033589236009936, 'lr': 0.009314578784784157, 'gamma': 0.9973597699122809, 'use_reversible_instance_norm': True, 'n': 38}. Best is trial 0 with value: 2.9276368271989526.\n",
      "2025-04-24 02:25:46,669 - INFO - Train dataset contains 302 samples.\n",
      "2025-04-24 02:25:46,682 - INFO - Time series values are 64-bits; casting model to float64.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-04-24 02:25:48,624] Trial 1 finished with value: 0.04830025725626836 and parameters: {'input_chunk_length': 24, 'num_encoder_layers': 3, 'num_decoder_layers': 1, 'hidden_size': 376, 'dropout': 0.36477809234746655, 'lr': 0.0025587115386399143, 'gamma': 0.9053198643013861, 'use_reversible_instance_norm': True, 'n': 44}. Best is trial 1 with value: 0.04830025725626836.\n",
      "2025-04-24 02:25:48,641 - INFO - Train dataset contains 290 samples.\n",
      "2025-04-24 02:25:48,648 - INFO - Time series values are 64-bits; casting model to float64.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-04-24 02:25:48,890] Trial 2 finished with value: 0.04038095723745903 and parameters: {'input_chunk_length': 36, 'num_encoder_layers': 3, 'num_decoder_layers': 1, 'hidden_size': 94, 'dropout': 0.2504654215738925, 'lr': 0.004148766008434017, 'gamma': 0.9859260126103628, 'use_reversible_instance_norm': True, 'n': 10}. Best is trial 2 with value: 0.04038095723745903.\n",
      "2025-04-24 02:25:48,911 - INFO - Train dataset contains 296 samples.\n",
      "2025-04-24 02:25:48,920 - INFO - Time series values are 64-bits; casting model to float64.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-04-24 02:25:49,273] Trial 3 finished with value: 6.176332276606472 and parameters: {'input_chunk_length': 30, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'hidden_size': 470, 'dropout': 0.47790068384203094, 'lr': 0.004440321669572087, 'gamma': 0.9832793147772404, 'use_reversible_instance_norm': True, 'n': 131}. Best is trial 2 with value: 0.04038095723745903.\n",
      "2025-04-24 02:25:49,292 - INFO - Train dataset contains 290 samples.\n",
      "2025-04-24 02:25:49,302 - INFO - Time series values are 64-bits; casting model to float64.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-04-24 02:25:49,568] Trial 4 finished with value: 7.236827574189183 and parameters: {'input_chunk_length': 36, 'num_encoder_layers': 1, 'num_decoder_layers': 2, 'hidden_size': 267, 'dropout': 0.3424267942105932, 'lr': 0.0075195105388269744, 'gamma': 0.9324295376298728, 'use_reversible_instance_norm': True, 'n': 132}. Best is trial 2 with value: 0.04038095723745903.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_chunk_length': 36,\n",
       " 'num_encoder_layers': 3,\n",
       " 'num_decoder_layers': 1,\n",
       " 'hidden_size': 94,\n",
       " 'dropout': 0.2504654215738925,\n",
       " 'optimizer_kwargs': {'lr': 0.004148766008434017},\n",
       " 'lr_scheduler_kwargs': {'gamma': 0.9859260126103628},\n",
       " 'use_reversible_instance_norm': True}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 02:25:49,605 - INFO - Train dataset contains 290 samples.\n",
      "2025-04-24 02:25:49,615 - INFO - Time series values are 64-bits; casting model to float64.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of features to include: 10\n"
     ]
    }
   ],
   "source": [
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "HORIZON = 1\n",
    "\n",
    "# Controlling input chunk length for now to decrease the size of the search space\n",
    "model_invariates = {\n",
    "    #'input_chunk_length': 48,\n",
    "    'output_chunk_length': HORIZON,\n",
    "    'lr_scheduler_cls': lr_scheduler.ExponentialLR,\n",
    "    'pl_trainer_kwargs': {\"callbacks\": [early_stopper]}\n",
    "}\n",
    "objective_fn = createObjective(model_invariates=model_invariates, HORIZON=HORIZON)\n",
    "study.optimize(objective_fn, n_trials=5)\n",
    "best_params ,best_n_features = getParams(study)\n",
    "\n",
    "model = TiDEModel(**best_params, **model_invariates)\n",
    "\n",
    "exo_train_ranked = exogenous_train.drop_columns(ranked_features[best_n_features:])\n",
    "\n",
    "model,targetScaler,exoScaler = train_tide(model=model,target = target_train,ranked_exo=exo_train_ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "408\n",
      "408\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,train_exog_df, train_target_series, test_exog_df, test_target_series,exog_scaler,target_scaler, input_size,horizon):\n",
    "    \n",
    "    combined_exog_df= pd.concat((train_exog_df,test_exog_df),axis=0)\n",
    "    combined_target_series= pd.concat((train_target_series,test_target_series),axis=0)\n",
    "\n",
    "    preds_list=[]\n",
    "\n",
    "    for i in range(0,12,horizon):\n",
    "        x,y=load_prediction(input_size,combined_exog_df,combined_target_series,exog_scaler,target_scaler,test_target.index[i])\n",
    "        model.predict(n=horizon,series=y,past_covariates=x,verbose=False)\n",
    "        \n",
    "        pred=target_scaler.inverse_transform().values().flatten()\n",
    "\n",
    "        print(test_target.index[i:i+horizon])\n",
    "        print(pred)\n",
    "        preds_list= np.append(preds_list,pred)\n",
    "    \n",
    "    return preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"horizon1/TideH1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['fred_PCEPI'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m train_df_ranked \u001b[38;5;241m=\u001b[39m get_ranked_df(train_df, ranked_features, best_n_features)\n\u001b[1;32m----> 4\u001b[0m train_exog_ranked\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df_ranked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfred_PCEPI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m exogenous_train_ranked \u001b[38;5;241m=\u001b[39m TimeSeries\u001b[38;5;241m.\u001b[39mfrom_dataframe(train_exog_ranked)\n\u001b[0;32m      6\u001b[0m target \u001b[38;5;241m=\u001b[39m TimeSeries\u001b[38;5;241m.\u001b[39mfrom_series(train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfred_PCEPI\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['fred_PCEPI'] not found in axis\""
     ]
    }
   ],
   "source": [
    "\n",
    "train_df_ranked = get_ranked_df(train_df, ranked_features, best_n_features)\n",
    "\n",
    "\n",
    "train_exog_ranked= train_df_ranked.drop('fred_PCEPI',axis=1)\n",
    "exogenous_train_ranked = TimeSeries.from_dataframe(train_exog_ranked)\n",
    "target = TimeSeries.from_series(train_df['fred_PCEPI'])\n",
    "\n",
    "\n",
    "test_df_ranked = get_ranked_df(test_df, ranked_features, best_n_features)\n",
    "\n",
    "test_exog_ranked= test_df_ranked.drop('fred_PCEPI',axis=1)\n",
    "exogenous_test_ranked = TimeSeries.from_dataframe(test_exog_ranked)\n",
    "target_test = TimeSeries.from_series(test_df['fred_PCEPI'])\n",
    "\n",
    "predict(model=model,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best parameters into a model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TiDEModel with suggested hyperparameters\n",
    "best_model = TiDEModel(**best_params, **model_invariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import TiDEModel\n",
    "\n",
    "# Load the model from the file\n",
    "model = TiDEModel.load(\"tide.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 21:16:15,040 - ERROR - ValueError: The provided `past_covariates` must have equal dimensionality as the `past_covariates` used for training the model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The provided `past_covariates` must have equal dimensionality as the `past_covariates` used for training the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m scaled_total_exo \u001b[38;5;241m=\u001b[39m scaled_total_exo\u001b[38;5;241m.\u001b[39mdrop_columns(ranked_features[\u001b[38;5;241m109\u001b[39m:])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# may error here if the \u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaled_total\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaled_total_exo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m finalout \u001b[38;5;241m=\u001b[39m targetScaler\u001b[38;5;241m.\u001b[39minverse_transform(pred)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal predictions:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfinal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\utils\\torch.py:80\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[0;32m     79\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1530\u001b[0m, in \u001b[0;36mTorchForecastingModel.predict\u001b[1;34m(self, n, series, past_covariates, future_covariates, trainer, batch_size, verbose, n_jobs, roll_size, num_samples, dataloader_kwargs, mc_dropout, predict_likelihood_parameters, show_warnings)\u001b[0m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m   1512\u001b[0m     n,\n\u001b[0;32m   1513\u001b[0m     series,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1518\u001b[0m     show_warnings\u001b[38;5;241m=\u001b[39mshow_warnings,\n\u001b[0;32m   1519\u001b[0m )\n\u001b[0;32m   1521\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_inference_dataset(\n\u001b[0;32m   1522\u001b[0m     target\u001b[38;5;241m=\u001b[39mseries,\n\u001b[0;32m   1523\u001b[0m     n\u001b[38;5;241m=\u001b[39mn,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1527\u001b[0m     bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1528\u001b[0m )\n\u001b[1;32m-> 1530\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_from_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroll_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroll_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmc_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmc_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_likelihood_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_likelihood_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m called_with_single_series \u001b[38;5;28;01melse\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\utils\\torch.py:80\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[0;32m     79\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1623\u001b[0m, in \u001b[0;36mTorchForecastingModel.predict_from_dataset\u001b[1;34m(self, n, input_series_dataset, trainer, batch_size, verbose, n_jobs, roll_size, num_samples, dataloader_kwargs, mc_dropout, predict_likelihood_parameters)\u001b[0m\n\u001b[0;32m   1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_inference_dataset_type(input_series_dataset)\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;66;03m# check that covariates and dimensions are matching what we had during training\u001b[39;00m\n\u001b[1;32m-> 1623\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verify_predict_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_series_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m roll_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1626\u001b[0m     roll_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_chunk_length\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:2958\u001b[0m, in \u001b[0;36mMixedCovariatesTorchModel._verify_predict_sample\u001b[1;34m(self, predict_sample)\u001b[0m\n\u001b[0;32m   2957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_verify_predict_sample\u001b[39m(\u001b[38;5;28mself\u001b[39m, predict_sample: \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m-> 2958\u001b[0m     \u001b[43m_mixed_compare_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_sample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:2592\u001b[0m, in \u001b[0;36m_mixed_compare_sample\u001b[1;34m(train_sample, predict_sample)\u001b[0m\n\u001b[0;32m   2583\u001b[0m raise_if(\n\u001b[0;32m   2584\u001b[0m     ds_in_train \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ds_in_predict,\n\u001b[0;32m   2585\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has been trained with `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`; some `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` of matching dimensionality are needed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2586\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor prediction.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2587\u001b[0m )\n\u001b[0;32m   2588\u001b[0m raise_if(\n\u001b[0;32m   2589\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m ds_in_train \u001b[38;5;129;01mand\u001b[39;00m ds_in_predict,\n\u001b[0;32m   2590\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has been trained without `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`; No `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` should be provided for prediction.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2591\u001b[0m )\n\u001b[1;32m-> 2592\u001b[0m \u001b[43mraise_if\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mds_in_train\u001b[49m\n\u001b[0;32m   2594\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds_in_predict\u001b[49m\n\u001b[0;32m   2595\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpredict_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2597\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstatic_covariates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpredict_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m   2599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2600\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe provided `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mds_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m` must have equal dimensionality as the `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mds_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m` used for training the model.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2601\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\logging.py:105\u001b[0m, in \u001b[0;36mraise_if\u001b[1;34m(condition, message, logger)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraise_if\u001b[39m(\n\u001b[0;32m     83\u001b[0m     condition: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m     84\u001b[0m     message: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     85\u001b[0m     logger: logging\u001b[38;5;241m.\u001b[39mLogger \u001b[38;5;241m=\u001b[39m get_logger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_logger\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     86\u001b[0m ):\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m    Checks provided boolean condition and raises a ValueError if it evaluates to True.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m    It logs the error to the provided logger before raising it.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m        if `condition` is satisfied\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[43mraise_if_not\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\logging.py:79\u001b[0m, in \u001b[0;36mraise_if_not\u001b[1;34m(condition, message, logger)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m condition:\n\u001b[0;32m     78\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValueError: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m message)\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The provided `past_covariates` must have equal dimensionality as the `past_covariates` used for training the model."
     ]
    }
   ],
   "source": [
    "# Finally, predict 12 months into the future from the end of the training dataset\n",
    "scaled_total = targetScaler.transform(target_train)\n",
    "scaled_total_exo = exoScaler.transform(exogenous_train)\n",
    "\n",
    "scaled_total_exo = scaled_total_exo.drop_columns(ranked_features[109:])\n",
    "\n",
    "# may error here if the \n",
    "pred = model.predict(1,series=scaled_total,past_covariates=scaled_total_exo)\n",
    "finalout = targetScaler.inverse_transform(pred)\n",
    "print(f'Final predictions:\\n{final}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(os.path.join(project_root, 'Predictions', 'Tide.npy'), finalout.values().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a433c3b09d462586585d2efcbabab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 03:22:39,994 - ERROR - ValueError: For the given forecasting horizon `n=3`, the provided past covariates at dataset index `0` do not extend far enough into the future. As `n <= output_chunk_length` the past covariates must end at time step `2024-03-01 00:00:00`, whereas now they end at time step `2023-12-01 00:00:00`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "For the given forecasting horizon `n=3`, the provided past covariates at dataset index `0` do not extend far enough into the future. As `n <= output_chunk_length` the past covariates must end at time step `2024-03-01 00:00:00`, whereas now they end at time step `2023-12-01 00:00:00`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m horizion3 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaled_total\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaled_total_exo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     horizion3\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Extend the series with the latest prediction for next iteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\utils\\torch.py:80\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[0;32m     79\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1530\u001b[0m, in \u001b[0;36mTorchForecastingModel.predict\u001b[1;34m(self, n, series, past_covariates, future_covariates, trainer, batch_size, verbose, n_jobs, roll_size, num_samples, dataloader_kwargs, mc_dropout, predict_likelihood_parameters, show_warnings)\u001b[0m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m   1512\u001b[0m     n,\n\u001b[0;32m   1513\u001b[0m     series,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1518\u001b[0m     show_warnings\u001b[38;5;241m=\u001b[39mshow_warnings,\n\u001b[0;32m   1519\u001b[0m )\n\u001b[0;32m   1521\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_inference_dataset(\n\u001b[0;32m   1522\u001b[0m     target\u001b[38;5;241m=\u001b[39mseries,\n\u001b[0;32m   1523\u001b[0m     n\u001b[38;5;241m=\u001b[39mn,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1527\u001b[0m     bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1528\u001b[0m )\n\u001b[1;32m-> 1530\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_from_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroll_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroll_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmc_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmc_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_likelihood_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_likelihood_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m called_with_single_series \u001b[38;5;28;01melse\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\utils\\torch.py:80\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[0;32m     79\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1623\u001b[0m, in \u001b[0;36mTorchForecastingModel.predict_from_dataset\u001b[1;34m(self, n, input_series_dataset, trainer, batch_size, verbose, n_jobs, roll_size, num_samples, dataloader_kwargs, mc_dropout, predict_likelihood_parameters)\u001b[0m\n\u001b[0;32m   1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_inference_dataset_type(input_series_dataset)\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;66;03m# check that covariates and dimensions are matching what we had during training\u001b[39;00m\n\u001b[1;32m-> 1623\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_predict_sample(\u001b[43minput_series_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m roll_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1626\u001b[0m     roll_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_chunk_length\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\utils\\data\\inference_dataset.py:711\u001b[0m, in \u001b[0;36mMixedCovariatesInferenceDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m, idx\n\u001b[0;32m    694\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    702\u001b[0m     Union[pd\u001b[38;5;241m.\u001b[39mTimestamp, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    703\u001b[0m ]:\n\u001b[0;32m    704\u001b[0m     (\n\u001b[0;32m    705\u001b[0m         past_target,\n\u001b[0;32m    706\u001b[0m         past_covariate,\n\u001b[0;32m    707\u001b[0m         future_past_covariate,\n\u001b[0;32m    708\u001b[0m         static_covariate,\n\u001b[0;32m    709\u001b[0m         ts_target,\n\u001b[0;32m    710\u001b[0m         pred_point,\n\u001b[1;32m--> 711\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds_past\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    712\u001b[0m     _, historic_future_covariate, future_covariate, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds_future[idx]\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    714\u001b[0m         past_target,\n\u001b[0;32m    715\u001b[0m         past_covariate,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    721\u001b[0m         pred_point,\n\u001b[0;32m    722\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\utils\\data\\inference_dataset.py:419\u001b[0m, in \u001b[0;36mPastCovariatesInferenceDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m\n\u001b[0;32m    411\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m     Union[pd\u001b[38;5;241m.\u001b[39mTimestamp, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    418\u001b[0m ]:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\utils\\data\\inference_dataset.py:292\u001b[0m, in \u001b[0;36mGenericInferenceDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    287\u001b[0m covariate_series \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariates[series_idx]\n\u001b[0;32m    289\u001b[0m )\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m covariate_series \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;66;03m# get start and end indices (integer) of the covariates including historic and future parts\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     covariate_start, covariate_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_covariate_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseries_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_series\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_start_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcovariate_series\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcovariate_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcovariate_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariate_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_chunk_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_chunk_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_chunk_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_chunk_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_chunk_shift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_chunk_shift\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;66;03m# extract covariate values and split into a past (historic) and future part\u001b[39;00m\n\u001b[0;32m    305\u001b[0m     covariate \u001b[38;5;241m=\u001b[39m covariate_series\u001b[38;5;241m.\u001b[39mrandom_component_values(copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\n\u001b[0;32m    306\u001b[0m         covariate_start:covariate_end\n\u001b[0;32m    307\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\utils\\data\\inference_dataset.py:109\u001b[0m, in \u001b[0;36mInferenceDataset._covariate_indexer\u001b[1;34m(target_idx, past_start, past_end, covariate_series, covariate_type, input_chunk_length, output_chunk_length, output_chunk_shift, n)\u001b[0m\n\u001b[0;32m     99\u001b[0m     raise_log(\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    101\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor the given forecasting case, the provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_covariate_type\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m covariates at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m         logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m covariate_series\u001b[38;5;241m.\u001b[39mend_time() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m future_end:\n\u001b[1;32m--> 109\u001b[0m     \u001b[43mraise_log\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;167;43;01mValueError\u001b[39;49;00m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFor the given forecasting horizon `n=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m`, the provided \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmain_covariate_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m covariates \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mat dataset index `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m` do not extend far enough into the future. As `\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn > output_chunk_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mn\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43moutput_chunk_length\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn <= output_chunk_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m` the \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmain_covariate_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m covariates must end at time step `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfuture_end\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m`, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    115\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhereas now they end at time step `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcovariate_series\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m`.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# extract the index position (integer index) from time_index value\u001b[39;00m\n\u001b[0;32m    121\u001b[0m covariate_start \u001b[38;5;241m=\u001b[39m covariate_series\u001b[38;5;241m.\u001b[39mtime_index\u001b[38;5;241m.\u001b[39mget_loc(past_start)\n",
      "File \u001b[1;32mc:\\Users\\James\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\darts\\logging.py:132\u001b[0m, in \u001b[0;36mraise_log\u001b[1;34m(exception, logger)\u001b[0m\n\u001b[0;32m    129\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exception)\n\u001b[0;32m    130\u001b[0m logger\u001b[38;5;241m.\u001b[39merror(exception_type \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m message)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mValueError\u001b[0m: For the given forecasting horizon `n=3`, the provided past covariates at dataset index `0` do not extend far enough into the future. As `n <= output_chunk_length` the past covariates must end at time step `2024-03-01 00:00:00`, whereas now they end at time step `2023-12-01 00:00:00`."
     ]
    }
   ],
   "source": [
    "horizion3 = []\n",
    "for i in range(4):\n",
    "    pred = model.predict(n=3,series=scaled_total,past_covariates=scaled_total_exo)\n",
    "    horizion3.append(pred)\n",
    "    \n",
    "    # Extend the series with the latest prediction for next iteration\n",
    "    scaled_total = scaled_total.append(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
