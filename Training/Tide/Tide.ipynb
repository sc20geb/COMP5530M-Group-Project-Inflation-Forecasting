{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import TiDEModel\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up two levels from notebook (Training/MLR) to project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"Project root added to sys.path:\", project_root)\n",
    "# Ensure the model save directory exists\n",
    "model_save_path = os.path.join('.')\n",
    "os.makedirs(model_save_path, exist_ok=True)  # Creates directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Training.Helper.dataPreprocessing import TRAIN_DATA_PATH_1990S, get_untransformed_exog\n",
    "date_col = 'observation_date'\n",
    "\n",
    "# Load and format training data (only using PCEPI)\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH_1990S)\n",
    "train_df = get_untransformed_exog(train_df)\n",
    "# Convert the date column to datetime format\n",
    "train_df[date_col] = pd.to_datetime(train_df[date_col], format='%m/%Y')\n",
    "\n",
    "# Set the date column as the index\n",
    "train_df.set_index(date_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "target_series = TimeSeries.from_series(train_df['fred_PCEPI'])\n",
    "\n",
    "# Extract the exogenous variables (all columns except 'fred_PCEPI')\n",
    "exogenous_variables = train_df.drop(columns=['fred_PCEPI'])\n",
    "exogenous_series = TimeSeries.from_dataframe(exogenous_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TiDEModel(input_chunk_length=24, output_chunk_length=6)\n",
    "\n",
    "# Fit the model with the target series and exogenous variables\n",
    "model.fit(series=target_series,past_covariates=exogenous_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(6)\n",
    "pred.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is terrible "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split validation and training then scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.dataPreprocessing import TRAIN_DATA_SPLIT\n",
    "train_target, val_target = target_series.split_after(TRAIN_DATA_SPLIT)\n",
    "train_exo, val_exo = exogenous_series.split_after(TRAIN_DATA_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetScaler = Scaler()  # default uses sklearn's MinMaxScaler\n",
    "scaled_train_target = targetScaler.fit_transform(train_target)\n",
    "scaled_val_target = targetScaler.transform(val_target)\n",
    "\n",
    "exoScaler = Scaler() \n",
    "scaled_train_exo = exoScaler.fit_transform(train_exo)\n",
    "scaled_val_exo = exoScaler.transform(val_exo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_target.plot(label=\"train\")\n",
    "scaled_val_target.plot(label=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_LENGTH = 12\n",
    "\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=1e-3,\n",
    "    mode='min'\n",
    ")\n",
    "lr_scheduler_kwargs = {\n",
    "    \"gamma\": 0.999,\n",
    "}\n",
    "model = TiDEModel(\n",
    "    input_chunk_length=48,\n",
    "    output_chunk_length=OUT_LENGTH,\n",
    "    pl_trainer_kwargs={\"callbacks\": [early_stopper]},\n",
    "    optimizer_kwargs={\"lr\": 1e-3},\n",
    "    lr_scheduler_cls= lr_scheduler.ExponentialLR,\n",
    "    lr_scheduler_kwargs = {\"gamma\": 0.999},\n",
    "    use_reversible_instance_norm = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "model.fit(series=scaled_train_target,past_covariates=scaled_train_exo,val_series=scaled_val_target,val_past_covariates=scaled_val_exo,verbose=True)\n",
    "# Make a copy of the model for potential later use\n",
    "original_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_size must be <= OUT_LENGTH\n",
    "prediction_size = OUT_LENGTH\n",
    "predictions = model.predict(prediction_size, verbose=False)\n",
    "transformed_predictions = targetScaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.metrics import mse\n",
    "from Evaluation.Helper.evaluation_helpers import calc_metrics_arrays\n",
    "\n",
    "train_target[-prediction_size:].plot(label='train')\n",
    "val_target[:prediction_size].plot(label='val')\n",
    "transformed_predictions.plot(label='predictions')\n",
    "original_model_metrics = calc_metrics_arrays(val_target[:prediction_size].values(), transformed_predictions.values(), model_names=['TiDE'])\n",
    "print('Metrics for first model without hyperparameter optimisation:')\n",
    "display(original_model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discontinuity in the above plot between the ground-truth training and validation sets looks incorrect, but that is simply because a natural slight downturn occurred between the two months in question at the end of the training period and the beginning of the validation period respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that good so optuna test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is currently not used, but could be switched to\n",
    "split_date = pd.Timestamp('2022-12')\n",
    "op_train , op_val = target_series.split_after(split_date)\n",
    "op_train_exo , op_val_exo = exogenous_series.split_after(split_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from darts.metrics import mse\n",
    "from Training.Helper.PyTorchModular import optuna_trial_get_kwargs\n",
    "\n",
    "model_search_space = {\n",
    "    'input_chunk_length': (int, (24, 60)),\n",
    "    'num_encoder_layers': (int, (1, 3)),\n",
    "    'num_decoder_layers': (int, (1, 3)),\n",
    "    'hidden_size': (int, (64, 512)),\n",
    "    'dropout': (float, (0.1, 0.5)),\n",
    "    'optimizer_kwargs': {\"lr\": (float, (1e-4, 1e-2))},\n",
    "    'lr_scheduler_kwargs': {\"gamma\": (float, (0.9, 1.0))},\n",
    "    'use_reversible_instance_norm': ('categorical', [True, False]),\n",
    "}\n",
    "\n",
    "# Controlling input chunk length for now to decrease the size of the search space\n",
    "model_invariates = {\n",
    "    #'input_chunk_length': 48,\n",
    "    'output_chunk_length': 12,\n",
    "    'lr_scheduler_cls': lr_scheduler.ExponentialLR,\n",
    "    'pl_trainer_kwargs': {\"callbacks\": [early_stopper]}\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    model_kwargs = optuna_trial_get_kwargs(trial, model_search_space)\n",
    "\n",
    "    # Initialize the TiDEModel with suggested hyperparameters\n",
    "    model = TiDEModel(**model_kwargs, **model_invariates)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(series = scaled_train_target,\n",
    "              past_covariates = scaled_train_exo,\n",
    "              val_series = scaled_val_target,\n",
    "              val_past_covariates = scaled_val_exo,\n",
    "              epochs=1000,\n",
    "              verbose = False)\n",
    "\n",
    "    # Evaluate the model\n",
    "    # (this is an alternative option for evaluation, where the model must predict the final prediction_size elements of the validation data having been given all other validation data;\n",
    "    #  if switching to this method, ensure that final prediction is performed with the same setup (this is currently done just by predicting the next n values))\n",
    "    #scaled_val_predictions = model.predict(n=prediction_size,series=scaled_val_target[:-prediction_size],past_covariates=scaled_val_exo[:-prediction_size], verbose=False)]\n",
    "    #val_predictions = targetScaler.inverse_transform(scaled_val_predictions, verbose=False)\n",
    "    #error = mse(val_target[-prediction_size:], val_predictions, verbose=False)\n",
    "\n",
    "    # Raw output is scaled, so inverse transform to become comparable with validation set\n",
    "    scaled_val_predictions = model.predict(n=prediction_size, verbose=False)\n",
    "    val_predictions = targetScaler.inverse_transform(scaled_val_predictions, verbose=False)\n",
    "    # Only uses the first prediction_size values of val_target, since this is the size of the prediction made by the model\n",
    "    error = mse(val_target[:prediction_size], val_predictions, verbose=False)\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.PyTorchModular import MAX_DEPTH\n",
    "\n",
    "def reformat_best_params(best_params, model_search_space, cur_depth=0):\n",
    "    if cur_depth > MAX_DEPTH: raise RecursionError(f'Cannot exceed recursion depth of {MAX_DEPTH}')\n",
    "    reformatted = {}\n",
    "    for key in model_search_space:\n",
    "        if key in best_params: reformatted[key] = best_params[key]\n",
    "        else: reformatted[key] = reformat_best_params(best_params, model_search_space[key], cur_depth=cur_depth+1)\n",
    "    return reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best hyperparameters\n",
    "best_params = study.best_params\n",
    "# Format parameters returned by study into the same style as the search space definition (can be passed straight into model as kwargs)\n",
    "best_params = reformat_best_params(best_params, model_search_space)\n",
    "print('Best hyperparameters:')\n",
    "display(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best params made into a model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TiDEModel with suggested hyperparameters\n",
    "best_model = TiDEModel(**best_params, **model_invariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fossilised suggested hyperparameters\n",
    "    # Define early stopping callback\n",
    "#early_stopper = EarlyStopping(\n",
    "#    monitor='val_loss',\n",
    "#    patience=10,\n",
    "#    min_delta=1e-3,\n",
    "#    mode='min'\n",
    "#)\n",
    "#model = TiDEModel(\n",
    "#    input_chunk_length=29,\n",
    "#    output_chunk_length=12,\n",
    "#    num_encoder_layers=3,\n",
    "#    num_decoder_layers=2,\n",
    "#    hidden_size=443,\n",
    "#    dropout= 0.19411763114257125,\n",
    "#    optimizer_kwargs={\"lr\": 0.00014544898516544107},\n",
    "#    lr_scheduler_cls=torch.optim.lr_scheduler.ExponentialLR,\n",
    "#    lr_scheduler_kwargs={\"gamma\": 0.9645025339005199},\n",
    "#    pl_trainer_kwargs={\"callbacks\": [early_stopper]},\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "best_model.fit(series=scaled_train_target,past_covariates=scaled_train_exo, val_series=scaled_val_target, val_past_covariates=scaled_val_exo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict over the test horizon\n",
    "TEST_HORIZON = 12\n",
    "val_predictions = best_model.predict(n=TEST_HORIZON)\n",
    "transformed_predictions = targetScaler.inverse_transform(val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation.Helper.evaluation_helpers import display_results\n",
    "\n",
    "actuals = val_target[:TEST_HORIZON].values()\n",
    "preds = transformed_predictions.values()\n",
    "\n",
    "display_results(actuals, preds, val_target[:TEST_HORIZON].time_index, 'TiDE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(f'Optuna trained model metrics on validation set:')\n",
    "best_model_metrics = calc_metrics_arrays(actuals, preds, model_names=['TiDE'])\n",
    "display(best_model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(original_model_metrics, best_model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the model from the that has the best RMSE and take that as the best\n",
    "if original_model_metrics.values.flatten()[0] < best_model_metrics.values.flatten()[0]:\n",
    "    best_model = original_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(\"tide.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go from here if wanting to load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import TiDEModel\n",
    "\n",
    "# Load the model from the file\n",
    "model = TiDEModel.load(\"tide.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, predict 12 months into the future from the end of the training dataset\n",
    "scaled_total = targetScaler.transform(target_series)\n",
    "scaled_total_exo = exoScaler.transform(exogenous_series)\n",
    "\n",
    "pred = model.predict(12,series=scaled_total,past_covariates=scaled_total_exo)\n",
    "finalout = targetScaler.inverse_transform(pred)\n",
    "print(f'Final predictions:\\n{finalout.to_dataframe()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(os.path.join(project_root, 'Predictions', 'Tide.npy'), finalout.values().flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
