{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import TiDEModel\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping as EarlyStopping_lightning\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up two levels from notebook (Training/MLR) to project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"Project root added to sys.path:\", project_root)\n",
    "# Ensure the model save directory exists\n",
    "model_save_path = os.path.join('.')\n",
    "os.makedirs(model_save_path, exist_ok=True)  # Creates directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Training.Helper.dataPreprocessing import TRAIN_DATA_PATH_1990S, get_untransformed_exog\n",
    "date_col = 'observation_date'\n",
    "\n",
    "# Load and format training data (only using PCEPI)\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH_1990S)\n",
    "train_df = get_untransformed_exog(train_df)\n",
    "# Convert the date column to datetime format\n",
    "train_df[date_col] = pd.to_datetime(train_df[date_col], format='%m/%Y')\n",
    "\n",
    "# Set the date column as the index\n",
    "train_df.set_index(date_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.dataPreprocessing import rank_features_ccf\n",
    "\n",
    "# Ranks all non-date variables\n",
    "ranked_features = rank_features_ccf(train_df)\n",
    "\n",
    "# Define the number of features that should be used\n",
    "FEATURES_TO_USE = len(ranked_features) #ensure this is all features for Optuna to work properly\n",
    "used_features = ranked_features[:FEATURES_TO_USE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_df(train_df, ranked_features, features_to_use):\n",
    "    return train_df.loc[:,ranked_features[:features_to_use]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'fred_PCEPI'\n",
    "\n",
    "train_df_ranked = get_ranked_df(train_df, ranked_features, FEATURES_TO_USE)\n",
    "train_df_ranked[target_col] = train_df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "\n",
    "def get_target_exogenous_series(train_df, target_col):\n",
    "    target_series = TimeSeries.from_series(train_df[target_col])\n",
    "\n",
    "    # Extract the exogenous variables (all columns except 'fred_PCEPI')\n",
    "    exogenous_variables = train_df.drop(columns=[target_col])\n",
    "    exogenous_series = TimeSeries.from_dataframe(exogenous_variables)\n",
    "    return target_series, exogenous_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "\n",
    "target_series, exogenous_series = get_target_exogenous_series(train_df, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_series_ranked, exogenous_series_ranked = get_target_exogenous_series(train_df_ranked, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TiDEModel(input_chunk_length=24, output_chunk_length=6)\n",
    "\n",
    "# Fit the model with the target series and exogenous variables\n",
    "model.fit(series=target_series,past_covariates=exogenous_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(6)\n",
    "pred.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is terrible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TiDEModel(input_chunk_length=24, output_chunk_length=6)\n",
    "\n",
    "# Fit the model with the target series and exogenous variables\n",
    "model.fit(series=target_series_ranked,past_covariates=exogenous_series_ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(6)\n",
    "pred.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot more reasonable - indicating that having too many features causes the model problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split validation and training then scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform_TimeSeries(train_ts, val_ts, scaler):\n",
    "    scaled_train = scaler.fit_transform(train_ts)\n",
    "    scaled_val = scaler.transform(val_ts)\n",
    "    return scaled_train, scaled_val, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.dataPreprocessing import TRAIN_DATA_SPLIT\n",
    "def split_and_scale_TimeSeries(target_series, exogenous_series):\n",
    "    train_target, val_target = target_series.split_after(TRAIN_DATA_SPLIT)\n",
    "    train_exo, val_exo = exogenous_series.split_after(TRAIN_DATA_SPLIT)\n",
    "\n",
    "    # default uses sklearn's MinMaxScaler\n",
    "    scaled_train_target, scaled_val_target, targetScaler = fit_transform_TimeSeries(train_target, val_target, Scaler())\n",
    "    scaled_train_exo, scaled_val_exo, exoScaler = fit_transform_TimeSeries(train_exo, val_exo, Scaler())\n",
    "    return scaled_train_target, scaled_train_exo, scaled_val_target, scaled_val_exo, targetScaler, exoScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_target, scaled_train_exo, scaled_val_target, scaled_val_exo, targetScaler, exoScaler = split_and_scale_TimeSeries(target_series, exogenous_series)\n",
    "scaled_train_target_r, scaled_train_exo_r, scaled_val_target_r, scaled_val_exo_r, targetScaler_r, exoScaler_r = split_and_scale_TimeSeries(target_series_ranked, exogenous_series_ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for the plot of unscaled series later\n",
    "train_target, val_target = target_series.split_after(TRAIN_DATA_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_target.plot(label=\"train\")\n",
    "scaled_val_target.plot(label=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_LENGTH = 12\n",
    "\n",
    "early_stopper = EarlyStopping_lightning(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=1e-3,\n",
    "    mode='min'\n",
    ")\n",
    "lr_scheduler_kwargs = {\n",
    "    \"gamma\": 0.999,\n",
    "}\n",
    "model = TiDEModel(\n",
    "    input_chunk_length=48,\n",
    "    output_chunk_length=OUT_LENGTH,\n",
    "    pl_trainer_kwargs={\"callbacks\": [early_stopper]},\n",
    "    optimizer_kwargs={\"lr\": 1e-3},\n",
    "    lr_scheduler_cls= lr_scheduler.ExponentialLR,\n",
    "    lr_scheduler_kwargs = {\"gamma\": 0.999},\n",
    "    use_reversible_instance_norm = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "model.fit(series=scaled_train_target,past_covariates=scaled_train_exo,val_series=scaled_val_target,val_past_covariates=scaled_val_exo,verbose=True)\n",
    "# Make a copy of the model for potential later use\n",
    "original_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_size must be <= OUT_LENGTH\n",
    "prediction_size = OUT_LENGTH\n",
    "predictions = model.predict(prediction_size, verbose=False)\n",
    "transformed_predictions = targetScaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output_chunk_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.metrics import mse\n",
    "from Evaluation.Helper.evaluation_helpers import calc_metrics_arrays\n",
    "\n",
    "train_target[-prediction_size:].plot(label='train')\n",
    "val_target[:prediction_size].plot(label='val')\n",
    "transformed_predictions.plot(label='predictions')\n",
    "original_model_metrics = calc_metrics_arrays(val_target[:prediction_size].values(), transformed_predictions.values(), model_names=['TiDE'])\n",
    "print('Metrics for first model without hyperparameter optimisation:')\n",
    "display(original_model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discontinuity in the above plot between the ground-truth training and validation sets looks incorrect, but that is simply because a natural slight downturn occurred between the two months in question at the end of the training period and the beginning of the validation period respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that good so optuna test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is currently not used, but could be switched to\n",
    "split_date = pd.Timestamp('2022-12')\n",
    "op_train , op_val = target_series.split_after(split_date)\n",
    "op_train_exo , op_val_exo = exogenous_series.split_after(split_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.metrics import mse\n",
    "from Training.Helper.PyTorchModular import optuna_trial_get_kwargs\n",
    "\n",
    "def get_optuna_ranked_series(trial, scaled_train_exo, scaled_val_exo, ranked_features):\n",
    "    n_features = optuna_trial_get_kwargs(trial, {'n': (int, 1, scaled_train_exo.n_components)})['n']\n",
    "    return scaled_train_exo.drop_columns(ranked_features[n_features:]), scaled_val_exo.drop_columns(ranked_features[n_features:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from darts.metrics import mse\n",
    "from Training.Helper.PyTorchModular import optuna_trial_get_kwargs\n",
    "\n",
    "model_search_space = {\n",
    "    'input_chunk_length': (int, 24, 60),\n",
    "    'num_encoder_layers': (int, 1, 3),\n",
    "    'num_decoder_layers': (int, 1, 3),\n",
    "    'hidden_size': (int, 64, 512),\n",
    "    'dropout': (float, 0.1, 0.5, {'log': True}),\n",
    "    'optimizer_kwargs': {\"lr\": (float, 1e-4, 1e-2)},\n",
    "    'lr_scheduler_kwargs': {\"gamma\": (float, 0.9, 1.0)},\n",
    "    'use_reversible_instance_norm': ('categorical', [True, False]),\n",
    "}\n",
    "\n",
    "# Controlling input chunk length for now to decrease the size of the search space\n",
    "model_invariates = {\n",
    "    #'input_chunk_length': 48,\n",
    "    'output_chunk_length': 12,\n",
    "    'lr_scheduler_cls': lr_scheduler.ExponentialLR,\n",
    "    'pl_trainer_kwargs': {\"callbacks\": [early_stopper]}\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    model_kwargs = optuna_trial_get_kwargs(trial, model_search_space)\n",
    "\n",
    "    scaled_train_exo_ranked, scaled_val_exo_ranked = get_optuna_ranked_series(trial, scaled_train_exo_r, scaled_val_exo_r, ranked_features)\n",
    "\n",
    "    # Initialize the TiDEModel with suggested hyperparameters\n",
    "    model = TiDEModel(**model_kwargs, **model_invariates)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(series = scaled_train_target,\n",
    "              past_covariates = scaled_train_exo_ranked,\n",
    "              val_series = scaled_val_target,\n",
    "              val_past_covariates = scaled_val_exo_ranked,\n",
    "              epochs=1000,\n",
    "              verbose = False)\n",
    "\n",
    "    # Evaluate the model\n",
    "    # (this is an alternative option for evaluation, where the model must predict the final prediction_size elements of the validation data having been given all other validation data;\n",
    "    #  if switching to this method, ensure that final prediction is performed with the same setup (this is currently done just by predicting the next n values))\n",
    "    #scaled_val_predictions = model.predict(n=prediction_size,series=scaled_val_target[:-prediction_size],past_covariates=scaled_val_exo[:-prediction_size], verbose=False)]\n",
    "    #val_predictions = targetScaler.inverse_transform(scaled_val_predictions, verbose=False)\n",
    "    #error = mse(val_target[-prediction_size:], val_predictions, verbose=False)\n",
    "\n",
    "    # Raw output is scaled, so inverse transform to become comparable with validation set\n",
    "    scaled_val_predictions = model.predict(n=prediction_size, verbose=False)\n",
    "    val_predictions = targetScaler.inverse_transform(scaled_val_predictions, verbose=False)\n",
    "    # Only uses the first prediction_size values of val_target, since this is the size of the prediction made by the model\n",
    "    error = mse(val_target[:prediction_size], val_predictions, verbose=False)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the generalised function - works for now but no find-best-feature-split functionality\n",
    "from Training.Helper.PyTorchModular import optuna_tune_and_train_darts\n",
    "\n",
    "amazing_model = optuna_tune_and_train_darts(TiDEModel,\n",
    "                                            scaled_train_target, scaled_val_target, scaled_train_exo, scaled_val_exo,\n",
    "                                            model_search_space, model_invariates,\n",
    "                                            n_trials=5, n_epochs_per_trial=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.PyTorchModular import reformat_best_params\n",
    "# Retrieve the best hyperparameters\n",
    "best_params = study.best_params\n",
    "# Get the 'n' parameter out of the best_params dictionary and extract just the value\n",
    "best_n_features = reformat_best_params(best_params, {'n': (int, (1, 2))})['n']\n",
    "# Format parameters returned by study into the same style as the search space definition (can be passed straight into model as kwargs)\n",
    "best_params = reformat_best_params(best_params, model_search_space)\n",
    "print('Best hyperparameters:')\n",
    "display(best_params)\n",
    "print('Best number of features to include:', best_n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best parameters into a model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TiDEModel with suggested hyperparameters\n",
    "best_model = TiDEModel(**best_params, **model_invariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fossilised suggested hyperparameters\n",
    "    # Define early stopping callback\n",
    "#early_stopper = EarlyStopping_lightning(\n",
    "#    monitor='val_loss',\n",
    "#    patience=10,\n",
    "#    min_delta=1e-3,\n",
    "#    mode='min'\n",
    "#)\n",
    "#model = TiDEModel(\n",
    "#    input_chunk_length=29,\n",
    "#    output_chunk_length=12,\n",
    "#    num_encoder_layers=3,\n",
    "#    num_decoder_layers=2,\n",
    "#    hidden_size=443,\n",
    "#    dropout= 0.19411763114257125,\n",
    "#    optimizer_kwargs={\"lr\": 0.00014544898516544107},\n",
    "#    lr_scheduler_cls=torch.optim.lr_scheduler.ExponentialLR,\n",
    "#    lr_scheduler_kwargs={\"gamma\": 0.9645025339005199},\n",
    "#    pl_trainer_kwargs={\"callbacks\": [early_stopper]},\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "#best_model.fit(series=scaled_train_target,past_covariates=scaled_train_exo, val_series=scaled_val_target, val_past_covariates=scaled_val_exo)\n",
    "best_model.fit(series=scaled_train_target,\n",
    "               past_covariates=scaled_train_exo.drop_columns(ranked_features[best_n_features:]),\n",
    "               val_series=scaled_val_target,\n",
    "               val_past_covariates=scaled_val_exo.drop_columns(ranked_features[best_n_features:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict over the test horizon\n",
    "TEST_HORIZON = 12\n",
    "val_predictions = best_model.predict(n=TEST_HORIZON)\n",
    "transformed_predictions = targetScaler.inverse_transform(val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation.Helper.evaluation_helpers import display_results\n",
    "\n",
    "actuals = val_target[:TEST_HORIZON].values()\n",
    "preds = transformed_predictions.values()\n",
    "\n",
    "display_results(actuals, preds, val_target[:TEST_HORIZON].time_index, 'TiDE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(f'Optuna trained model metrics on validation set:')\n",
    "best_model_metrics = calc_metrics_arrays(actuals, preds, model_names=['TiDE'])\n",
    "display(best_model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(original_model_metrics, best_model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the model from the that has the best RMSE and take that as the best\n",
    "if original_model_metrics.values.flatten()[0] < best_model_metrics.values.flatten()[0]:\n",
    "    best_model = original_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(\"tide.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go from here if wanting to load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import TiDEModel\n",
    "\n",
    "# Load the model from the file\n",
    "model = TiDEModel.load(\"tide.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, predict 12 months into the future from the end of the training dataset\n",
    "scaled_total = targetScaler.transform(target_series)\n",
    "scaled_total_exo = exoScaler.transform(exogenous_series)\n",
    "\n",
    "pred = model.predict(12,series=scaled_total,past_covariates=scaled_total_exo)\n",
    "finalout = targetScaler.inverse_transform(pred)\n",
    "print(f'Final predictions:\\n{finalout.to_dataframe()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(os.path.join(project_root, 'Predictions', 'Tide.npy'), finalout.values().flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
