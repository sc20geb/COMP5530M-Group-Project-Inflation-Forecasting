{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import TiDEModel\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping as EarlyStopping_lightning\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up two levels from notebook (Training/MLR) to project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"Project root added to sys.path:\", project_root)\n",
    "# Ensure the model save directory exists\n",
    "model_save_path = os.path.join('.')\n",
    "os.makedirs(model_save_path, exist_ok=True)  # Creates directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Training.Helper.dataPreprocessing import TRAIN_DATA_PATH_1990S, get_untransformed_exog, TEST_DATA_PATH_1990S\n",
    "date_col = 'observation_date'\n",
    "\n",
    "# Load and format training data (only using PCEPI)\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH_1990S)\n",
    "train_df = get_untransformed_exog(train_df)\n",
    "train_df['observation_date'] = pd.to_datetime(train_df['observation_date'], format='%m/%Y')\n",
    "train_df.set_index('observation_date', inplace=True)\n",
    "\n",
    "test_df = pd.read_csv(TEST_DATA_PATH_1990S)\n",
    "test_df= get_untransformed_exog(test_df)\n",
    "test_df['observation_date'] = pd.to_datetime(test_df['observation_date'], format='%m/%Y')\n",
    "test_df.set_index('observation_date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.dataPreprocessing import rank_features_ccf\n",
    "\n",
    "# Ranks all non-date variables\n",
    "ranked_features = rank_features_ccf(train_df)\n",
    "    \n",
    "# Define the number of features that should be used\n",
    "FEATURES_TO_USE = len(ranked_features) #ensure this is all features for Optuna to work properly\n",
    "used_features = ranked_features[:FEATURES_TO_USE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_df(df, ranked_features, features_to_use):\n",
    "    return df.loc[:,ranked_features[:features_to_use]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target_col = \"fred_PCEPI\"\n",
    "\n",
    "train_df_ranked = get_ranked_df(train_df, ranked_features, FEATURES_TO_USE)\n",
    "train_df_ranked[target_col] = train_df[target_col]\n",
    "\n",
    "train_exog= train_df.drop('fred_PCEPI',axis=1)\n",
    "train_target = train_df['fred_PCEPI']\n",
    "\n",
    "\n",
    "test_exog= test_df.drop('fred_PCEPI',axis=1)\n",
    "test_target= test_df['fred_PCEPI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "exogenous_train = TimeSeries.from_dataframe(train_exog)\n",
    "target_train = TimeSeries.from_series(train_df['fred_PCEPI'])\n",
    "\n",
    "exogenous_test = TimeSeries.from_dataframe(test_exog)\n",
    "target_test = TimeSeries.from_series(test_df['fred_PCEPI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split validation and training then scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform_TimeSeries(train_ts, val_ts, scaler):\n",
    "    scaled_train = scaler.fit_transform(train_ts)\n",
    "    scaled_val = scaler.transform(val_ts)\n",
    "    return scaled_train, scaled_val, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.dataPreprocessing import TRAIN_DATA_SPLIT\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "def split_and_scale_TimeSeries(target_series, exogenous_series):\n",
    "    train_target, val_target = target_series.split_after(TRAIN_DATA_SPLIT)\n",
    "    train_exo, val_exo = exogenous_series.split_after(TRAIN_DATA_SPLIT)\n",
    "    \n",
    "    # default uses sklearn's MinMaxScaler\n",
    "    scaled_train_target, scaled_val_target, targetScaler = fit_transform_TimeSeries(train_target, val_target, Scaler(RobustScaler()))\n",
    "    scaled_train_exo, scaled_val_exo, exoScaler = fit_transform_TimeSeries(train_exo, val_exo, Scaler(RobustScaler()))\n",
    "    return scaled_train_target, scaled_train_exo, scaled_val_target, scaled_val_exo, targetScaler, exoScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_target, scaled_train_exo, scaled_val_target, scaled_val_exo, targetScaler, exoScaler = split_and_scale_TimeSeries(target_train, exogenous_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_LENGTH = 12\n",
    "\n",
    "early_stopper = EarlyStopping_lightning(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=1e-3,\n",
    "    mode='min'\n",
    ")\n",
    "lr_scheduler_kwargs = {\n",
    "    \"gamma\": 0.999,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.metrics import mse\n",
    "from Training.Helper.PyTorchModular import optuna_trial_get_kwargs,load_prediction\n",
    "\n",
    "def get_optuna_ranked_series(trial, scaled_train_exo, scaled_val_exo, ranked_features):\n",
    "    n_features = optuna_trial_get_kwargs(trial, {'n': (int, 1, scaled_train_exo.n_components)})['n']\n",
    "    return scaled_train_exo.drop_columns(ranked_features[n_features:]), scaled_val_exo.drop_columns(ranked_features[n_features:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target, val_target = target_train.split_after(TRAIN_DATA_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from darts.metrics import mse\n",
    "from Training.Helper.PyTorchModular import optuna_trial_get_kwargs\n",
    "\n",
    "model_search_space = {\n",
    "    'input_chunk_length': (int, 24, 60),\n",
    "    'num_encoder_layers': (int, 1, 3),\n",
    "    'num_decoder_layers': (int, 1, 3),\n",
    "    'hidden_size': (int, 64, 512),\n",
    "    'dropout': (float, 0.1, 0.5, {'log': True}),\n",
    "    'optimizer_kwargs': {\"lr\": (float, 1e-4, 1e-2)},\n",
    "    'lr_scheduler_kwargs': {\"gamma\": (float, 0.9, 1.0)},\n",
    "    'use_reversible_instance_norm': ('categorical', [True, False]),\n",
    "}\n",
    "\n",
    "def createObjective(model_invariates,HORIZON):\n",
    "    def objective(trial):\n",
    "\n",
    "        model_kwargs = optuna_trial_get_kwargs(trial, model_search_space)\n",
    "\n",
    "        scaled_train_exo_ranked, scaled_val_exo_ranked = get_optuna_ranked_series(trial, scaled_train_exo, scaled_val_exo, ranked_features)\n",
    "        \n",
    "        # Initialize the TiDEModel with suggested hyperparameters\n",
    "        model = TiDEModel(**model_kwargs, **model_invariates)\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(series = scaled_train_target,\n",
    "                past_covariates = scaled_train_exo_ranked,\n",
    "                val_series = scaled_val_target,\n",
    "                val_past_covariates = scaled_val_exo_ranked,\n",
    "                epochs=1000,\n",
    "                verbose = False)\n",
    "\n",
    "        # Evaluate the model\n",
    "        # (this is an alternative option for evaluation, where the model must predict the final prediction_size elements of the validation data having been given all other validation data;\n",
    "        #  if switching to this method, ensure that final prediction is performed with the same setup (this is currently done just by predicting the next n values))\n",
    "        #scaled_val_predictions = model.predict(n=prediction_size,series=scaled_val_target[:-prediction_size],past_covariates=scaled_val_exo[:-prediction_size], verbose=False)]\n",
    "        #val_predictions = targetScaler.inverse_transform(scaled_val_predictions, verbose=False)\n",
    "        #error = mse(val_target[-prediction_size:], val_predictions, verbose=False)\n",
    "\n",
    "        # Raw output is scaled, so inverse transform to become comparable with validation set\n",
    "        scaled_val_predictions = model.predict(n=HORIZON, verbose=False)\n",
    "        val_predictions = targetScaler.inverse_transform(scaled_val_predictions, verbose=False)\n",
    "        # Only uses the first prediction_size values of val_target, since this is the size of the prediction made by the model\n",
    "        error = mse(val_target[:HORIZON], val_predictions, verbose=False)\n",
    "        return error\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.Helper.PyTorchModular import reformat_best_params\n",
    "def getParams(study):\n",
    "    # Retrieve the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "    # Get the 'n' parameter out of the best_params dictionary and extract just the value\n",
    "    best_n_features = reformat_best_params(best_params, {'n': (int, (1, 2))})['n']\n",
    "    # Format parameters returned by study into the same style as the search space definition (can be passed straight into model as kwargs)\n",
    "    best_params = reformat_best_params(best_params, model_search_space)\n",
    "    print('Best hyperparameters:')\n",
    "    display(best_params)\n",
    "    print('Best number of features to include:', best_n_features)\n",
    "    return best_params ,best_n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tide(model,target,ranked_exo):\n",
    "    scaled_train_target, scaled_train_exo, scaled_val_target, scaled_val_exo, targetScaler, exoScaler = split_and_scale_TimeSeries(target, ranked_exo)\n",
    "    model.fit(series = scaled_train_target,\n",
    "        past_covariates = scaled_train_exo,\n",
    "        val_series = scaled_val_target,\n",
    "        val_past_covariates = scaled_val_exo,\n",
    "        epochs=1000,\n",
    "        verbose = False)\n",
    "    return model, targetScaler,exoScaler\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,train_exog_df, train_target_series, test_exog_df, test_target_series,exog_scaler,target_scaler, input_size,horizon):\n",
    "    \n",
    "    combined_exog_df= pd.concat((train_exog_df,test_exog_df),axis=0)\n",
    "    combined_target_series= pd.concat((train_target_series,test_target_series),axis=0)\n",
    "\n",
    "    preds_list=[]\n",
    "    for i in range(0,12,horizon):\n",
    "        print(test_target.index[i])\n",
    "        x,y=load_prediction(input_size,combined_exog_df,combined_target_series,exog_scaler,target_scaler,test_target.index[i])\n",
    "        #print(x)\n",
    " \n",
    "        values = model.predict(n=horizon,series=y,past_covariates=x,verbose=False)\n",
    "        \n",
    "        pred=target_scaler.inverse_transform(values).values().flatten()\n",
    "\n",
    "        print(test_target.index[i:i+horizon])\n",
    "        print(pred)\n",
    "        preds_list= np.append(preds_list,pred)\n",
    "    \n",
    "    return preds_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horizon 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "HORIZON = 1\n",
    "\n",
    "# Controlling input chunk length for now to decrease the size of the search space\n",
    "model_invariates = {\n",
    "    #'input_chunk_length': 48,\n",
    "    'output_chunk_length': HORIZON,\n",
    "    'lr_scheduler_cls': lr_scheduler.ExponentialLR,\n",
    "    'pl_trainer_kwargs': {\"callbacks\": [early_stopper]}\n",
    "}\n",
    "objective_fn = createObjective(model_invariates=model_invariates, HORIZON=HORIZON)\n",
    "study.optimize(objective_fn, n_trials=50)\n",
    "best_params ,best_n_features = getParams(study)\n",
    "\n",
    "model = TiDEModel(**best_params, **model_invariates)\n",
    "input_size = best_params[\"input_chunk_length\"]\n",
    "\n",
    "train_df_ranked = get_ranked_df(train_df, ranked_features, best_n_features)\n",
    "train_df_ranked[target_col] = train_df[target_col]\n",
    "\n",
    "train_exog= train_df_ranked.drop('fred_PCEPI',axis=1)\n",
    "train_target = train_df['fred_PCEPI']\n",
    "\n",
    "model,targetScaler,exoScaler= train_tide(model=model,target = TimeSeries.from_series(train_target),ranked_exo=TimeSeries.from_dataframe(train_exog))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"horizon1/TideH1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_ranked = get_ranked_df(test_df, ranked_features, best_n_features)\n",
    "test_df_ranked[target_col] = test_df[target_col]\n",
    "\n",
    "test_exog= test_df_ranked.drop('fred_PCEPI',axis=1)\n",
    "test_target = test_df['fred_PCEPI']\n",
    "\n",
    "preds_horizon_1 = predict(model=model,train_exog_df=train_exog,train_target_series=train_target,test_exog_df=test_exog,test_target_series=test_target,exog_scaler=exoScaler,target_scaler=targetScaler,input_size=input_size,horizon=HORIZON)\n",
    "np.save(os.path.join(project_root, 'Predictions/Horizon1', 'Tide_horizon_1.npy'),preds_horizon_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horizon = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "HORIZON = 3\n",
    "\n",
    "# Controlling input chunk length for now to decrease the size of the search space\n",
    "model_invariates = {\n",
    "    #'input_chunk_length': 48,\n",
    "    'output_chunk_length': HORIZON,\n",
    "    'lr_scheduler_cls': lr_scheduler.ExponentialLR,\n",
    "    'pl_trainer_kwargs': {\"callbacks\": [early_stopper]}\n",
    "}\n",
    "objective_fn = createObjective(model_invariates=model_invariates, HORIZON=HORIZON)\n",
    "study.optimize(objective_fn, n_trials=50)\n",
    "best_params ,best_n_features = getParams(study)\n",
    "\n",
    "model = TiDEModel(**best_params, **model_invariates)\n",
    "input_size = best_params[\"input_chunk_length\"]\n",
    "\n",
    "train_df_ranked = get_ranked_df(train_df, ranked_features, best_n_features)\n",
    "train_df_ranked[target_col] = train_df[target_col]\n",
    "\n",
    "train_exog= train_df_ranked.drop('fred_PCEPI',axis=1)\n",
    "train_target = train_df['fred_PCEPI']\n",
    "\n",
    "model,targetScaler,exoScaler= train_tide(model=model,target = TimeSeries.from_series(train_target),ranked_exo=TimeSeries.from_dataframe(train_exog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"horizon3/TideH3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_ranked = get_ranked_df(test_df, ranked_features, best_n_features)\n",
    "test_df_ranked[target_col] = test_df[target_col]\n",
    "\n",
    "test_exog= test_df_ranked.drop('fred_PCEPI',axis=1)\n",
    "test_target = test_df['fred_PCEPI']\n",
    "\n",
    "preds_horizon_3 = predict(model=model,train_exog_df=train_exog,train_target_series=train_target,test_exog_df=test_exog,test_target_series=test_target,exog_scaler=exoScaler,target_scaler=targetScaler,input_size=input_size,horizon=HORIZON)\n",
    "np.save(os.path.join(project_root, 'Predictions/Horizon3', 'Tide_horizon_3.npy'),preds_horizon_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horizon = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "HORIZON = 6\n",
    "\n",
    "# Controlling input chunk length for now to decrease the size of the search space\n",
    "model_invariates = {\n",
    "    #'input_chunk_length': 48,\n",
    "    'output_chunk_length': HORIZON,\n",
    "    'lr_scheduler_cls': lr_scheduler.ExponentialLR,\n",
    "    'pl_trainer_kwargs': {\"callbacks\": [early_stopper]}\n",
    "}\n",
    "objective_fn = createObjective(model_invariates=model_invariates, HORIZON=HORIZON)\n",
    "study.optimize(objective_fn, n_trials=50)\n",
    "best_params ,best_n_features = getParams(study)\n",
    "\n",
    "model = TiDEModel(**best_params, **model_invariates)\n",
    "input_size = best_params[\"input_chunk_length\"]\n",
    "\n",
    "train_df_ranked = get_ranked_df(train_df, ranked_features, best_n_features)\n",
    "train_df_ranked[target_col] = train_df[target_col]\n",
    "\n",
    "train_exog= train_df_ranked.drop('fred_PCEPI',axis=1)\n",
    "train_target = train_df['fred_PCEPI']\n",
    "\n",
    "model,targetScaler,exoScaler= train_tide(model=model,target = TimeSeries.from_series(train_target),ranked_exo=TimeSeries.from_dataframe(train_exog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"horizon6/TideH6.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_ranked = get_ranked_df(test_df, ranked_features, best_n_features)\n",
    "test_df_ranked[target_col] = test_df[target_col]\n",
    "\n",
    "test_exog= test_df_ranked.drop('fred_PCEPI',axis=1)\n",
    "test_target = test_df['fred_PCEPI']\n",
    "\n",
    "preds_horizon_6 = predict(model=model,train_exog_df=train_exog,train_target_series=train_target,test_exog_df=test_exog,test_target_series=test_target,exog_scaler=exoScaler,target_scaler=targetScaler,input_size=input_size,horizon=HORIZON)\n",
    "np.save(os.path.join(project_root, 'Predictions/Horizon6', 'Tide_horizon_6.npy'),preds_horizon_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horizon = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "HORIZON = 12\n",
    "\n",
    "# Controlling input chunk length for now to decrease the size of the search space\n",
    "model_invariates = {\n",
    "    #'input_chunk_length': 48,\n",
    "    'output_chunk_length': HORIZON,\n",
    "    'lr_scheduler_cls': lr_scheduler.ExponentialLR,\n",
    "    'pl_trainer_kwargs': {\"callbacks\": [early_stopper]}\n",
    "}\n",
    "objective_fn = createObjective(model_invariates=model_invariates, HORIZON=HORIZON)\n",
    "study.optimize(objective_fn, n_trials=50)\n",
    "best_params ,best_n_features = getParams(study)\n",
    "\n",
    "model = TiDEModel(**best_params, **model_invariates)\n",
    "input_size = best_params[\"input_chunk_length\"]\n",
    "\n",
    "train_df_ranked = get_ranked_df(train_df, ranked_features, best_n_features)\n",
    "train_df_ranked[target_col] = train_df[target_col]\n",
    "\n",
    "train_exog= train_df_ranked.drop('fred_PCEPI',axis=1)\n",
    "train_target = train_df['fred_PCEPI']\n",
    "\n",
    "model,targetScaler,exoScaler= train_tide(model=model,target = TimeSeries.from_series(train_target),ranked_exo=TimeSeries.from_dataframe(train_exog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"horizon12/TideH12.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_ranked = get_ranked_df(test_df, ranked_features, best_n_features)\n",
    "test_df_ranked[target_col] = test_df[target_col]\n",
    "\n",
    "test_exog= test_df_ranked.drop('fred_PCEPI',axis=1)\n",
    "test_target = test_df['fred_PCEPI']\n",
    "\n",
    "preds_horizon_12 = predict(model=model,train_exog_df=train_exog,train_target_series=train_target,test_exog_df=test_exog,test_target_series=test_target,exog_scaler=exoScaler,target_scaler=targetScaler,input_size=input_size,horizon=HORIZON)\n",
    "np.save(os.path.join(project_root, 'Predictions/Horizon12', 'Tide_horizon_12.npy'),preds_horizon_12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
