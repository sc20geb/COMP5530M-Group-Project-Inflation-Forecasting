{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "# Set root and paths\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path.append(ROOT_PATH)\n",
    "\n",
    "from Training.Helper.dataPreprocessing import (\n",
    "    add_time_features, add_lagged_features, add_rolling_features,\n",
    "    sklearn_fit_transform, prepare_dataloader, rank_features_ccf,\n",
    "    TRAIN_DATA_PATH_1990S\n",
    ")\n",
    "from Models.LSTM import LSTM\n",
    "\n",
    "def create_direct_delta_sequences(X, y, seq_len=36, horizon=12):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len - horizon):\n",
    "        X_seq.append(X[i:i + seq_len])\n",
    "        base = y[i + seq_len - 1]\n",
    "        future = y[i + seq_len: i + seq_len + horizon]\n",
    "        delta = future - base\n",
    "        y_seq.append(delta)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# === CONFIG ===\n",
    "SEQ_LEN = 36\n",
    "HORIZON = 12\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "PATIENCE = 10\n",
    "LR = 1e-3\n",
    "TOP_K_FEATURES = 30\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:37:50,211 - INFO - Added time features: year, month, quarter. DataFrame shape: (408, 363)\n"
     ]
    }
   ],
   "source": [
    "# === LOAD & FEATURE ENGINEERING ===\n",
    "df = pd.read_csv(TRAIN_DATA_PATH_1990S)\n",
    "df[\"ds\"] = pd.to_datetime(df[\"observation_date\"], format=\"%m/%Y\")\n",
    "df = df.rename(columns={\"fred_PCEPI\": \"y_original\"})\n",
    "\n",
    "df = add_time_features(df, \"ds\")\n",
    "for k in [1, 2, 3, 4]:\n",
    "    df[f\"sin_{k}\"] = np.sin(2 * np.pi * k * df[\"month\"] / 12)\n",
    "    df[f\"cos_{k}\"] = np.cos(2 * np.pi * k * df[\"month\"] / 12)\n",
    "df[\"pct_change\"] = df[\"y_original\"].pct_change()\n",
    "df[\"momentum\"] = df[\"pct_change\"].diff()\n",
    "df = add_lagged_features(df, [\"y_original\"], lags=[1, 6, 12])\n",
    "df = add_rolling_features(df, \"y_original\", windows=[3, 6, 12])\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# === CCF SELECTION ===\n",
    "df_numeric = df.select_dtypes(include=[np.number]).copy()\n",
    "ccf_ranked = rank_features_ccf(df_numeric, targetCol=\"y_original\")\n",
    "selected_features = [col for col in list(ccf_ranked[:TOP_K_FEATURES]) if col in df.columns]\n",
    "features = df[selected_features]\n",
    "target_log = np.log1p(df[\"y_original\"])\n",
    "\n",
    "# === SCALE ===\n",
    "features_scaled_list, x_scaler = sklearn_fit_transform(features, StandardScaler())\n",
    "target_scaled_list, y_scaler = sklearn_fit_transform(target_log.to_frame(), StandardScaler())\n",
    "\n",
    "X_scaled = features_scaled_list[0].values\n",
    "y_scaled = target_scaled_list[0].values.flatten()\n",
    "\n",
    "# === SEQUENCES ===\n",
    "X_seq, y_seq = create_direct_delta_sequences(X_scaled, y_scaled, SEQ_LEN, HORIZON)\n",
    "X_seq = X_seq.reshape(X_seq.shape[0], SEQ_LEN, -1)\n",
    "y_seq = y_seq.reshape(y_seq.shape[0], HORIZON)\n",
    "\n",
    "# === SPLIT ===\n",
    "val_split = int(len(X_seq) * 0.8)\n",
    "X_train, X_val = X_seq[:val_split], X_seq[val_split:]\n",
    "y_train, y_val = y_seq[:val_split], y_seq[val_split:]\n",
    "\n",
    "train_loader = prepare_dataloader(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "val_loader = prepare_dataloader(X_val, y_val, shuffle=False, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training LSTM with Delta Learning...\n",
      "Epoch 01: Train MAE=0.381685 | Val MAE=0.175143\n",
      "Epoch 02: Train MAE=0.301909 | Val MAE=0.144671\n",
      "Epoch 03: Train MAE=0.250947 | Val MAE=0.150709\n",
      "Epoch 04: Train MAE=0.214598 | Val MAE=0.132989\n",
      "Epoch 05: Train MAE=0.175730 | Val MAE=0.084105\n",
      "Epoch 06: Train MAE=0.142522 | Val MAE=0.091752\n",
      "Epoch 07: Train MAE=0.108456 | Val MAE=0.087438\n",
      "Epoch 08: Train MAE=0.083856 | Val MAE=0.083011\n",
      "Epoch 09: Train MAE=0.063647 | Val MAE=0.059622\n",
      "Epoch 10: Train MAE=0.048567 | Val MAE=0.066273\n",
      "Epoch 11: Train MAE=0.043053 | Val MAE=0.065587\n",
      "Epoch 12: Train MAE=0.038577 | Val MAE=0.053362\n",
      "Epoch 13: Train MAE=0.036905 | Val MAE=0.052874\n",
      "Epoch 14: Train MAE=0.034734 | Val MAE=0.053327\n",
      "Epoch 15: Train MAE=0.032161 | Val MAE=0.052069\n",
      "Epoch 16: Train MAE=0.030443 | Val MAE=0.044928\n",
      "Epoch 17: Train MAE=0.029013 | Val MAE=0.047525\n",
      "Epoch 18: Train MAE=0.027545 | Val MAE=0.046967\n",
      "Epoch 19: Train MAE=0.027640 | Val MAE=0.049984\n",
      "Epoch 20: Train MAE=0.026611 | Val MAE=0.062664\n",
      "Epoch 21: Train MAE=0.030089 | Val MAE=0.047287\n",
      "Epoch 22: Train MAE=0.028268 | Val MAE=0.047704\n",
      "Epoch 23: Train MAE=0.026215 | Val MAE=0.051333\n",
      "Epoch 24: Train MAE=0.025177 | Val MAE=0.042279\n",
      "Epoch 25: Train MAE=0.025452 | Val MAE=0.046703\n",
      "Epoch 26: Train MAE=0.025059 | Val MAE=0.050254\n",
      "Epoch 27: Train MAE=0.022418 | Val MAE=0.047937\n",
      "Epoch 28: Train MAE=0.025354 | Val MAE=0.046151\n",
      "Epoch 29: Train MAE=0.026406 | Val MAE=0.042320\n",
      "Epoch 30: Train MAE=0.022468 | Val MAE=0.055738\n",
      "Epoch 31: Train MAE=0.022820 | Val MAE=0.051294\n",
      "Epoch 32: Train MAE=0.022410 | Val MAE=0.047669\n",
      "Epoch 33: Train MAE=0.021264 | Val MAE=0.054941\n",
      "Epoch 34: Train MAE=0.021808 | Val MAE=0.054402\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "# === MODEL ===\n",
    "model = LSTM(input_size=X_seq.shape[2], output_size=HORIZON).to(DEVICE)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# === TRAIN LOOP ===\n",
    "print(\"\\n Training LSTM with Delta Learning...\")\n",
    "best_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(DEVICE)\n",
    "            y_batch = y_batch.to(DEVICE)\n",
    "            pred = model(X_batch)\n",
    "            val_loss += criterion(pred, y_batch).item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1:02d}: Train MAE={avg_train_loss:.6f} | Val MAE={avg_val_loss:.6f}\")\n",
    "\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# === FINAL FORECAST (add deltas to base value) ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_input = torch.tensor(X_seq[-1][np.newaxis], dtype=torch.float32).to(DEVICE)\n",
    "    pred_delta = model(x_input).cpu().numpy().flatten()\n",
    "    base_val = y_scaled[-1]\n",
    "    pred_scaled = base_val + pred_delta\n",
    "\n",
    "y_pred_rescaled = y_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_pred_final = np.expm1(y_pred_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved improved LSTM with residuals to: ../../Predictions/LSTM.npy\n"
     ]
    }
   ],
   "source": [
    "# === SAVE ===\n",
    "save_path = os.path.join(\"..\", \"..\", \"Predictions\", \"LSTM.npy\")\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "np.save(save_path, y_pred_final)\n",
    "print(f\"\\nSaved improved LSTM with residuals to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
