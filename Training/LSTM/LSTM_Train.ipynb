{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Set root and paths\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path.append(ROOT_PATH)\n",
    "\n",
    "from Training.Helper.dataPreprocessing import (\n",
    "    add_time_features, add_lagged_features, add_rolling_features,\n",
    "    sklearn_fit_transform, prepare_dataloader, rank_features_ccf,\n",
    "    TRAIN_DATA_PATH_1990S\n",
    ")\n",
    "from Models.LSTM import LSTM\n",
    "\n",
    "def create_direct_delta_sequences(X, y, seq_len=36, horizon=12):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len - horizon):\n",
    "        X_seq.append(X[i:i + seq_len])\n",
    "        base = y[i + seq_len - 1]\n",
    "        future = y[i + seq_len: i + seq_len + horizon]\n",
    "        delta = future - base\n",
    "        y_seq.append(delta)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# === CONFIG ===\n",
    "SEQ_LEN = 36\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "PATIENCE = 10\n",
    "LR = 1e-3\n",
    "TOP_K_FEATURES = 30\n",
    "HORIZONS = [1, 3, 6, 12]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:54:53,999 - INFO - Added time features: year, month, quarter. DataFrame shape: (408, 363)\n",
      "2025-04-24 21:54:54,004 - INFO - Added lagged features with lags [1, 6, 12] to target columns ['y_original']. DataFrame shape: (408, 376)\n",
      "2025-04-24 21:54:54,006 - INFO - Added rolling mean and standard deviation features to target column y_original. DataFrame shape: (408, 382)\n"
     ]
    }
   ],
   "source": [
    "# === LOAD & FEATURE ENGINEERING ===\n",
    "df = pd.read_csv(TRAIN_DATA_PATH_1990S)\n",
    "df[\"ds\"] = pd.to_datetime(df[\"observation_date\"], format=\"%m/%Y\")\n",
    "df = df.rename(columns={\"fred_PCEPI\": \"y_original\"})\n",
    "\n",
    "df = add_time_features(df, \"ds\")\n",
    "for k in [1, 2, 3, 4]:\n",
    "    df[f\"sin_{k}\"] = np.sin(2 * np.pi * k * df[\"month\"] / 12)\n",
    "    df[f\"cos_{k}\"] = np.cos(2 * np.pi * k * df[\"month\"] / 12)\n",
    "df[\"pct_change\"] = df[\"y_original\"].pct_change()\n",
    "df[\"momentum\"] = df[\"pct_change\"].diff()\n",
    "df = add_lagged_features(df, [\"y_original\"], lags=[1, 6, 12])\n",
    "df = add_rolling_features(df, \"y_original\", windows=[3, 6, 12])\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# === CCF SELECTION ===\n",
    "df_numeric = df.select_dtypes(include=[np.number]).copy()\n",
    "ccf_ranked = rank_features_ccf(df_numeric, targetCol=\"y_original\")\n",
    "selected_features = [col for col in list(ccf_ranked[:TOP_K_FEATURES]) if col in df.columns]\n",
    "features = df[selected_features]\n",
    "target_log = np.log1p(df[\"y_original\"])\n",
    "\n",
    "# === SCALE ===\n",
    "features_scaled_list, x_scaler = sklearn_fit_transform(features, StandardScaler())\n",
    "target_scaled_list, y_scaler = sklearn_fit_transform(target_log.to_frame(), StandardScaler())\n",
    "\n",
    "X_scaled = features_scaled_list[0].values\n",
    "y_scaled = target_scaled_list[0].values.flatten()\n",
    "\n",
    "# === LOOP OVER HORIZONS ===\n",
    "for HORIZON in HORIZONS:\n",
    "\n",
    "    # === SEQUENCES ===\n",
    "    X_seq, y_seq = create_direct_delta_sequences(X_scaled, y_scaled, SEQ_LEN, HORIZON)\n",
    "    X_seq = X_seq.reshape(X_seq.shape[0], SEQ_LEN, -1)\n",
    "    y_seq = y_seq.reshape(y_seq.shape[0], HORIZON)\n",
    "\n",
    "    # === SPLIT ===\n",
    "    val_split = int(len(X_seq) * 0.8)\n",
    "    X_train, X_val = X_seq[:val_split], X_seq[val_split:]\n",
    "    y_train, y_val = y_seq[:val_split], y_seq[val_split:]\n",
    "\n",
    "    train_loader = prepare_dataloader(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "    val_loader = prepare_dataloader(X_val, y_val, shuffle=False, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:21:43,355] A new study created in memory with name: LSTM_horizon_1_hyperparameter_optimisation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning and Training LSTM for horizon 1 ===\n",
      "Running Optuna hyperparameter tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c646f24604345128768ba01d7f4753e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:21:44,898] Trial 0 finished with value: 0.00016229724294842326 and parameters: {'hidden_size': 43, 'num_layers': 3, 'dropout': 0.30870062689410327, 'lr': 0.0008356084131511738}. Best is trial 0 with value: 0.00016229724294842326.\n",
      "[I 2025-04-24 22:21:45,492] Trial 1 finished with value: 0.003494447407623132 and parameters: {'hidden_size': 55, 'num_layers': 1, 'dropout': 0.36039773032852307, 'lr': 0.0003057463399686791}. Best is trial 0 with value: 0.00016229724294842326.\n",
      "[I 2025-04-24 22:21:46,028] Trial 2 pruned. \n",
      "[I 2025-04-24 22:21:49,033] Trial 3 finished with value: 0.0007150119377507104 and parameters: {'hidden_size': 120, 'num_layers': 3, 'dropout': 0.2887207958491334, 'lr': 0.006068824980721918}. Best is trial 0 with value: 0.00016229724294842326.\n",
      "[I 2025-04-24 22:21:51,239] Trial 4 pruned. \n",
      "[I 2025-04-24 22:21:54,217] Trial 5 finished with value: 0.0002224437533489739 and parameters: {'hidden_size': 114, 'num_layers': 3, 'dropout': 0.3684899246352909, 'lr': 0.008204516949072019}. Best is trial 0 with value: 0.00016229724294842326.\n",
      "[I 2025-04-24 22:21:56,216] Trial 6 finished with value: 0.001231646430419965 and parameters: {'hidden_size': 87, 'num_layers': 3, 'dropout': 0.11369681961860878, 'lr': 0.0004280771541007791}. Best is trial 0 with value: 0.00016229724294842326.\n",
      "[I 2025-04-24 22:21:57,169] Trial 7 finished with value: 0.00022613134933635592 and parameters: {'hidden_size': 48, 'num_layers': 2, 'dropout': 0.4084972018017105, 'lr': 0.003967991935049644}. Best is trial 0 with value: 0.00016229724294842326.\n",
      "[I 2025-04-24 22:21:57,412] Trial 8 pruned. \n",
      "[I 2025-04-24 22:21:58,499] Trial 9 finished with value: 0.00024215292230817594 and parameters: {'hidden_size': 53, 'num_layers': 2, 'dropout': 0.042870893934331245, 'lr': 0.0017997271474298915}. Best is trial 0 with value: 0.00016229724294842326.\n",
      "[I 2025-04-24 22:21:59,199] Trial 10 pruned. \n",
      "[I 2025-04-24 22:22:00,464] Trial 11 pruned. \n",
      "[I 2025-04-24 22:22:03,358] Trial 12 finished with value: 0.00017636849548176138 and parameters: {'hidden_size': 107, 'num_layers': 3, 'dropout': 0.21356566117977366, 'lr': 0.009378560447901892}. Best is trial 0 with value: 0.00016229724294842326.\n",
      "[I 2025-04-24 22:22:05,627] Trial 13 pruned. \n",
      "[I 2025-04-24 22:22:06,145] Trial 14 pruned. \n",
      "[I 2025-04-24 22:22:06,660] Trial 15 pruned. \n",
      "[I 2025-04-24 22:22:08,226] Trial 16 pruned. \n",
      "[I 2025-04-24 22:22:09,668] Trial 17 pruned. \n",
      "[I 2025-04-24 22:22:12,680] Trial 18 finished with value: 0.00013835964798797958 and parameters: {'hidden_size': 98, 'num_layers': 3, 'dropout': 0.46043703816423837, 'lr': 0.0010875139412362192}. Best is trial 18 with value: 0.00013835964798797958.\n",
      "[I 2025-04-24 22:22:13,450] Trial 19 pruned. \n",
      "Best hyperparameters found:\n",
      "{'hidden_size': 98, 'num_layers': 3, 'dropout': 0.46043703816423837, 'lr': 0.0010875139412362192}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51dc3514f154457fa3a6981c416c000f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.053930, Val Loss: 0.001719\n",
      "Best model saved at ../../Models/Horizon1/LSTM_horizon_1_BEST_STOPPED_AT_1.pth (Epoch 1)\n",
      "Epoch 2/100 - Train Loss: 0.013493, Val Loss: 0.000902\n",
      "Best model saved at ../../Models/Horizon1/LSTM_horizon_1_BEST_STOPPED_AT_2.pth (Epoch 2)\n",
      "Epoch 3/100 - Train Loss: 0.006401, Val Loss: 0.000185\n",
      "Best model saved at ../../Models/Horizon1/LSTM_horizon_1_BEST_STOPPED_AT_3.pth (Epoch 3)\n",
      "Epoch 4/100 - Train Loss: 0.002788, Val Loss: 0.000795\n",
      "Epoch 5/100 - Train Loss: 0.001511, Val Loss: 0.000262\n",
      "Epoch 6/100 - Train Loss: 0.001185, Val Loss: 0.000311\n",
      "Epoch 7/100 - Train Loss: 0.000914, Val Loss: 0.000233\n",
      "Epoch 8/100 - Train Loss: 0.000722, Val Loss: 0.000210\n",
      "Epoch 9/100 - Train Loss: 0.000621, Val Loss: 0.000264\n",
      "Epoch 10/100 - Train Loss: 0.000426, Val Loss: 0.000210\n",
      "Epoch 11/100 - Train Loss: 0.000361, Val Loss: 0.000206\n",
      "Epoch 12/100 - Train Loss: 0.000333, Val Loss: 0.000208\n",
      "Epoch 13/100 - Train Loss: 0.000369, Val Loss: 0.000190\n",
      "Epoch 14/100 - Train Loss: 0.000254, Val Loss: 0.000208\n",
      "Epoch 15/100 - Train Loss: 0.000305, Val Loss: 0.000193\n",
      "Epoch 16/100 - Train Loss: 0.000192, Val Loss: 0.000193\n",
      "Epoch 17/100 - Train Loss: 0.000237, Val Loss: 0.000192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:22:25,780] A new study created in memory with name: LSTM_horizon_3_hyperparameter_optimisation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 - Train Loss: 0.000192, Val Loss: 0.000195\n",
      "Early stopping. counter: 15\n",
      "Best weights restored.\n",
      "Early stopping at epoch 18. Best model restored.\n",
      "Model training complete and saved!\n",
      "Saved tuned and trained LSTM with residuals to: ../../Predictions/Horizon1/LSTM_horizon_1.npy\n",
      "\n",
      "=== Tuning and Training LSTM for horizon 3 ===\n",
      "Running Optuna hyperparameter tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3128306c969d48e7b7b5dffd93a78e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:22:26,661] Trial 0 finished with value: 0.001821543413421346 and parameters: {'hidden_size': 84, 'num_layers': 1, 'dropout': 0.24343150986384277, 'lr': 0.001496215348432405}. Best is trial 0 with value: 0.001821543413421346.\n",
      "[I 2025-04-24 22:22:28,885] Trial 1 finished with value: 0.0014072859355817651 and parameters: {'hidden_size': 62, 'num_layers': 3, 'dropout': 0.3255401625485849, 'lr': 0.00023486406831604032}. Best is trial 1 with value: 0.0014072859355817651.\n",
      "[I 2025-04-24 22:22:29,682] Trial 2 finished with value: 0.005928562617757254 and parameters: {'hidden_size': 88, 'num_layers': 1, 'dropout': 0.31938486786063663, 'lr': 0.00012657377182822332}. Best is trial 1 with value: 0.0014072859355817651.\n",
      "[I 2025-04-24 22:22:33,486] Trial 3 finished with value: 0.0024353433141691815 and parameters: {'hidden_size': 101, 'num_layers': 3, 'dropout': 0.4343702899030695, 'lr': 0.00043099453141838946}. Best is trial 1 with value: 0.0014072859355817651.\n",
      "[I 2025-04-24 22:22:34,453] Trial 4 finished with value: 0.0011019608710840759 and parameters: {'hidden_size': 33, 'num_layers': 2, 'dropout': 0.025441107391293016, 'lr': 0.002688219549602043}. Best is trial 4 with value: 0.0011019608710840759.\n",
      "[I 2025-04-24 22:22:35,196] Trial 5 pruned. \n",
      "[I 2025-04-24 22:22:36,234] Trial 6 pruned. \n",
      "[I 2025-04-24 22:22:36,788] Trial 7 pruned. \n",
      "[I 2025-04-24 22:22:38,502] Trial 8 pruned. \n",
      "[I 2025-04-24 22:22:39,019] Trial 9 finished with value: 0.0008070864860201254 and parameters: {'hidden_size': 40, 'num_layers': 1, 'dropout': 0.31526043113815727, 'lr': 0.009172140054712107}. Best is trial 9 with value: 0.0008070864860201254.\n",
      "[I 2025-04-24 22:22:39,737] Trial 10 pruned. \n",
      "[I 2025-04-24 22:22:40,543] Trial 11 pruned. \n",
      "[I 2025-04-24 22:22:40,983] Trial 12 pruned. \n",
      "[I 2025-04-24 22:22:41,243] Trial 13 pruned. \n",
      "[I 2025-04-24 22:22:42,078] Trial 14 pruned. \n",
      "[I 2025-04-24 22:22:42,608] Trial 15 pruned. \n",
      "[I 2025-04-24 22:22:43,253] Trial 16 pruned. \n",
      "[I 2025-04-24 22:22:43,760] Trial 17 pruned. \n",
      "[I 2025-04-24 22:22:44,193] Trial 18 pruned. \n",
      "[I 2025-04-24 22:22:44,647] Trial 19 pruned. \n",
      "Best hyperparameters found:\n",
      "{'hidden_size': 40, 'num_layers': 1, 'dropout': 0.31526043113815727, 'lr': 0.009172140054712107}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac8e49f4af645d688b8d9f7f4450bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.032145, Val Loss: 0.003770\n",
      "Best model saved at ../../Models/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_1.pth (Epoch 1)\n",
      "Epoch 2/100 - Train Loss: 0.002170, Val Loss: 0.002140\n",
      "Best model saved at ../../Models/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_2.pth (Epoch 2)\n",
      "Epoch 3/100 - Train Loss: 0.000945, Val Loss: 0.001469\n",
      "Best model saved at ../../Models/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_3.pth (Epoch 3)\n",
      "Epoch 4/100 - Train Loss: 0.000559, Val Loss: 0.000980\n",
      "Best model saved at ../../Models/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_4.pth (Epoch 4)\n",
      "Epoch 5/100 - Train Loss: 0.000457, Val Loss: 0.000845\n",
      "Best model saved at ../../Models/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_5.pth (Epoch 5)\n",
      "Epoch 6/100 - Train Loss: 0.000390, Val Loss: 0.000746\n",
      "Best model saved at ../../Models/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_6.pth (Epoch 6)\n",
      "Epoch 7/100 - Train Loss: 0.000376, Val Loss: 0.000750\n",
      "Epoch 8/100 - Train Loss: 0.000370, Val Loss: 0.000729\n",
      "Best model saved at ../../Models/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_8.pth (Epoch 8)\n",
      "Epoch 9/100 - Train Loss: 0.000354, Val Loss: 0.000697\n",
      "Best model saved at ../../Models/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_9.pth (Epoch 9)\n",
      "Epoch 10/100 - Train Loss: 0.000344, Val Loss: 0.000694\n",
      "Best model saved at ../../Models/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_10.pth (Epoch 10)\n",
      "Epoch 11/100 - Train Loss: 0.000340, Val Loss: 0.000718\n",
      "Epoch 12/100 - Train Loss: 0.000354, Val Loss: 0.000652\n",
      "Best model saved at ../../Models/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_12.pth (Epoch 12)\n",
      "Epoch 13/100 - Train Loss: 0.000360, Val Loss: 0.000787\n",
      "Epoch 14/100 - Train Loss: 0.000366, Val Loss: 0.000625\n",
      "Best model saved at ../../Models/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_14.pth (Epoch 14)\n",
      "Epoch 15/100 - Train Loss: 0.000347, Val Loss: 0.000705\n",
      "Epoch 16/100 - Train Loss: 0.000352, Val Loss: 0.000807\n",
      "Epoch 17/100 - Train Loss: 0.000330, Val Loss: 0.000727\n",
      "Epoch 18/100 - Train Loss: 0.000341, Val Loss: 0.000772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:22:47,142] A new study created in memory with name: LSTM_horizon_6_hyperparameter_optimisation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 - Train Loss: 0.000336, Val Loss: 0.000780\n",
      "Early stopping. counter: 15\n",
      "Best weights restored.\n",
      "Early stopping at epoch 19. Best model restored.\n",
      "Model training complete and saved!\n",
      "Saved tuned and trained LSTM with residuals to: ../../Predictions/Horizon3/LSTM_horizon_3.npy\n",
      "\n",
      "=== Tuning and Training LSTM for horizon 6 ===\n",
      "Running Optuna hyperparameter tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a649a49df1824276b4d7cb95549ffd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:22:48,574] Trial 0 finished with value: 0.0037525150085418995 and parameters: {'hidden_size': 101, 'num_layers': 1, 'dropout': 0.46205596114130365, 'lr': 0.0015564324099740894}. Best is trial 0 with value: 0.0037525150085418995.\n",
      "[I 2025-04-24 22:22:49,901] Trial 1 finished with value: 0.006633510511420982 and parameters: {'hidden_size': 52, 'num_layers': 2, 'dropout': 0.38057407625569967, 'lr': 0.0001632028860581231}. Best is trial 0 with value: 0.0037525150085418995.\n",
      "[I 2025-04-24 22:22:51,439] Trial 2 finished with value: 0.003236895036870535 and parameters: {'hidden_size': 88, 'num_layers': 2, 'dropout': 0.45354412153020107, 'lr': 0.002261600285796373}. Best is trial 2 with value: 0.003236895036870535.\n",
      "[I 2025-04-24 22:22:52,726] Trial 3 finished with value: 0.003713541173927066 and parameters: {'hidden_size': 97, 'num_layers': 1, 'dropout': 0.11671336161291895, 'lr': 0.0008343211736006362}. Best is trial 2 with value: 0.003236895036870535.\n",
      "[I 2025-04-24 22:22:53,302] Trial 4 pruned. \n",
      "[I 2025-04-24 22:22:56,943] Trial 5 finished with value: 0.00309081723533151 and parameters: {'hidden_size': 116, 'num_layers': 3, 'dropout': 0.04553028830640693, 'lr': 0.008454606396458824}. Best is trial 5 with value: 0.00309081723533151.\n",
      "[I 2025-04-24 22:22:57,186] Trial 6 pruned. \n",
      "[I 2025-04-24 22:22:58,277] Trial 7 finished with value: 0.003808712248992122 and parameters: {'hidden_size': 95, 'num_layers': 1, 'dropout': 0.24970799862375243, 'lr': 0.000992808913683087}. Best is trial 5 with value: 0.00309081723533151.\n",
      "[I 2025-04-24 22:22:59,342] Trial 8 finished with value: 0.002610625846731201 and parameters: {'hidden_size': 120, 'num_layers': 1, 'dropout': 0.20807358596361542, 'lr': 0.0010863280464874672}. Best is trial 8 with value: 0.002610625846731201.\n",
      "[I 2025-04-24 22:23:00,127] Trial 9 pruned. \n",
      "[I 2025-04-24 22:23:01,511] Trial 10 pruned. \n",
      "[I 2025-04-24 22:23:04,541] Trial 11 finished with value: 0.002188084393733142 and parameters: {'hidden_size': 121, 'num_layers': 3, 'dropout': 0.0016370352110728903, 'lr': 0.0032985578521002386}. Best is trial 11 with value: 0.002188084393733142.\n",
      "[I 2025-04-24 22:23:07,330] Trial 12 finished with value: 0.0021290611377326957 and parameters: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.03199250417946585, 'lr': 0.003099167344553753}. Best is trial 12 with value: 0.0021290611377326957.\n",
      "[I 2025-04-24 22:23:10,273] Trial 13 finished with value: 0.002245130173986594 and parameters: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.04077810502793788, 'lr': 0.00328138980803644}. Best is trial 12 with value: 0.0021290611377326957.\n",
      "[I 2025-04-24 22:23:12,600] Trial 14 pruned. \n",
      "[I 2025-04-24 22:23:13,737] Trial 15 pruned. \n",
      "[I 2025-04-24 22:23:15,421] Trial 16 finished with value: 0.002268140069799314 and parameters: {'hidden_size': 67, 'num_layers': 3, 'dropout': 0.06611874987671829, 'lr': 0.0005493514647661967}. Best is trial 12 with value: 0.0021290611377326957.\n",
      "[I 2025-04-24 22:23:16,255] Trial 17 pruned. \n",
      "[I 2025-04-24 22:23:17,772] Trial 18 pruned. \n",
      "[I 2025-04-24 22:23:19,508] Trial 19 pruned. \n",
      "Best hyperparameters found:\n",
      "{'hidden_size': 128, 'num_layers': 3, 'dropout': 0.03199250417946585, 'lr': 0.003099167344553753}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69731e2a7fe84422a60a7edbedbb5d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.030837, Val Loss: 0.002088\n",
      "Best model saved at ../../Models/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_1.pth (Epoch 1)\n",
      "Epoch 2/100 - Train Loss: 0.000991, Val Loss: 0.002781\n",
      "Epoch 3/100 - Train Loss: 0.000912, Val Loss: 0.002426\n",
      "Epoch 4/100 - Train Loss: 0.000807, Val Loss: 0.002656\n",
      "Epoch 5/100 - Train Loss: 0.000805, Val Loss: 0.001964\n",
      "Best model saved at ../../Models/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_5.pth (Epoch 5)\n",
      "Epoch 6/100 - Train Loss: 0.000652, Val Loss: 0.002095\n",
      "Epoch 7/100 - Train Loss: 0.000775, Val Loss: 0.002666\n",
      "Epoch 8/100 - Train Loss: 0.000611, Val Loss: 0.003045\n",
      "Epoch 9/100 - Train Loss: 0.000590, Val Loss: 0.003902\n",
      "Epoch 10/100 - Train Loss: 0.000624, Val Loss: 0.002727\n",
      "Epoch 11/100 - Train Loss: 0.000562, Val Loss: 0.002238\n",
      "Epoch 12/100 - Train Loss: 0.000535, Val Loss: 0.002550\n",
      "Epoch 13/100 - Train Loss: 0.000501, Val Loss: 0.005071\n",
      "Epoch 14/100 - Train Loss: 0.000549, Val Loss: 0.002772\n",
      "Epoch 15/100 - Train Loss: 0.000433, Val Loss: 0.001980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:23:31,119] A new study created in memory with name: LSTM_horizon_12_hyperparameter_optimisation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 - Train Loss: 0.000605, Val Loss: 0.003365\n",
      "Early stopping. counter: 15\n",
      "Best weights restored.\n",
      "Early stopping at epoch 16. Best model restored.\n",
      "Model training complete and saved!\n",
      "Saved tuned and trained LSTM with residuals to: ../../Predictions/Horizon6/LSTM_horizon_6.npy\n",
      "\n",
      "=== Tuning and Training LSTM for horizon 12 ===\n",
      "Running Optuna hyperparameter tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba44be79a5504d25a0039d89d5407a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:23:32,569] Trial 0 finished with value: 0.01535551351095949 and parameters: {'hidden_size': 67, 'num_layers': 2, 'dropout': 0.19715174359731152, 'lr': 0.00016234042012073786}. Best is trial 0 with value: 0.01535551351095949.\n",
      "[I 2025-04-24 22:23:33,312] Trial 1 finished with value: 0.006965970191439348 and parameters: {'hidden_size': 36, 'num_layers': 1, 'dropout': 0.31325831379064634, 'lr': 0.007965843231532373}. Best is trial 1 with value: 0.006965970191439348.\n",
      "[I 2025-04-24 22:23:35,742] Trial 2 finished with value: 0.006258247892505356 and parameters: {'hidden_size': 93, 'num_layers': 2, 'dropout': 0.10959170748786556, 'lr': 0.0036352055447556693}. Best is trial 2 with value: 0.006258247892505356.\n",
      "[I 2025-04-24 22:23:36,677] Trial 3 finished with value: 0.011891262871878487 and parameters: {'hidden_size': 99, 'num_layers': 1, 'dropout': 0.3025992548619762, 'lr': 0.00042953958846669556}. Best is trial 2 with value: 0.006258247892505356.\n",
      "[I 2025-04-24 22:23:39,212] Trial 4 finished with value: 0.008105118170247547 and parameters: {'hidden_size': 120, 'num_layers': 2, 'dropout': 0.18407214166888308, 'lr': 0.0011849162211630786}. Best is trial 2 with value: 0.006258247892505356.\n",
      "[I 2025-04-24 22:23:40,813] Trial 5 pruned. \n",
      "[I 2025-04-24 22:23:43,231] Trial 6 pruned. \n",
      "[I 2025-04-24 22:23:43,645] Trial 7 pruned. \n",
      "[I 2025-04-24 22:23:45,267] Trial 8 finished with value: 0.006748411674717707 and parameters: {'hidden_size': 61, 'num_layers': 3, 'dropout': 0.3143477618945639, 'lr': 0.008195427245692158}. Best is trial 2 with value: 0.006258247892505356.\n",
      "[I 2025-04-24 22:23:45,999] Trial 9 pruned. \n",
      "[I 2025-04-24 22:23:47,949] Trial 10 finished with value: 0.008262627977611764 and parameters: {'hidden_size': 89, 'num_layers': 3, 'dropout': 0.015599552806772998, 'lr': 0.0029298651171553343}. Best is trial 2 with value: 0.006258247892505356.\n",
      "[I 2025-04-24 22:23:49,400] Trial 11 pruned. \n",
      "[I 2025-04-24 22:23:52,000] Trial 12 finished with value: 0.007200278035764183 and parameters: {'hidden_size': 98, 'num_layers': 3, 'dropout': 0.08139069929701828, 'lr': 0.003908356719557292}. Best is trial 2 with value: 0.006258247892505356.\n",
      "[I 2025-04-24 22:23:52,562] Trial 13 finished with value: 0.00816820277167218 and parameters: {'hidden_size': 57, 'num_layers': 1, 'dropout': 0.1123428871658192, 'lr': 0.00396350467414145}. Best is trial 2 with value: 0.006258247892505356.\n",
      "[I 2025-04-24 22:23:54,007] Trial 14 pruned. \n",
      "[I 2025-04-24 22:23:55,453] Trial 15 pruned. \n",
      "[I 2025-04-24 22:23:56,039] Trial 16 pruned. \n",
      "[I 2025-04-24 22:23:56,294] Trial 17 pruned. \n",
      "[I 2025-04-24 22:23:56,988] Trial 18 pruned. \n",
      "[I 2025-04-24 22:23:57,924] Trial 19 pruned. \n",
      "Best hyperparameters found:\n",
      "{'hidden_size': 93, 'num_layers': 2, 'dropout': 0.10959170748786556, 'lr': 0.0036352055447556693}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1251b017704c40309085ab14207e3597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.013127, Val Loss: 0.010110\n",
      "Best model saved at ../../Models/Horizon12/LSTM_horizon_12_BEST_STOPPED_AT_1.pth (Epoch 1)\n",
      "Epoch 2/100 - Train Loss: 0.002098, Val Loss: 0.009207\n",
      "Best model saved at ../../Models/Horizon12/LSTM_horizon_12_BEST_STOPPED_AT_2.pth (Epoch 2)\n",
      "Epoch 3/100 - Train Loss: 0.001550, Val Loss: 0.008851\n",
      "Best model saved at ../../Models/Horizon12/LSTM_horizon_12_BEST_STOPPED_AT_3.pth (Epoch 3)\n",
      "Epoch 4/100 - Train Loss: 0.001350, Val Loss: 0.008653\n",
      "Best model saved at ../../Models/Horizon12/LSTM_horizon_12_BEST_STOPPED_AT_4.pth (Epoch 4)\n",
      "Epoch 5/100 - Train Loss: 0.001208, Val Loss: 0.007401\n",
      "Best model saved at ../../Models/Horizon12/LSTM_horizon_12_BEST_STOPPED_AT_5.pth (Epoch 5)\n",
      "Epoch 6/100 - Train Loss: 0.001180, Val Loss: 0.008737\n",
      "Epoch 7/100 - Train Loss: 0.001141, Val Loss: 0.009991\n",
      "Epoch 8/100 - Train Loss: 0.001035, Val Loss: 0.007409\n",
      "Epoch 9/100 - Train Loss: 0.001086, Val Loss: 0.007518\n",
      "Epoch 10/100 - Train Loss: 0.000846, Val Loss: 0.006289\n",
      "Best model saved at ../../Models/Horizon12/LSTM_horizon_12_BEST_STOPPED_AT_10.pth (Epoch 10)\n",
      "Epoch 11/100 - Train Loss: 0.000820, Val Loss: 0.007343\n",
      "Epoch 12/100 - Train Loss: 0.000798, Val Loss: 0.007764\n",
      "Epoch 13/100 - Train Loss: 0.000804, Val Loss: 0.007768\n",
      "Epoch 14/100 - Train Loss: 0.000660, Val Loss: 0.008786\n",
      "Epoch 15/100 - Train Loss: 0.000715, Val Loss: 0.008052\n",
      "Epoch 16/100 - Train Loss: 0.000542, Val Loss: 0.008233\n",
      "Epoch 17/100 - Train Loss: 0.000633, Val Loss: 0.007176\n",
      "Epoch 18/100 - Train Loss: 0.000632, Val Loss: 0.008039\n",
      "Epoch 19/100 - Train Loss: 0.000600, Val Loss: 0.007289\n",
      "Epoch 20/100 - Train Loss: 0.000594, Val Loss: 0.007157\n",
      "Epoch 21/100 - Train Loss: 0.000620, Val Loss: 0.007759\n",
      "Epoch 22/100 - Train Loss: 0.000639, Val Loss: 0.007804\n",
      "Epoch 23/100 - Train Loss: 0.000576, Val Loss: 0.007476\n",
      "Epoch 24/100 - Train Loss: 0.000578, Val Loss: 0.007847\n",
      "Epoch 25/100 - Train Loss: 0.000520, Val Loss: 0.007130\n",
      "Early stopping. counter: 15\n",
      "Best weights restored.\n",
      "Early stopping at epoch 25. Best model restored.\n",
      "Model training complete and saved!\n",
      "Saved tuned and trained LSTM with residuals to: ../../Predictions/Horizon12/LSTM_horizon_12.npy\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from Training.Helper.PyTorchModular import optuna_tune_and_train_pytorch\n",
    "\n",
    "HORIZONS = [1, 3, 6, 12]\n",
    "\n",
    "for HORIZON in HORIZONS:\n",
    "    print(f\"\\n=== Tuning and Training LSTM for horizon {HORIZON} ===\")\n",
    "\n",
    "    model_search_space = {\n",
    "        \"hidden_size\": (int, 32, 128),\n",
    "        \"num_layers\": (int, 1, 3),\n",
    "        \"dropout\": (float, 0.0, 0.5),\n",
    "    }\n",
    "\n",
    "    optim_search_space = {\n",
    "        \"lr\": (float, 1e-4, 1e-2, {\"log\": True})\n",
    "    }\n",
    "\n",
    "    model_invariates = {\n",
    "        \"input_size\": X_seq.shape[2],\n",
    "        \"output_size\": HORIZON\n",
    "    }\n",
    "\n",
    "    # Recreate sequences and loaders for each HORIZON to match target shape\n",
    "    X_seq, y_seq = create_direct_delta_sequences(X_scaled, y_scaled, SEQ_LEN, HORIZON)\n",
    "    X_seq = X_seq.reshape(X_seq.shape[0], SEQ_LEN, -1)\n",
    "    y_seq = y_seq.reshape(y_seq.shape[0], HORIZON)\n",
    "\n",
    "    val_split = int(len(X_seq) * 0.8)\n",
    "    X_train, X_val = X_seq[:val_split], X_seq[val_split:]\n",
    "    y_train, y_val = y_seq[:val_split], y_seq[val_split:]\n",
    "\n",
    "    train_loader = prepare_dataloader(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "    val_loader = prepare_dataloader(X_val, y_val, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model, metadata = optuna_tune_and_train_pytorch(\n",
    "        model_class=LSTM,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=DEVICE,\n",
    "        model_search_space=model_search_space,\n",
    "        model_invariates=model_invariates,\n",
    "        optim_search_space=optim_search_space,\n",
    "        max_epochs=EPOCHS,\n",
    "        model_save_path=os.path.join(\"..\", \"..\", \"Models\", f\"Horizon{HORIZON}\"),\n",
    "        model_name=f\"LSTM_horizon_{HORIZON}\",\n",
    "        n_trials=20,\n",
    "        n_epochs_per_trial=5,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # === FINAL FORECAST ===\n",
    "    model.eval()\n",
    "    y_pred_final = []\n",
    "    with torch.no_grad():\n",
    "        x_input = X_seq[-1].copy()\n",
    "        base_val = y_scaled[-1]\n",
    "\n",
    "        for _ in range(12):\n",
    "            x_tensor = torch.tensor(x_input[np.newaxis], dtype=torch.float32).to(DEVICE)\n",
    "            pred_delta = model(x_tensor).cpu().numpy().flatten()\n",
    "\n",
    "            step_index = min(HORIZON, len(pred_delta)) - 1\n",
    "            next_scaled = base_val + pred_delta[step_index]\n",
    "            y_pred_final.append(next_scaled)\n",
    "\n",
    "            x_input = np.roll(x_input, -1, axis=0)\n",
    "            x_input[-1] = np.concatenate([x_input[-2][:-1], [next_scaled]])\n",
    "            base_val = next_scaled\n",
    "\n",
    "    y_pred_rescaled = y_scaler.inverse_transform(np.array(y_pred_final).reshape(-1, 1)).flatten()\n",
    "    y_pred_final = np.expm1(y_pred_rescaled)\n",
    "\n",
    "    # === SAVE ===\n",
    "    save_path = os.path.join(\"..\", \"..\", \"Predictions\", f\"Horizon{HORIZON}\", f\"LSTM_horizon_{HORIZON}.npy\")\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    np.save(save_path, y_pred_final)\n",
    "    print(f\"Saved tuned and trained LSTM with residuals to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
