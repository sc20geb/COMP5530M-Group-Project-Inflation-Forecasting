{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Set root and paths\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path.append(ROOT_PATH)\n",
    "\n",
    "from Training.Helper.dataPreprocessing import (\n",
    "    add_time_features, add_lagged_features, add_rolling_features,\n",
    "    sklearn_fit_transform, prepare_dataloader, rank_features_ccf,\n",
    "    TRAIN_DATA_PATH_1990S\n",
    ")\n",
    "from Models.LSTM import LSTM\n",
    "\n",
    "def create_direct_delta_sequences(X, y, seq_len=36, horizon=12):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len - horizon):\n",
    "        X_seq.append(X[i:i + seq_len])\n",
    "        base = y[i + seq_len - 1]\n",
    "        future = y[i + seq_len: i + seq_len + horizon]\n",
    "        delta = future - base\n",
    "        y_seq.append(delta)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# === CONFIG ===\n",
    "SEQ_LEN = 36\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "PATIENCE = 10\n",
    "LR = 1e-3\n",
    "TOP_K_FEATURES = 30\n",
    "HORIZONS = [1, 3, 6, 12]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:54:53,999 - INFO - Added time features: year, month, quarter. DataFrame shape: (408, 363)\n",
      "2025-04-24 21:54:54,004 - INFO - Added lagged features with lags [1, 6, 12] to target columns ['y_original']. DataFrame shape: (408, 376)\n",
      "2025-04-24 21:54:54,006 - INFO - Added rolling mean and standard deviation features to target column y_original. DataFrame shape: (408, 382)\n"
     ]
    }
   ],
   "source": [
    "# === LOAD & FEATURE ENGINEERING ===\n",
    "df = pd.read_csv(TRAIN_DATA_PATH_1990S)\n",
    "df[\"ds\"] = pd.to_datetime(df[\"observation_date\"], format=\"%m/%Y\")\n",
    "df = df.rename(columns={\"fred_PCEPI\": \"y_original\"})\n",
    "\n",
    "df = add_time_features(df, \"ds\")\n",
    "for k in [1, 2, 3, 4]:\n",
    "    df[f\"sin_{k}\"] = np.sin(2 * np.pi * k * df[\"month\"] / 12)\n",
    "    df[f\"cos_{k}\"] = np.cos(2 * np.pi * k * df[\"month\"] / 12)\n",
    "df[\"pct_change\"] = df[\"y_original\"].pct_change()\n",
    "df[\"momentum\"] = df[\"pct_change\"].diff()\n",
    "df = add_lagged_features(df, [\"y_original\"], lags=[1, 6, 12])\n",
    "df = add_rolling_features(df, \"y_original\", windows=[3, 6, 12])\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# === CCF SELECTION ===\n",
    "df_numeric = df.select_dtypes(include=[np.number]).copy()\n",
    "ccf_ranked = rank_features_ccf(df_numeric, targetCol=\"y_original\")\n",
    "selected_features = [col for col in list(ccf_ranked[:TOP_K_FEATURES]) if col in df.columns]\n",
    "features = df[selected_features]\n",
    "target_log = np.log1p(df[\"y_original\"])\n",
    "\n",
    "# === SCALE ===\n",
    "features_scaled_list, x_scaler = sklearn_fit_transform(features, StandardScaler())\n",
    "target_scaled_list, y_scaler = sklearn_fit_transform(target_log.to_frame(), StandardScaler())\n",
    "\n",
    "X_scaled = features_scaled_list[0].values\n",
    "y_scaled = target_scaled_list[0].values.flatten()\n",
    "\n",
    "# === LOOP OVER HORIZONS ===\n",
    "for HORIZON in HORIZONS:\n",
    "\n",
    "    # === SEQUENCES ===\n",
    "    X_seq, y_seq = create_direct_delta_sequences(X_scaled, y_scaled, SEQ_LEN, HORIZON)\n",
    "    X_seq = X_seq.reshape(X_seq.shape[0], SEQ_LEN, -1)\n",
    "    y_seq = y_seq.reshape(y_seq.shape[0], HORIZON)\n",
    "\n",
    "    # === SPLIT ===\n",
    "    val_split = int(len(X_seq) * 0.8)\n",
    "    X_train, X_val = X_seq[:val_split], X_seq[val_split:]\n",
    "    y_train, y_val = y_seq[:val_split], y_seq[val_split:]\n",
    "\n",
    "    train_loader = prepare_dataloader(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "    val_loader = prepare_dataloader(X_val, y_val, shuffle=False, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:37:55,881] A new study created in memory with name: LSTM_horizon_1_hyperparameter_optimisation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning and Training LSTM for horizon 1 ===\n",
      "Running Optuna hyperparameter tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d70e49d110f4e91974e98cb62dad12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:37:58,873] Trial 0 finished with value: 0.0014949571664652063 and parameters: {'hidden_size': 94, 'num_layers': 3, 'dropout': 0.022034817585603106, 'lr': 0.00022427552398717043}. Best is trial 0 with value: 0.0014949571664652063.\n",
      "[I 2025-04-24 22:37:59,902] Trial 1 finished with value: 0.000813305704569858 and parameters: {'hidden_size': 115, 'num_layers': 1, 'dropout': 0.49046961576313775, 'lr': 0.0010656205110117247}. Best is trial 1 with value: 0.000813305704569858.\n",
      "[I 2025-04-24 22:38:02,211] Trial 2 finished with value: 0.0002952970260392047 and parameters: {'hidden_size': 127, 'num_layers': 2, 'dropout': 0.20719841914505382, 'lr': 0.006737422240871544}. Best is trial 2 with value: 0.0002952970260392047.\n",
      "[I 2025-04-24 22:38:03,693] Trial 3 finished with value: 0.003413836368256145 and parameters: {'hidden_size': 36, 'num_layers': 3, 'dropout': 0.3763559868865426, 'lr': 0.00034176785621033666}. Best is trial 2 with value: 0.0002952970260392047.\n",
      "[I 2025-04-24 22:38:05,765] Trial 4 finished with value: 0.0010734709147881302 and parameters: {'hidden_size': 89, 'num_layers': 3, 'dropout': 0.22182395051821996, 'lr': 0.00021767655218769968}. Best is trial 2 with value: 0.0002952970260392047.\n",
      "[I 2025-04-24 22:38:07,079] Trial 5 pruned. \n",
      "[I 2025-04-24 22:38:08,522] Trial 6 pruned. \n",
      "[I 2025-04-24 22:38:09,207] Trial 7 pruned. \n",
      "[I 2025-04-24 22:38:09,435] Trial 8 pruned. \n",
      "[I 2025-04-24 22:38:09,893] Trial 9 pruned. \n",
      "[I 2025-04-24 22:38:11,119] Trial 10 finished with value: 0.00026070643960136094 and parameters: {'hidden_size': 63, 'num_layers': 2, 'dropout': 0.07599040977991045, 'lr': 0.009485525779548625}. Best is trial 10 with value: 0.00026070643960136094.\n",
      "[I 2025-04-24 22:38:12,452] Trial 11 finished with value: 0.00015424963122414838 and parameters: {'hidden_size': 62, 'num_layers': 2, 'dropout': 0.06916234405063654, 'lr': 0.007866575025481097}. Best is trial 11 with value: 0.00015424963122414838.\n",
      "[I 2025-04-24 22:38:13,443] Trial 12 pruned. \n",
      "[I 2025-04-24 22:38:14,430] Trial 13 pruned. \n",
      "[I 2025-04-24 22:38:15,359] Trial 14 pruned. \n",
      "[I 2025-04-24 22:38:16,442] Trial 15 finished with value: 0.00016641034421304034 and parameters: {'hidden_size': 54, 'num_layers': 2, 'dropout': 0.346652621074602, 'lr': 0.0098409267944708}. Best is trial 11 with value: 0.00015424963122414838.\n",
      "[I 2025-04-24 22:38:16,993] Trial 16 finished with value: 0.0002733331493800506 and parameters: {'hidden_size': 46, 'num_layers': 1, 'dropout': 0.3268841751830112, 'lr': 0.0024498926964793416}. Best is trial 11 with value: 0.00015424963122414838.\n",
      "[I 2025-04-24 22:38:17,846] Trial 17 pruned. \n",
      "[I 2025-04-24 22:38:18,352] Trial 18 pruned. \n",
      "[I 2025-04-24 22:38:18,623] Trial 19 pruned. \n",
      "Best hyperparameters found:\n",
      "{'hidden_size': 62, 'num_layers': 2, 'dropout': 0.06916234405063654, 'lr': 0.007866575025481097}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e02d4f09a434102a16f369d59634c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.066616, Val Loss: 0.000148\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon1/LSTM_horizon_1_BEST_STOPPED_AT_1.pth (Epoch 1)\n",
      "Epoch 2/100 - Train Loss: 0.002089, Val Loss: 0.001812\n",
      "Epoch 3/100 - Train Loss: 0.000529, Val Loss: 0.000283\n",
      "Epoch 4/100 - Train Loss: 0.000264, Val Loss: 0.000252\n",
      "Epoch 5/100 - Train Loss: 0.000240, Val Loss: 0.000477\n",
      "Epoch 6/100 - Train Loss: 0.000209, Val Loss: 0.000293\n",
      "Epoch 7/100 - Train Loss: 0.000167, Val Loss: 0.000376\n",
      "Epoch 8/100 - Train Loss: 0.000161, Val Loss: 0.000281\n",
      "Epoch 9/100 - Train Loss: 0.000160, Val Loss: 0.000198\n",
      "Epoch 10/100 - Train Loss: 0.000158, Val Loss: 0.000209\n",
      "Epoch 11/100 - Train Loss: 0.000142, Val Loss: 0.000303\n",
      "Epoch 12/100 - Train Loss: 0.000146, Val Loss: 0.000263\n",
      "Epoch 13/100 - Train Loss: 0.000151, Val Loss: 0.000261\n",
      "Epoch 14/100 - Train Loss: 0.000152, Val Loss: 0.000311\n",
      "Epoch 15/100 - Train Loss: 0.000133, Val Loss: 0.000316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:38:22,865] A new study created in memory with name: LSTM_horizon_3_hyperparameter_optimisation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 - Train Loss: 0.000123, Val Loss: 0.000251\n",
      "Early stopping. counter: 15\n",
      "Best weights restored.\n",
      "Early stopping at epoch 16. Best model restored.\n",
      "Model training complete and saved!\n",
      "Saved tuned and trained LSTM with residuals to: ../../Predictions/Horizon1/LSTM_horizon_1.npy\n",
      "\n",
      "=== Tuning and Training LSTM for horizon 3 ===\n",
      "Running Optuna hyperparameter tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225e6217076e4ea59c542a6f9c7e3408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:38:26,706] Trial 0 finished with value: 0.0016614171391766933 and parameters: {'hidden_size': 101, 'num_layers': 3, 'dropout': 0.33565578290229253, 'lr': 0.0003168278162653966}. Best is trial 0 with value: 0.0016614171391766933.\n",
      "[I 2025-04-24 22:38:28,072] Trial 1 finished with value: 0.0011313498253002763 and parameters: {'hidden_size': 88, 'num_layers': 2, 'dropout': 0.3649210218822819, 'lr': 0.0036595440479575817}. Best is trial 1 with value: 0.0011313498253002763.\n",
      "[I 2025-04-24 22:38:28,936] Trial 2 finished with value: 0.005334043487285574 and parameters: {'hidden_size': 81, 'num_layers': 1, 'dropout': 0.13153548388896552, 'lr': 0.0003929392701245735}. Best is trial 1 with value: 0.0011313498253002763.\n",
      "[I 2025-04-24 22:38:30,576] Trial 3 finished with value: 0.0007728916284072006 and parameters: {'hidden_size': 41, 'num_layers': 3, 'dropout': 0.1812967978308065, 'lr': 0.00011798837509171583}. Best is trial 3 with value: 0.0007728916284072006.\n",
      "[I 2025-04-24 22:38:31,415] Trial 4 pruned. \n",
      "[I 2025-04-24 22:38:32,195] Trial 5 pruned. \n",
      "[I 2025-04-24 22:38:32,923] Trial 6 pruned. \n",
      "[I 2025-04-24 22:38:34,026] Trial 7 pruned. \n",
      "[I 2025-04-24 22:38:34,971] Trial 8 finished with value: 0.0005609368430062508 and parameters: {'hidden_size': 121, 'num_layers': 1, 'dropout': 0.32087126924304843, 'lr': 0.004850453419876738}. Best is trial 8 with value: 0.0005609368430062508.\n",
      "[I 2025-04-24 22:38:35,603] Trial 9 pruned. \n",
      "[I 2025-04-24 22:38:36,486] Trial 10 pruned. \n",
      "[I 2025-04-24 22:38:36,847] Trial 11 pruned. \n",
      "[I 2025-04-24 22:38:37,498] Trial 12 pruned. \n",
      "[I 2025-04-24 22:38:37,938] Trial 13 pruned. \n",
      "[I 2025-04-24 22:38:38,999] Trial 14 finished with value: 0.0009926965317895843 and parameters: {'hidden_size': 59, 'num_layers': 2, 'dropout': 0.12065491953945973, 'lr': 0.002532183260768845}. Best is trial 8 with value: 0.0005609368430062508.\n",
      "[I 2025-04-24 22:38:39,753] Trial 15 pruned. \n",
      "[I 2025-04-24 22:38:40,501] Trial 16 pruned. \n",
      "[I 2025-04-24 22:38:40,840] Trial 17 pruned. \n",
      "[I 2025-04-24 22:38:41,588] Trial 18 pruned. \n",
      "[I 2025-04-24 22:38:42,685] Trial 19 pruned. \n",
      "Best hyperparameters found:\n",
      "{'hidden_size': 121, 'num_layers': 1, 'dropout': 0.32087126924304843, 'lr': 0.004850453419876738}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e816b284062d43c18109b9f27c2a23ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.075450, Val Loss: 0.008479\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_1.pth (Epoch 1)\n",
      "Epoch 2/100 - Train Loss: 0.003478, Val Loss: 0.002372\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_2.pth (Epoch 2)\n",
      "Epoch 3/100 - Train Loss: 0.000740, Val Loss: 0.001222\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_3.pth (Epoch 3)\n",
      "Epoch 4/100 - Train Loss: 0.000473, Val Loss: 0.000974\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_4.pth (Epoch 4)\n",
      "Epoch 5/100 - Train Loss: 0.000391, Val Loss: 0.000897\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_5.pth (Epoch 5)\n",
      "Epoch 6/100 - Train Loss: 0.000364, Val Loss: 0.000860\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_6.pth (Epoch 6)\n",
      "Epoch 7/100 - Train Loss: 0.000356, Val Loss: 0.000789\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_7.pth (Epoch 7)\n",
      "Epoch 8/100 - Train Loss: 0.000334, Val Loss: 0.000803\n",
      "Epoch 9/100 - Train Loss: 0.000344, Val Loss: 0.000783\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_9.pth (Epoch 9)\n",
      "Epoch 10/100 - Train Loss: 0.000340, Val Loss: 0.000765\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon3/LSTM_horizon_3_BEST_STOPPED_AT_10.pth (Epoch 10)\n",
      "Epoch 11/100 - Train Loss: 0.000342, Val Loss: 0.000814\n",
      "Epoch 12/100 - Train Loss: 0.000333, Val Loss: 0.000788\n",
      "Epoch 13/100 - Train Loss: 0.000331, Val Loss: 0.000770\n",
      "Epoch 14/100 - Train Loss: 0.000330, Val Loss: 0.000780\n",
      "Epoch 15/100 - Train Loss: 0.000328, Val Loss: 0.000782\n",
      "Epoch 16/100 - Train Loss: 0.000333, Val Loss: 0.000846\n",
      "Epoch 17/100 - Train Loss: 0.000330, Val Loss: 0.000844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:38:47,593] A new study created in memory with name: LSTM_horizon_6_hyperparameter_optimisation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 - Train Loss: 0.000323, Val Loss: 0.000830\n",
      "Early stopping. counter: 15\n",
      "Best weights restored.\n",
      "Early stopping at epoch 18. Best model restored.\n",
      "Model training complete and saved!\n",
      "Saved tuned and trained LSTM with residuals to: ../../Predictions/Horizon3/LSTM_horizon_3.npy\n",
      "\n",
      "=== Tuning and Training LSTM for horizon 6 ===\n",
      "Running Optuna hyperparameter tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e882d9850634632903c0750e594a947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:38:48,998] Trial 0 finished with value: 0.002885661784782481 and parameters: {'hidden_size': 112, 'num_layers': 1, 'dropout': 0.3079622361033817, 'lr': 0.00290515073920013}. Best is trial 0 with value: 0.002885661784782481.\n",
      "[I 2025-04-24 22:38:49,819] Trial 1 finished with value: 0.002237703147816511 and parameters: {'hidden_size': 82, 'num_layers': 1, 'dropout': 0.2798202818296304, 'lr': 0.009019854618858195}. Best is trial 1 with value: 0.002237703147816511.\n",
      "[I 2025-04-24 22:38:54,101] Trial 2 finished with value: 0.0042892839057817 and parameters: {'hidden_size': 106, 'num_layers': 3, 'dropout': 0.43467859963981564, 'lr': 0.00029994744015681964}. Best is trial 1 with value: 0.002237703147816511.\n",
      "[I 2025-04-24 22:38:56,483] Trial 3 finished with value: 0.0023751853273289514 and parameters: {'hidden_size': 107, 'num_layers': 2, 'dropout': 0.4434370036215599, 'lr': 0.0032895681427602243}. Best is trial 1 with value: 0.002237703147816511.\n",
      "[I 2025-04-24 22:38:58,721] Trial 4 finished with value: 0.0021746684961073417 and parameters: {'hidden_size': 116, 'num_layers': 2, 'dropout': 0.34237995079111017, 'lr': 0.002464245664749533}. Best is trial 4 with value: 0.0021746684961073417.\n",
      "[I 2025-04-24 22:38:59,834] Trial 5 pruned. \n",
      "[I 2025-04-24 22:39:00,255] Trial 6 pruned. \n",
      "[I 2025-04-24 22:39:01,357] Trial 7 pruned. \n",
      "[I 2025-04-24 22:39:02,979] Trial 8 pruned. \n",
      "[I 2025-04-24 22:39:03,444] Trial 9 pruned. \n",
      "[I 2025-04-24 22:39:04,992] Trial 10 pruned. \n",
      "[I 2025-04-24 22:39:05,273] Trial 11 pruned. \n",
      "[I 2025-04-24 22:39:05,638] Trial 12 pruned. \n",
      "[I 2025-04-24 22:39:05,898] Trial 13 pruned. \n",
      "[I 2025-04-24 22:39:06,845] Trial 14 finished with value: 0.0022334079832022247 and parameters: {'hidden_size': 36, 'num_layers': 2, 'dropout': 0.3782730878917633, 'lr': 0.005159869488421883}. Best is trial 4 with value: 0.0021746684961073417.\n",
      "[I 2025-04-24 22:39:07,232] Trial 15 pruned. \n",
      "[I 2025-04-24 22:39:07,653] Trial 16 pruned. \n",
      "[I 2025-04-24 22:39:08,646] Trial 17 pruned. \n",
      "[I 2025-04-24 22:39:09,368] Trial 18 pruned. \n",
      "[I 2025-04-24 22:39:10,655] Trial 19 pruned. \n",
      "Best hyperparameters found:\n",
      "{'hidden_size': 116, 'num_layers': 2, 'dropout': 0.34237995079111017, 'lr': 0.002464245664749533}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4c569c21a541f5b9d6c61dcb4834ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.031564, Val Loss: 0.006142\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_1.pth (Epoch 1)\n",
      "Epoch 2/100 - Train Loss: 0.002724, Val Loss: 0.004087\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_2.pth (Epoch 2)\n",
      "Epoch 3/100 - Train Loss: 0.001407, Val Loss: 0.003297\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_3.pth (Epoch 3)\n",
      "Epoch 4/100 - Train Loss: 0.001133, Val Loss: 0.002756\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_4.pth (Epoch 4)\n",
      "Epoch 5/100 - Train Loss: 0.000904, Val Loss: 0.002394\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_5.pth (Epoch 5)\n",
      "Epoch 6/100 - Train Loss: 0.000798, Val Loss: 0.002356\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_6.pth (Epoch 6)\n",
      "Epoch 7/100 - Train Loss: 0.000800, Val Loss: 0.002160\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_7.pth (Epoch 7)\n",
      "Epoch 8/100 - Train Loss: 0.000794, Val Loss: 0.002327\n",
      "Epoch 9/100 - Train Loss: 0.000723, Val Loss: 0.002243\n",
      "Epoch 10/100 - Train Loss: 0.000710, Val Loss: 0.002158\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_10.pth (Epoch 10)\n",
      "Epoch 11/100 - Train Loss: 0.000686, Val Loss: 0.002076\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_11.pth (Epoch 11)\n",
      "Epoch 12/100 - Train Loss: 0.000672, Val Loss: 0.002070\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_12.pth (Epoch 12)\n",
      "Epoch 13/100 - Train Loss: 0.000653, Val Loss: 0.001936\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_13.pth (Epoch 13)\n",
      "Epoch 14/100 - Train Loss: 0.000674, Val Loss: 0.002295\n",
      "Epoch 15/100 - Train Loss: 0.000659, Val Loss: 0.001738\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_15.pth (Epoch 15)\n",
      "Epoch 16/100 - Train Loss: 0.000622, Val Loss: 0.001977\n",
      "Epoch 17/100 - Train Loss: 0.000648, Val Loss: 0.001735\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_17.pth (Epoch 17)\n",
      "Epoch 18/100 - Train Loss: 0.000656, Val Loss: 0.002071\n",
      "Epoch 19/100 - Train Loss: 0.000642, Val Loss: 0.001684\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_19.pth (Epoch 19)\n",
      "Epoch 20/100 - Train Loss: 0.000644, Val Loss: 0.001881\n",
      "Epoch 21/100 - Train Loss: 0.000631, Val Loss: 0.001815\n",
      "Epoch 22/100 - Train Loss: 0.000621, Val Loss: 0.001772\n",
      "Epoch 23/100 - Train Loss: 0.000617, Val Loss: 0.001831\n",
      "Epoch 24/100 - Train Loss: 0.000624, Val Loss: 0.001976\n",
      "Epoch 25/100 - Train Loss: 0.000599, Val Loss: 0.001662\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_25.pth (Epoch 25)\n",
      "Epoch 26/100 - Train Loss: 0.000648, Val Loss: 0.001514\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon6/LSTM_horizon_6_BEST_STOPPED_AT_26.pth (Epoch 26)\n",
      "Epoch 27/100 - Train Loss: 0.000616, Val Loss: 0.001723\n",
      "Epoch 28/100 - Train Loss: 0.000654, Val Loss: 0.002286\n",
      "Epoch 29/100 - Train Loss: 0.000629, Val Loss: 0.001749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:39:28,853] A new study created in memory with name: LSTM_horizon_12_hyperparameter_optimisation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 - Train Loss: 0.000602, Val Loss: 0.002024\n",
      "Early stopping. counter: 15\n",
      "Best weights restored.\n",
      "Early stopping at epoch 30. Best model restored.\n",
      "Model training complete and saved!\n",
      "Saved tuned and trained LSTM with residuals to: ../../Predictions/Horizon6/LSTM_horizon_6.npy\n",
      "\n",
      "=== Tuning and Training LSTM for horizon 12 ===\n",
      "Running Optuna hyperparameter tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a732b542d3084a7280b61f26e815e8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 22:39:29,603] Trial 0 finished with value: 0.008287428226321936 and parameters: {'hidden_size': 52, 'num_layers': 1, 'dropout': 0.01226943226085675, 'lr': 0.0035654472802951255}. Best is trial 0 with value: 0.008287428226321936.\n",
      "[I 2025-04-24 22:39:31,319] Trial 1 finished with value: 0.0089647318502622 and parameters: {'hidden_size': 89, 'num_layers': 2, 'dropout': 0.17307106062017213, 'lr': 0.0003193096308946454}. Best is trial 0 with value: 0.008287428226321936.\n",
      "[I 2025-04-24 22:39:35,135] Trial 2 finished with value: 0.008313497395387717 and parameters: {'hidden_size': 124, 'num_layers': 3, 'dropout': 0.4386668724999063, 'lr': 0.00014171220023638422}. Best is trial 0 with value: 0.008287428226321936.\n",
      "[I 2025-04-24 22:39:35,891] Trial 3 finished with value: 0.014094747602939606 and parameters: {'hidden_size': 63, 'num_layers': 1, 'dropout': 0.21653350629417462, 'lr': 0.00022475828292846403}. Best is trial 0 with value: 0.008287428226321936.\n",
      "[I 2025-04-24 22:39:37,949] Trial 4 finished with value: 0.008700052678718099 and parameters: {'hidden_size': 89, 'num_layers': 3, 'dropout': 0.11764944515654363, 'lr': 0.004782214172144222}. Best is trial 0 with value: 0.008287428226321936.\n",
      "[I 2025-04-24 22:39:38,602] Trial 5 pruned. \n",
      "[I 2025-04-24 22:39:39,656] Trial 6 pruned. \n",
      "[I 2025-04-24 22:39:39,987] Trial 7 pruned. \n",
      "[I 2025-04-24 22:39:40,453] Trial 8 pruned. \n",
      "[I 2025-04-24 22:39:40,967] Trial 9 pruned. \n",
      "[I 2025-04-24 22:39:41,363] Trial 10 pruned. \n",
      "[I 2025-04-24 22:39:42,972] Trial 11 pruned. \n",
      "[I 2025-04-24 22:39:45,435] Trial 12 pruned. \n",
      "[I 2025-04-24 22:39:45,857] Trial 13 pruned. \n",
      "[I 2025-04-24 22:39:47,293] Trial 14 pruned. \n",
      "[I 2025-04-24 22:39:48,343] Trial 15 pruned. \n",
      "[I 2025-04-24 22:39:48,567] Trial 16 pruned. \n",
      "[I 2025-04-24 22:39:49,277] Trial 17 pruned. \n",
      "[I 2025-04-24 22:39:49,974] Trial 18 pruned. \n",
      "[I 2025-04-24 22:39:50,571] Trial 19 finished with value: 0.0071945677644440105 and parameters: {'hidden_size': 58, 'num_layers': 1, 'dropout': 0.3660531587675019, 'lr': 0.008507304461726336}. Best is trial 19 with value: 0.0071945677644440105.\n",
      "Best hyperparameters found:\n",
      "{'hidden_size': 58, 'num_layers': 1, 'dropout': 0.3660531587675019, 'lr': 0.008507304461726336}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661499ad7bf24650a9053d7b2c4b395f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.031648, Val Loss: 0.012215\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon12/LSTM_horizon_12_BEST_STOPPED_AT_1.pth (Epoch 1)\n",
      "Epoch 2/100 - Train Loss: 0.002848, Val Loss: 0.007810\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon12/LSTM_horizon_12_BEST_STOPPED_AT_2.pth (Epoch 2)\n",
      "Epoch 3/100 - Train Loss: 0.001511, Val Loss: 0.006742\n",
      "Best model saved at ../../Models/Weights/LSTM/Horizon12/LSTM_horizon_12_BEST_STOPPED_AT_3.pth (Epoch 3)\n",
      "Epoch 4/100 - Train Loss: 0.001439, Val Loss: 0.006941\n",
      "Epoch 5/100 - Train Loss: 0.001389, Val Loss: 0.006887\n",
      "Epoch 6/100 - Train Loss: 0.001371, Val Loss: 0.006945\n",
      "Epoch 7/100 - Train Loss: 0.001365, Val Loss: 0.006902\n",
      "Epoch 8/100 - Train Loss: 0.001373, Val Loss: 0.006857\n",
      "Epoch 9/100 - Train Loss: 0.001367, Val Loss: 0.007303\n",
      "Epoch 10/100 - Train Loss: 0.001294, Val Loss: 0.007348\n",
      "Epoch 11/100 - Train Loss: 0.001273, Val Loss: 0.007137\n",
      "Epoch 12/100 - Train Loss: 0.001284, Val Loss: 0.007381\n",
      "Epoch 13/100 - Train Loss: 0.001278, Val Loss: 0.007014\n",
      "Epoch 14/100 - Train Loss: 0.001252, Val Loss: 0.007115\n",
      "Epoch 15/100 - Train Loss: 0.001289, Val Loss: 0.007352\n",
      "Epoch 16/100 - Train Loss: 0.001249, Val Loss: 0.007484\n",
      "Epoch 17/100 - Train Loss: 0.001201, Val Loss: 0.007827\n",
      "Epoch 18/100 - Train Loss: 0.001240, Val Loss: 0.007735\n",
      "Early stopping. counter: 15\n",
      "Best weights restored.\n",
      "Early stopping at epoch 18. Best model restored.\n",
      "Model training complete and saved!\n",
      "Saved tuned and trained LSTM with residuals to: ../../Predictions/Horizon12/LSTM_horizon_12.npy\n"
     ]
    }
   ],
   "source": [
    "from Training.Helper.PyTorchModular import optuna_tune_and_train_pytorch\n",
    "\n",
    "HORIZONS = [1, 3, 6, 12]\n",
    "\n",
    "for HORIZON in HORIZONS:\n",
    "    print(f\"\\n=== Tuning and Training LSTM for horizon {HORIZON} ===\")\n",
    "\n",
    "    model_search_space = {\n",
    "        \"hidden_size\": (int, 32, 128),\n",
    "        \"num_layers\": (int, 1, 3),\n",
    "        \"dropout\": (float, 0.0, 0.5),\n",
    "    }\n",
    "\n",
    "    optim_search_space = {\n",
    "        \"lr\": (float, 1e-4, 1e-2, {\"log\": True})\n",
    "    }\n",
    "\n",
    "    model_invariates = {\n",
    "        \"input_size\": X_seq.shape[2],\n",
    "        \"output_size\": HORIZON\n",
    "    }\n",
    "\n",
    "    # Recreate sequences and loaders for each HORIZON to match target shape\n",
    "    X_seq, y_seq = create_direct_delta_sequences(X_scaled, y_scaled, SEQ_LEN, HORIZON)\n",
    "    X_seq = X_seq.reshape(X_seq.shape[0], SEQ_LEN, -1)\n",
    "    y_seq = y_seq.reshape(y_seq.shape[0], HORIZON)\n",
    "\n",
    "    val_split = int(len(X_seq) * 0.8)\n",
    "    X_train, X_val = X_seq[:val_split], X_seq[val_split:]\n",
    "    y_train, y_val = y_seq[:val_split], y_seq[val_split:]\n",
    "\n",
    "    train_loader = prepare_dataloader(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "    val_loader = prepare_dataloader(X_val, y_val, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model, metadata = optuna_tune_and_train_pytorch(\n",
    "        model_class=LSTM,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=DEVICE,\n",
    "        model_search_space=model_search_space,\n",
    "        model_invariates=model_invariates,\n",
    "        optim_search_space=optim_search_space,\n",
    "        max_epochs=EPOCHS,\n",
    "        model_save_path=os.path.join(\"..\", \"..\", \"Models\",\"Weights\", \"LSTM\", f\"Horizon{HORIZON}\"),\n",
    "        model_name=f\"LSTM_horizon_{HORIZON}\",\n",
    "        n_trials=20,\n",
    "        n_epochs_per_trial=5,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # === FINAL FORECAST ===\n",
    "    model.eval()\n",
    "    y_pred_final = []\n",
    "    with torch.no_grad():\n",
    "        x_input = X_seq[-1].copy()\n",
    "        base_val = y_scaled[-1]\n",
    "\n",
    "        for _ in range(12):\n",
    "            x_tensor = torch.tensor(x_input[np.newaxis], dtype=torch.float32).to(DEVICE)\n",
    "            pred_delta = model(x_tensor).cpu().numpy().flatten()\n",
    "\n",
    "            step_index = min(HORIZON, len(pred_delta)) - 1\n",
    "            next_scaled = base_val + pred_delta[step_index]\n",
    "            y_pred_final.append(next_scaled)\n",
    "\n",
    "            x_input = np.roll(x_input, -1, axis=0)\n",
    "            x_input[-1] = np.concatenate([x_input[-2][:-1], [next_scaled]])\n",
    "            base_val = next_scaled\n",
    "\n",
    "    y_pred_rescaled = y_scaler.inverse_transform(np.array(y_pred_final).reshape(-1, 1)).flatten()\n",
    "    y_pred_final = np.expm1(y_pred_rescaled)\n",
    "\n",
    "    # === SAVE ===\n",
    "    save_path = os.path.join(\"..\", \"..\", \"Predictions\", f\"Horizon{HORIZON}\", f\"LSTM_horizon_{HORIZON}.npy\")\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    np.save(save_path, y_pred_final)\n",
    "    print(f\"Saved tuned and trained LSTM with residuals to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
