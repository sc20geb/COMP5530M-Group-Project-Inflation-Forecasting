{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import random\n",
    "\n",
    "# SEED SETTING\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)  # Ensures reproducibility\n",
    "\n",
    "# Set root and paths\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path.append(ROOT_PATH)\n",
    "\n",
    "from Training.Helper.dataPreprocessing import (\n",
    "    add_time_features, add_lagged_features, add_rolling_features,\n",
    "    sklearn_fit_transform, prepare_dataloader, rank_features_ccf,\n",
    "    TRAIN_DATA_PATH_1990S\n",
    ")\n",
    "from Models.LSTM import LSTM\n",
    "\n",
    "def create_direct_delta_sequences(X, y, seq_len=36, horizon=12):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len - horizon):\n",
    "        X_seq.append(X[i:i + seq_len])\n",
    "        base = y[i + seq_len - 1]\n",
    "        future = y[i + seq_len: i + seq_len + horizon]\n",
    "        delta = future - base\n",
    "        y_seq.append(delta)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# === CONFIG ===\n",
    "SEQ_LEN = 36\n",
    "HORIZON = 12\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "PATIENCE = 10\n",
    "LR = 1e-3\n",
    "TOP_K_FEATURES = 30\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 00:52:30,703 - INFO - Added time features: year, month, quarter. DataFrame shape: (408, 363)\n"
     ]
    }
   ],
   "source": [
    "# === LOAD & FEATURE ENGINEERING ===\n",
    "df = pd.read_csv(TRAIN_DATA_PATH_1990S)\n",
    "df[\"ds\"] = pd.to_datetime(df[\"observation_date\"], format=\"%m/%Y\")\n",
    "df = df.rename(columns={\"fred_PCEPI\": \"y_original\"})\n",
    "\n",
    "df = add_time_features(df, \"ds\")\n",
    "for k in [1, 2, 3, 4]:\n",
    "    df[f\"sin_{k}\"] = np.sin(2 * np.pi * k * df[\"month\"] / 12)\n",
    "    df[f\"cos_{k}\"] = np.cos(2 * np.pi * k * df[\"month\"] / 12)\n",
    "df[\"pct_change\"] = df[\"y_original\"].pct_change()\n",
    "df[\"momentum\"] = df[\"pct_change\"].diff()\n",
    "df = add_lagged_features(df, [\"y_original\"], lags=[1, 6, 12])\n",
    "df = add_rolling_features(df, \"y_original\", windows=[3, 6, 12])\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# === CCF SELECTION ===\n",
    "df_numeric = df.select_dtypes(include=[np.number]).copy()\n",
    "ccf_ranked = rank_features_ccf(df_numeric, targetCol=\"y_original\")\n",
    "selected_features = [col for col in list(ccf_ranked[:TOP_K_FEATURES]) if col in df.columns]\n",
    "features = df[selected_features]\n",
    "target_log = np.log1p(df[\"y_original\"])\n",
    "\n",
    "# === SCALE ===\n",
    "features_scaled_list, x_scaler = sklearn_fit_transform(features, StandardScaler())\n",
    "target_scaled_list, y_scaler = sklearn_fit_transform(target_log.to_frame(), StandardScaler())\n",
    "\n",
    "X_scaled = features_scaled_list[0].values\n",
    "y_scaled = target_scaled_list[0].values.flatten()\n",
    "\n",
    "# === SEQUENCES ===\n",
    "X_seq, y_seq = create_direct_delta_sequences(X_scaled, y_scaled, SEQ_LEN, HORIZON)\n",
    "X_seq = X_seq.reshape(X_seq.shape[0], SEQ_LEN, -1)\n",
    "y_seq = y_seq.reshape(y_seq.shape[0], HORIZON)\n",
    "\n",
    "# === SPLIT ===\n",
    "val_split = int(len(X_seq) * 0.8)\n",
    "X_train, X_val = X_seq[:val_split], X_seq[val_split:]\n",
    "y_train, y_val = y_seq[:val_split], y_seq[val_split:]\n",
    "\n",
    "train_loader = prepare_dataloader(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "val_loader = prepare_dataloader(X_val, y_val, shuffle=False, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalieleung/miniforge3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training LSTM with Delta Learning...\n",
      "Epoch 01: Train MAE=0.136012 | Val MAE=0.077422\n",
      "    → Learning rate: 0.001000\n",
      "Epoch 02: Train MAE=0.067680 | Val MAE=0.072237\n",
      "    → Learning rate: 0.001000\n",
      "Epoch 03: Train MAE=0.045388 | Val MAE=0.061155\n",
      "    → Learning rate: 0.001000\n",
      "Epoch 04: Train MAE=0.036974 | Val MAE=0.062949\n",
      "    → Learning rate: 0.001000\n",
      "Epoch 05: Train MAE=0.033609 | Val MAE=0.060846\n",
      "    → Learning rate: 0.001000\n",
      "Epoch 06: Train MAE=0.031757 | Val MAE=0.057116\n",
      "    → Learning rate: 0.001000\n",
      "Epoch 07: Train MAE=0.027929 | Val MAE=0.056622\n",
      "    → Learning rate: 0.001000\n",
      "Epoch 08: Train MAE=0.027212 | Val MAE=0.058799\n",
      "    → Learning rate: 0.001000\n",
      "Epoch 09: Train MAE=0.025158 | Val MAE=0.060713\n",
      "    → Learning rate: 0.001000\n",
      "Epoch 10: Train MAE=0.024334 | Val MAE=0.061584\n",
      "    → Learning rate: 0.001000\n",
      "Epoch 11: Train MAE=0.024647 | Val MAE=0.067601\n",
      "    → Learning rate: 0.000500\n",
      "Epoch 12: Train MAE=0.023073 | Val MAE=0.065682\n",
      "    → Learning rate: 0.000500\n",
      "Epoch 13: Train MAE=0.021944 | Val MAE=0.067194\n",
      "    → Learning rate: 0.000500\n",
      "Epoch 14: Train MAE=0.021681 | Val MAE=0.068991\n",
      "    → Learning rate: 0.000500\n",
      "Epoch 15: Train MAE=0.022725 | Val MAE=0.069257\n",
      "    → Learning rate: 0.000250\n",
      "Epoch 16: Train MAE=0.022008 | Val MAE=0.068774\n",
      "    → Learning rate: 0.000250\n",
      "Epoch 17: Train MAE=0.022778 | Val MAE=0.069380\n",
      "    → Learning rate: 0.000250\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "# === MODEL ===\n",
    "model = LSTM(input_size=X_seq.shape[2], output_size=HORIZON).to(DEVICE)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Add Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# === TRAIN LOOP ===\n",
    "print(\"\\n Training LSTM with Delta Learning...\")\n",
    "best_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(DEVICE)\n",
    "            y_batch = y_batch.to(DEVICE)\n",
    "            pred = model(X_batch)\n",
    "            val_loss += criterion(pred, y_batch).item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1:02d}: Train MAE={avg_train_loss:.6f} | Val MAE={avg_val_loss:.6f}\")\n",
    "\n",
    "    # Step scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Optional: print current learning rate\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f\"    → Learning rate: {current_lr:.6f}\")\n",
    "\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# === FINAL FORECAST (add deltas to base value) ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_input = torch.tensor(X_seq[-1][np.newaxis], dtype=torch.float32).to(DEVICE)\n",
    "    pred_delta = model(x_input).cpu().numpy().flatten()\n",
    "    base_val = y_scaled[-1]\n",
    "    pred_scaled = base_val + pred_delta\n",
    "\n",
    "y_pred_rescaled = y_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_pred_final = np.expm1(y_pred_rescaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved improved LSTM with residuals to: ../../Predictions/LSTM.npy\n"
     ]
    }
   ],
   "source": [
    "# === SAVE ===\n",
    "save_path = os.path.join(\"..\", \"..\", \"Predictions\", \"LSTM.npy\")\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "np.save(save_path, y_pred_final)\n",
    "print(f\"\\nSaved improved LSTM with residuals to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
